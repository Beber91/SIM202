
R version 4.0.4 (2021-02-15) -- "Lost Library Book"
Copyright (C) 2021 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R est un logiciel libre livré sans AUCUNE GARANTIE.
Vous pouvez le redistribuer sous certaines conditions.
Tapez 'license()' ou 'licence()' pour plus de détails.

R est un projet collaboratif avec de nombreux contributeurs.
Tapez 'contributors()' pour plus d'information et
'citation()' pour la façon de le citer dans les publications.

Tapez 'demo()' pour des démonstrations, 'help()' pour l'aide
en ligne ou 'help.start()' pour obtenir l'aide au format HTML.
Tapez 'q()' pour quitter R.

[Sauvegarde de la session précédente restaurée]

> setwd('/home/nicolas/Documents/SIM202/')
> rm(list=objects())
> graphics.off()
> libs.to.load = c("tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
   tidyverse    lubridate       ranger       pracma      Metrics         mgcv 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       keras       visreg        caret         mc2d        opera        abind 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
randomForest 
        TRUE 
> expert1.train  = predict(model,train)
Error in predict(model, train) : objet 'model' introuvable
> rm(list=objects())
> graphics.off()
> libs.to.load = c("tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
   tidyverse    lubridate       ranger       pracma      Metrics         mgcv 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       keras       visreg        caret         mc2d        opera        abind 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
randomForest 
        TRUE 
> setwd("C:/Users/CM/code/M1/R")
Error in setwd("C:/Users/CM/code/M1/R") : 
  impossible de changer de répertoire de travail
> files.sources = list.files()
> sapply(files.sources, source) 
Error in FUN(X[[i]], ...) : #main_matthieu.r#:230:1: '}' inattendu(e)
229:     experts.train = array_reshape(abind(experts.train, new.train, along = 2), c(3028,1,length(ex
230: }
     ^
> rm(list=objects())
> graphics.off()
> libs.to.load = c("tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
   tidyverse    lubridate       ranger       pracma      Metrics         mgcv 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       keras       visreg        caret         mc2d        opera        abind 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
randomForest 
        TRUE 
> setwd("C:/Users/CM/code/M1/R")
Error in setwd("C:/Users/CM/code/M1/R") : 
  impossible de changer de répertoire de travail
> files.sources = list.files()
> sapply(files.sources, source) 
Error in setwd("C:/Users/CM/code/M1/R") : 
  impossible de changer de répertoire de travail
> train <- read_delim(file="data/train_V2.csv",delim=',')

── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character()
)
ℹ Use `spec()` for the full column specifications.

> test <- read_delim(file="data/test_V2.csv",delim=',')

── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character(),
  Usage = col_character()
)
ℹ Use `spec()` for the full column specifications.

> plot(train$Date, train$Load, type='l')
> train$WeekDays = days_to_numeric(train)
Error in days_to_numeric(train) : 
  impossible de trouver la fonction "days_to_numeric"
> test$WeekDays = days_to_numeric(test)
Error in days_to_numeric(test) : 
  impossible de trouver la fonction "days_to_numeric"
> sessionInfo()
R version 4.0.4 (2021-02-15)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Debian GNU/Linux bullseye/sid

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.13.so

locale:
 [1] LC_CTYPE=fr_FR.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=fr_FR.UTF-8        LC_COLLATE=fr_FR.UTF-8    
 [5] LC_MONETARY=fr_FR.UTF-8    LC_MESSAGES=fr_FR.UTF-8   
 [7] LC_PAPER=fr_FR.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=fr_FR.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] randomForest_4.6-14 abind_1.4-5         opera_1.1          
 [4] mc2d_0.1-18         mvtnorm_1.1-1       caret_6.0-86       
 [7] lattice_0.20-41     visreg_2.7.0        keras_2.3.0.0      
[10] mgcv_1.8-34         nlme_3.1-152        Metrics_0.1.4      
[13] pracma_2.3.3        ranger_0.12.1       lubridate_1.7.9.2  
[16] forcats_0.5.1       stringr_1.4.0       dplyr_1.0.3        
[19] purrr_0.3.4         readr_1.4.0         tidyr_1.1.2        
[22] tibble_3.0.5        ggplot2_3.3.3       tidyverse_1.3.0    

loaded via a namespace (and not attached):
 [1] fs_1.5.0             httr_1.4.2           tools_4.0.4         
 [4] backports_1.2.1      R6_2.5.0             rpart_4.1-15        
 [7] DBI_1.1.1            colorspace_2.0-0     nnet_7.3-15         
[10] withr_2.4.1          tidyselect_1.1.0     compiler_4.0.4      
[13] cli_2.2.0            rvest_0.3.6          xml2_1.3.2          
[16] scales_1.1.1         tfruns_1.4           base64enc_0.1-3     
[19] pkgconfig_2.0.3      dbplyr_2.0.0         rlang_0.4.10        
[22] readxl_1.3.1         rstudioapi_0.13      generics_0.1.0      
[25] jsonlite_1.7.2       tensorflow_2.2.0     ModelMetrics_1.2.2.2
[28] magrittr_2.0.1       Matrix_1.3-2         Rcpp_1.0.6          
[31] munsell_0.5.0        fansi_0.4.2          reticulate_1.18     
[34] lifecycle_0.2.0      stringi_1.5.3        whisker_0.4         
[37] pROC_1.17.0.1        MASS_7.3-53.1        plyr_1.8.6          
[40] recipes_0.1.15       grid_4.0.4           crayon_1.3.4        
[43] haven_2.3.1          splines_4.0.4        hms_1.0.0           
[46] zeallot_0.1.0        ps_1.5.0             pillar_1.4.7        
[49] reshape2_1.4.4       codetools_0.2-18     stats4_4.0.4        
[52] reprex_1.0.0         glue_1.4.2           data.table_1.13.6   
[55] modelr_0.1.8         vctrs_0.3.6          foreach_1.5.1       
[58] cellranger_1.1.0     gtable_0.3.0         assertthat_0.2.1    
[61] gower_0.2.2          prodlim_2019.11.13   broom_0.7.3         
[64] class_7.3-18         survival_3.2-7       timeDate_3043.102   
[67] iterators_1.0.13     lava_1.6.8.1         ellipsis_0.3.1      
[70] ipred_0.9-9         
> days_to_numeric
Erreur : objet 'days_to_numeric' introuvable
> days_to_numeric(train)
Error in days_to_numeric(train) : 
  impossible de trouver la fonction "days_to_numeric"
> list.files()
 [1] "#main_matthieu.r#" "data"              "days_to_numeric.r"
 [4] "evaluate.r"        "evaluate.R~"       "fig"              
 [7] "format_data.r"     "format_data.R~"    "fourier.r"        
[10] "fourier.r~"        "lstm.r"            "main_matthieu.r"  
[13] "main.r"            "main.R~"           "scale.r"          
[16] "submission.csv~"   "submission~"       "submissions"      
[19] "xgboost.r"        
> files.sources = list.files(pattern = "*.r")
> list.files(pattern = "*.r")

 [1] "days_to_numeric.r" "evaluate.r"        "format_data.r"    
 [4] "format_data.R~"    "fourier.r"         "fourier.r~"       
 [7] "lstm.r"            "main_matthieu.r"   "main.r"           
[10] "scale.r"           "xgboost.r"        
> list.files(pattern = "*.r\b")
character(0)
> list.files(pattern = "*.r")
 [1] "days_to_numeric.r" "evaluate.r"        "format_data.r"    
 [4] "format_data.R~"    "fourier.r"         "fourier.r~"       
 [7] "lstm.r"            "main_matthieu.r"   "main.r"           
[10] "scale.r"           "xgboost.r"        
> list.files(pattern = "*.r$")
[1] "days_to_numeric.r" "evaluate.r"        "format_data.r"    
[4] "fourier.r"         "lstm.r"            "main_matthieu.r"  
[7] "main.r"            "scale.r"           "xgboost.r"        
> q()
Save workspace image? [y/n/c]: n

Process R finished at Wed Feb 24 14:30:57 2021


R version 4.0.4 (2021-02-15) -- "Lost Library Book"
Copyright (C) 2021 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R est un logiciel libre livré sans AUCUNE GARANTIE.
Vous pouvez le redistribuer sous certaines conditions.
Tapez 'license()' ou 'licence()' pour plus de détails.

R est un projet collaboratif avec de nombreux contributeurs.
Tapez 'contributors()' pour plus d'information et
'citation()' pour la façon de le citer dans les publications.

Tapez 'demo()' pour des démonstrations, 'help()' pour l'aide
en ligne ou 'help.start()' pour obtenir l'aide au format HTML.
Tapez 'q()' pour quitter R.

[Sauvegarde de la session précédente restaurée]

> setwd('/home/nicolas/Documents/SIM202/')
> rm(list=objects())
> graphics.off()
> libs.to.load = c("tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
   tidyverse    lubridate       ranger       pracma      Metrics         mgcv 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       keras       visreg        caret         mc2d        opera        abind 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
randomForest 
        TRUE 
> files.sources = list.files(pattern = "*.r$")
> sapply(files.sources, source) 
Error in setwd("C:/Users/CM/code/M1/R") : 
  impossible de changer de répertoire de travail
> sapply(files.sources, source) 
Error in lapply(X = X, FUN = FUN, ...) : 
  objet 'files.sources' introuvable
> files.sources = list.files(pattern = "*.r$")
> sapply(files.sources, source) 
Erreur : C stack usage  9521940 is too close to the limit
> Cstack_info()
      size    current  direction eval_depth 
   9521766      28464          1          2 
> q()
Save workspace image? [y/n/c]: n

Process R finished at Wed Feb 24 14:33:54 2021


R version 4.0.4 (2021-02-15) -- "Lost Library Book"
Copyright (C) 2021 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R est un logiciel libre livré sans AUCUNE GARANTIE.
Vous pouvez le redistribuer sous certaines conditions.
Tapez 'license()' ou 'licence()' pour plus de détails.

R est un projet collaboratif avec de nombreux contributeurs.
Tapez 'contributors()' pour plus d'information et
'citation()' pour la façon de le citer dans les publications.

Tapez 'demo()' pour des démonstrations, 'help()' pour l'aide
en ligne ou 'help.start()' pour obtenir l'aide au format HTML.
Tapez 'q()' pour quitter R.

[Sauvegarde de la session précédente restaurée]

> setwd('/home/nicolas/Documents/SIM202/')
> rm(list=objects())
> graphics.off()
> libs.to.load = c("tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
files.sources = list.files(pattern = "*.r$")
   tidyverse    lubridate       ranger       pracma      Metrics         mgcv 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       keras       visreg        caret         mc2d        opera        abind 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
randomForest 
        TRUE 
> > files.sources = list.files(pattern = "*.r$")
> sapply(files.sources, source) 
Erreur : C stack usage  9530996 is too close to the limit
> lapply(files.sources, source) 
Error in lapply(files.sources, source) : 
  objet 'files.sources' introuvable
> files.sources = list.files(pattern = "*.r$")
> lapply(files.sources, source) 
Erreur : C stack usage  9522164 is too close to the limit
> invisible(sapply(files.sources, source)) 
Error in lapply(X = X, FUN = FUN, ...) : 
  objet 'files.sources' introuvable
> files.sources = list.files(pattern = "*.r$")
> invisible(sapply(files.sources, source)) 
Erreur : C stack usage  9530740 is too close to the limit
> invisible(sapply(list.files(pattern = "*.r$"), source)) 
Erreur : C stack usage  9530740 is too close to the limit
> files.sources = list.files(pattern = "*.r$")
> files.sources
[1] "days_to_numeric.r" "evaluate.r"        "format_data.r"    
[4] "fourier.r"         "lstm.r"            "main_matthieu.r"  
[7] "main.r"            "scale.r"           "xgboost.r"        
> source(files.sources)
Error in file(filename, "r", encoding = encoding) : 
  argument 'description' incorrect
De plus : Warning message:
In if (file == "") { :
  la condition a une longueur > 1 et seul le premier élément est utilisé
> files.sources = list.files(pattern = "*.r$")
> for (f in files.sources) {
+     source(f)
+ }
Erreur : C stack usage  9524996 is too close to the limit
> files.sources = list.files(pattern = "*.r$", ignore.case = "main_matthieu.r")
Error in list.files(pattern = "*.r$", ignore.case = "main_matthieu.r") : 
  argument 'ignore.case' incorrect
> files.sources
Erreur : objet 'files.sources' introuvable
> files.sources = list.files(pattern = "*.r$", ignore.case = "main_matthieu.r")
Error in list.files(pattern = "*.r$", ignore.case = "main_matthieu.r") : 
  argument 'ignore.case' incorrect
> files.sources = list.files(pattern = "*.r$")
> files.sources[names(files.sources) != "main_matthieu.r")]
Erreur : ')' inattendu(e) in "files.sources[names(files.sources) != "main_matthieu.r")"
> files.sources[names(files.sources) != "main_matthieu.r"]
character(0)
> files.sources
[1] "days_to_numeric.r" "evaluate.r"        "format_data.r"    
[4] "fourier.r"         "lstm.r"            "main_matthieu.r"  
[7] "main.r"            "scale.r"           "xgboost.r"        
> files.sources[files.sources != "main_matthieu.r"]
[1] "days_to_numeric.r" "evaluate.r"        "format_data.r"    
[4] "fourier.r"         "lstm.r"            "main.r"           
[7] "scale.r"           "xgboost.r"        
> files.sources = list.files(pattern = "*.r$")
> file.sources = files.sources[files.sources != "main_matthieu.r"]
> sapply(files.sources, source) 
Erreur : C stack usage  9530996 is too close to the limit
> files.sources
Erreur : objet 'files.sources' introuvable
> files.sources = list.files(pattern = "*.r$")
> file.sources = files.sources[files.sources != "main_matthieu.r"]
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> sapply(files.sources, source) 

Attachement du package : ‘tensorflow’

The following object is masked from ‘package:caret’:

    train

Error in file(filename, "r", encoding = encoding) : 
  impossible d'ouvrir la connexion
De plus : Warning message:
In file(filename, "r", encoding = encoding) :
  impossible d'ouvrir le fichier 'format_data.R' : Aucun fichier ou dossier de ce type
> rm(list=objects())
> graphics.off()
> libs.to.load = c("tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
   tidyverse    lubridate       ranger       pracma      Metrics         mgcv 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       keras       visreg        caret         mc2d        opera        abind 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
randomForest   tensorflow 
        TRUE         TRUE 
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> sapply(files.sources, source) 
Error in format_data("./data/train_V2.csv", "./data/test_V2.csv") (from main.r#6) : 
  impossible de trouver la fonction "format_data"
> rm(list=objects())
> graphics.off()
> libs.to.load = c("tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
   tidyverse    lubridate       ranger       pracma      Metrics         mgcv 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       keras       visreg        caret         mc2d        opera        abind 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
randomForest   tensorflow 
        TRUE         TRUE 
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> sapply(files.sources, source)
        days_to_numeric.r evaluate.r format_data.r fourier.r lstm.r main.r
value   ?                 ?          ?             ?         ?      NULL  
visible FALSE             FALSE      FALSE         FALSE     FALSE  FALSE 
        scale.r xgboost.r
value   ?       ?        
visible FALSE   FALSE    
> model = "lstm"
> data = format_data("./data/train_V2.csv", "./data/test_V2.csv")
Error in format_data("./data/train_V2.csv", "./data/test_V2.csv") : 
  impossible de trouver la fonction "format_data"
> source("days_to_numeric.r")
> files.sources
Erreur : objet 'files.sources' introuvable
> sourceDir(dir, pattern = "\\.[r|R]$", all.files = FALSE,
+   C-c C-c
> sourceDir(dir, pattern = "\\.[r|R]$", all.files = FALSE, recursive = FALSE, quiet = TRUE)
Error in sourceDir(dir, pattern = "\\.[r|R]$", all.files = FALSE, recursive = FALSE,  : 
  impossible de trouver la fonction "sourceDir"
> sessionInfo()
R version 4.0.4 (2021-02-15)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Debian GNU/Linux bullseye/sid

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.13.so

locale:
 [1] LC_CTYPE=fr_FR.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=fr_FR.UTF-8        LC_COLLATE=fr_FR.UTF-8    
 [5] LC_MONETARY=fr_FR.UTF-8    LC_MESSAGES=fr_FR.UTF-8   
 [7] LC_PAPER=fr_FR.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=fr_FR.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] tensorflow_2.2.0    xgboost_1.3.2.1     randomForest_4.6-14
 [4] abind_1.4-5         opera_1.1           mc2d_0.1-18        
 [7] mvtnorm_1.1-1       caret_6.0-86        lattice_0.20-41    
[10] visreg_2.7.0        keras_2.3.0.0       mgcv_1.8-34        
[13] nlme_3.1-152        Metrics_0.1.4       pracma_2.3.3       
[16] ranger_0.12.1       lubridate_1.7.9.2   forcats_0.5.1      
[19] stringr_1.4.0       dplyr_1.0.3         purrr_0.3.4        
[22] readr_1.4.0         tidyr_1.1.2         tibble_3.0.5       
[25] ggplot2_3.3.3       tidyverse_1.3.0    

loaded via a namespace (and not attached):
 [1] fs_1.5.0             httr_1.4.2           tools_4.0.4         
 [4] backports_1.2.1      R6_2.5.0             rpart_4.1-15        
 [7] DBI_1.1.1            colorspace_2.0-0     nnet_7.3-15         
[10] withr_2.4.1          tidyselect_1.1.0     compiler_4.0.4      
[13] cli_2.2.0            rvest_0.3.6          xml2_1.3.2          
[16] scales_1.1.1         tfruns_1.4           base64enc_0.1-3     
[19] pkgconfig_2.0.3      dbplyr_2.0.0         rlang_0.4.10        
[22] readxl_1.3.1         rstudioapi_0.13      generics_0.1.0      
[25] jsonlite_1.7.2       ModelMetrics_1.2.2.2 magrittr_2.0.1      
[28] Matrix_1.3-2         Rcpp_1.0.6           munsell_0.5.0       
[31] fansi_0.4.2          reticulate_1.18      lifecycle_0.2.0     
[34] stringi_1.5.3        whisker_0.4          pROC_1.17.0.1       
[37] MASS_7.3-53.1        plyr_1.8.6           recipes_0.1.15      
[40] grid_4.0.4           crayon_1.3.4         haven_2.3.1         
[43] splines_4.0.4        hms_1.0.0            zeallot_0.1.0       
[46] ps_1.5.0             pillar_1.4.7         reshape2_1.4.4      
[49] codetools_0.2-18     stats4_4.0.4         reprex_1.0.0        
[52] glue_1.4.2           data.table_1.13.6    modelr_0.1.8        
[55] vctrs_0.3.6          foreach_1.5.1        cellranger_1.1.0    
[58] gtable_0.3.0         assertthat_0.2.1     gower_0.2.2         
[61] prodlim_2019.11.13   broom_0.7.3          class_7.3-18        
[64] survival_3.2-7       timeDate_3043.102    iterators_1.0.13    
[67] lava_1.6.8.1         ellipsis_0.3.1       ipred_0.9-9         
> rm(list=objects())
> graphics.off()
> libs.to.load = c("source", "tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
      source    tidyverse    lubridate       ranger       pracma      Metrics 
       FALSE         TRUE         TRUE         TRUE         TRUE         TRUE 
        mgcv        keras       visreg        caret         mc2d        opera 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       abind randomForest   tensorflow 
        TRUE         TRUE         TRUE 
Warning message:
In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  aucun package nommé ‘source’ n'est trouvé
> install.packages("source")
Installation du package dans ‘/home/nicolas/R/x86_64-pc-linux-gnu-library/4.0’
(car ‘lib’ n'est pas spécifié)
Warning message:
package ‘source’ is not available for this version of R

A version of this package for your version of R might be available elsewhere,
see the ideas at
https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages 
> install.packages("icesTAF-package")
Installation du package dans ‘/home/nicolas/R/x86_64-pc-linux-gnu-library/4.0’
(car ‘lib’ n'est pas spécifié)
Warning message:
package ‘icesTAF-package’ is not available for this version of R

A version of this package for your version of R might be available elsewhere,
see the ideas at
https://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages 
> install.packages("icesTAF")
Installation du package dans ‘/home/nicolas/R/x86_64-pc-linux-gnu-library/4.0’
(car ‘lib’ n'est pas spécifié)
installation des dépendances ‘brew’, ‘roxygen2’

essai de l'URL 'https://cloud.r-project.org/src/contrib/brew_1.0-6.tar.gz'
Content type 'application/x-gzip' length 83315 bytes (81 KB)
==================================================
downloaded 81 KB

essai de l'URL 'https://cloud.r-project.org/src/contrib/roxygen2_7.1.1.tar.gz'
Content type 'application/x-gzip' length 254118 bytes (248 KB)
==================================================
downloaded 248 KB

essai de l'URL 'https://cloud.r-project.org/src/contrib/icesTAF_3.6.0.tar.gz'
Content type 'application/x-gzip' length 84640 bytes (82 KB)
==================================================
downloaded 82 KB

* installing *source* package ‘brew’ ...
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (brew)
* installing *source* package ‘roxygen2’ ...
** package ‘roxygen2’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** libs
g++ -std=gnu++11 -I"/usr/share/R/include" -DNDEBUG  -I'/home/nicolas/R/x86_64-pc-linux-gnu-library/4.0/Rcpp/include'    -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c RcppExports.cpp -o RcppExports.o
g++ -std=gnu++11 -I"/usr/share/R/include" -DNDEBUG  -I'/home/nicolas/R/x86_64-pc-linux-gnu-library/4.0/Rcpp/include'    -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c escapeExamples.cpp -o escapeExamples.o
g++ -std=gnu++11 -I"/usr/share/R/include" -DNDEBUG  -I'/home/nicolas/R/x86_64-pc-linux-gnu-library/4.0/Rcpp/include'    -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c isComplete.cpp -o isComplete.o
g++ -std=gnu++11 -I"/usr/share/R/include" -DNDEBUG  -I'/home/nicolas/R/x86_64-pc-linux-gnu-library/4.0/Rcpp/include'    -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c leadingSpaces.cpp -o leadingSpaces.o
g++ -std=gnu++11 -I"/usr/share/R/include" -DNDEBUG  -I'/home/nicolas/R/x86_64-pc-linux-gnu-library/4.0/Rcpp/include'    -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c parser2.cpp -o parser2.o
g++ -std=gnu++11 -I"/usr/share/R/include" -DNDEBUG  -I'/home/nicolas/R/x86_64-pc-linux-gnu-library/4.0/Rcpp/include'    -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c wrapUsage.cpp -o wrapUsage.o
g++ -std=gnu++11 -shared -L/usr/lib/R/lib -Wl,-z,relro -o roxygen2.so RcppExports.o escapeExamples.o isComplete.o leadingSpaces.o parser2.o wrapUsage.o -L/usr/lib/R/lib -lR
installing to /home/nicolas/R/x86_64-pc-linux-gnu-library/4.0/00LOCK-roxygen2/00new/roxygen2/libs
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
*** copying figures
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** checking absolute paths in shared objects and dynamic libraries
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (roxygen2)
* installing *source* package ‘icesTAF’ ...
** package ‘icesTAF’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** R
** data
*** moving datasets to lazyload DB
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (icesTAF)

Les packages source téléchargés sont dans
	‘/tmp/RtmpA8r08B/downloaded_packages’
> rm(list=objects())
> graphics.off()
> libs.to.load = c("icesTAF", "tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
     icesTAF    tidyverse    lubridate       ranger       pracma      Metrics 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
        mgcv        keras       visreg        caret         mc2d        opera 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       abind randomForest   tensorflow 
        TRUE         TRUE         TRUE 
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> sourceDir(dir, pattern = "\\.[r|R]$", all.files = FALSE, recursive = FALSE, quiet = TRUE)
Error in dir[1] : objet de type 'closure' non indiçable
> sourceDir(".", pattern = "\\.[r|R]$", all.files = FALSE, recursive = FALSE, quiet = TRUE)
Erreur : C stack usage  9522420 is too close to the limit
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> sapply(files.sources, source)
        days_to_numeric.r evaluate.r format_data.r fourier.r lstm.r main.r
value   ?                 ?          ?             ?         ?      NULL  
visible FALSE             FALSE      FALSE         FALSE     FALSE  FALSE 
        scale.r xgboost.r
value   ?       ?        
visible FALSE   FALSE    
> sourceDir(files.sources, pattern = "\\.[r|R]$", all.files = FALSE, recursive = FALSE, quiet = TRUE)
Error in dir.exists(dir[1]) : objet 'files.sources' introuvable
> files.sources = files.sources[files.sources != "main_matthieu.r"]
Erreur : objet 'files.sources' introuvable
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> sourceDir(files.sources, pattern = "\\.[r|R]$", all.files = FALSE, recursive = FALSE, quiet = TRUE)
> data = format_data("./data/train_V2.csv", "./data/test_V2.csv")
Error in format_data("./data/train_V2.csv", "./data/test_V2.csv") : 
  impossible de trouver la fonction "format_data"
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> sourceDir(files.sources, quiet = TRUE)
> data = format_data("./data/train_V2.csv", "./data/test_V2.csv")
Error in format_data("./data/train_V2.csv", "./data/test_V2.csv") : 
  impossible de trouver la fonction "format_data"
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> sourceDir(files.sources, quiet = FALSE)
  days_to_numeric.r
  evaluate.r
  format_data.r
  fourier.r
  lstm.r
  main.r
  scale.r
  xgboost.r
> data = format_data("./data/train_V2.csv", "./data/test_V2.csv")
Error in format_data("./data/train_V2.csv", "./data/test_V2.csv") : 
  impossible de trouver la fonction "format_data"
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> sapply(files.sources, source)
        days_to_numeric.r evaluate.r format_data.r fourier.r lstm.r main.r
value   ?                 ?          ?             ?         ?      NULL  
visible FALSE             FALSE      FALSE         FALSE     FALSE  FALSE 
        scale.r xgboost.r
value   ?       ?        
visible FALSE   FALSE    
> q()
Save workspace image? [y/n/c]: n

Process R finished at Wed Feb 24 15:44:17 2021


R version 4.0.4 (2021-02-15) -- "Lost Library Book"
Copyright (C) 2021 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R est un logiciel libre livré sans AUCUNE GARANTIE.
Vous pouvez le redistribuer sous certaines conditions.
Tapez 'license()' ou 'licence()' pour plus de détails.

R est un projet collaboratif avec de nombreux contributeurs.
Tapez 'contributors()' pour plus d'information et
'citation()' pour la façon de le citer dans les publications.

Tapez 'demo()' pour des démonstrations, 'help()' pour l'aide
en ligne ou 'help.start()' pour obtenir l'aide au format HTML.
Tapez 'q()' pour quitter R.

[Sauvegarde de la session précédente restaurée]

> setwd('/home/nicolas/Documents/SIM202/')
> rm(list=objects())
> graphics.off()
> libs.to.load = c("icesTAF", "tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
     icesTAF    tidyverse    lubridate       ranger       pracma      Metrics 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
        mgcv        keras       visreg        caret         mc2d        opera 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       abind randomForest   tensorflow 
        TRUE         TRUE         TRUE 
> files.sources = list.files(pattern = "*.r$")
> files.sources
[1] "days_to_numeric.r" "evaluate.r"        "format_data.r"    
[4] "fourier.r"         "lstm.r"            "main_matthieu.r"  
[7] "main.r"            "scale.r"           "xgboost.r"        
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> sapply(files.sources, source)
        days_to_numeric.r evaluate.r format_data.r fourier.r lstm.r main.r
value   ?                 ?          ?             ?         ?      NULL  
visible FALSE             FALSE      FALSE         FALSE     FALSE  FALSE 
        scale.r xgboost.r
value   ?       ?        
visible FALSE   FALSE    
> install.packages("devtools")
Installation du package dans ‘/home/nicolas/R/x86_64-pc-linux-gnu-library/4.0’
(car ‘lib’ n'est pas spécifié)
installation des dépendances ‘credentials’, ‘zip’, ‘gitcreds’, ‘ini’, ‘gert’, ‘gh’, ‘xopen’, ‘usethis’, ‘DT’, ‘memoise’, ‘rcmdcheck’, ‘remotes’, ‘rversions’, ‘sessioninfo’

essai de l'URL 'https://cloud.r-project.org/src/contrib/credentials_1.3.0.tar.gz'
Content type 'application/x-gzip' length 230082 bytes (224 KB)
==================================================
downloaded 224 KB

essai de l'URL 'https://cloud.r-project.org/src/contrib/zip_2.1.1.tar.gz'
Content type 'application/x-gzip' length 112865 bytes (110 KB)
==================================================
downloaded 110 KB

essai de l'URL 'https://cloud.r-project.org/src/contrib/gitcreds_0.1.1.tar.gz'
Content type 'application/x-gzip' length 61913 bytes (60 KB)
==================================================
downloaded 60 KB

essai de l'URL 'https://cloud.r-project.org/src/contrib/ini_0.3.1.tar.gz'
Content type 'application/x-gzip' length 3488 bytes
==================================================
downloaded 3488 bytes

essai de l'URL 'https://cloud.r-project.org/src/contrib/gert_1.2.0.tar.gz'
Content type 'application/x-gzip' length 66100 bytes (64 KB)
==================================================
downloaded 64 KB

essai de l'URL 'https://cloud.r-project.org/src/contrib/gh_1.2.0.tar.gz'
Content type 'application/x-gzip' length 43909 bytes (42 KB)
==================================================
downloaded 42 KB

essai de l'URL 'https://cloud.r-project.org/src/contrib/xopen_1.0.0.tar.gz'
Content type 'application/x-gzip' length 11221 bytes (10 KB)
==================================================
downloaded 10 KB

essai de l'URL 'https://cloud.r-project.org/src/contrib/usethis_2.0.1.tar.gz'
Content type 'application/x-gzip' length 306090 bytes (298 KB)
==================================================
downloaded 298 KB

essai de l'URL 'https://cloud.r-project.org/src/contrib/DT_0.17.tar.gz'
Content type 'application/x-gzip' length 1583156 bytes (1.5 MB)
==================================================
downloaded 1.5 MB

essai de l'URL 'https://cloud.r-project.org/src/contrib/memoise_2.0.0.tar.gz'
Content type 'application/x-gzip' length 17337 bytes (16 KB)
==================================================
downloaded 16 KB

essai de l'URL 'https://cloud.r-project.org/src/contrib/rcmdcheck_1.3.3.tar.gz'
Content type 'application/x-gzip' length 45380 bytes (44 KB)
==================================================
downloaded 44 KB

essai de l'URL 'https://cloud.r-project.org/src/contrib/remotes_2.2.0.tar.gz'
Content type 'application/x-gzip' length 145553 bytes (142 KB)
==================================================
downloaded 142 KB

essai de l'URL 'https://cloud.r-project.org/src/contrib/rversions_2.0.2.tar.gz'
Content type 'application/x-gzip' length 41558 bytes (40 KB)
==================================================
downloaded 40 KB

essai de l'URL 'https://cloud.r-project.org/src/contrib/sessioninfo_1.1.1.tar.gz'
Content type 'application/x-gzip' length 46027 bytes (44 KB)
==================================================
downloaded 44 KB

essai de l'URL 'https://cloud.r-project.org/src/contrib/devtools_2.3.2.tar.gz'
Content type 'application/x-gzip' length 373387 bytes (364 KB)
==================================================
downloaded 364 KB

* installing *source* package ‘credentials’ ...
** package ‘credentials’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (credentials)
* installing *source* package ‘zip’ ...
** package ‘zip’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** libs
gcc -std=gnu99 -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  miniz.c zip.c unixutils.c tools/cmdzip.c -o tools/cmdzip
gcc -std=gnu99 -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  miniz.c zip.c unixutils.c tools/cmdunzip.c -o tools/cmdunzip
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c init.c -o init.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c miniz.c -o miniz.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rzip.c -o rzip.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c zip.c -o zip.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c unixutils.c -o unixutils.o
gcc -std=gnu99 -shared -L/usr/lib/R/lib -Wl,-z,relro -o zip.so init.o miniz.o rzip.o zip.o unixutils.o -L/usr/lib/R/lib -lR
installing via 'install.libs.R' to /home/nicolas/R/x86_64-pc-linux-gnu-library/4.0/00LOCK-zip/00new/zip
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded from temporary location
** checking absolute paths in shared objects and dynamic libraries
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (zip)
* installing *source* package ‘gitcreds’ ...
** package ‘gitcreds’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (gitcreds)
* installing *source* package ‘ini’ ...
** package ‘ini’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** R
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (ini)
* installing *source* package ‘xopen’ ...
** package ‘xopen’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (xopen)
* installing *source* package ‘DT’ ...
** package ‘DT’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (DT)
* installing *source* package ‘memoise’ ...
** package ‘memoise’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** R
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (memoise)
* installing *source* package ‘remotes’ ...
** package ‘remotes’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (remotes)
* installing *source* package ‘rversions’ ...
** package ‘rversions’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
*** copying figures
** building package indices
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (rversions)
* installing *source* package ‘sessioninfo’ ...
** package ‘sessioninfo’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (sessioninfo)
* installing *source* package ‘gert’ ...
** package ‘gert’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
> curl::curl_download("https://r-lib.github.io/gert/get-libgit2-linux.sh","get-libgit2-linux.sh")
> 
> 
> curl::curl_download('https://r-lib.github.io/gert/libgit2-1.1.0.x86_64_linux.tar.gz','bundle.tar.gz')
> 
> 
Using static libgit2-1.1.0 for Linux x86_64
Using PKG_CFLAGS=-DSTATIC_LIBGIT2 -I/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/include
Using PKG_LIBS=-L/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/lib -lgit2 -lrt -lpthread -lssh2 -lssl -lcrypto -ldl -lpcre -lz
Configuration OK!
** libs
rm -f gert.so branch.o clone.o commit.o config.o conflicts.o files.o init.o merge.o rebase.o stash.o submodules.o tag.o utils.o version.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG -DSTATIC_LIBGIT2 -I/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/include -DR_NO_REMAP -DSTRICT_R_HEADERS    -fvisibility=hidden -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c branch.c -o branch.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG -DSTATIC_LIBGIT2 -I/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/include -DR_NO_REMAP -DSTRICT_R_HEADERS    -fvisibility=hidden -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c clone.c -o clone.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG -DSTATIC_LIBGIT2 -I/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/include -DR_NO_REMAP -DSTRICT_R_HEADERS    -fvisibility=hidden -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c commit.c -o commit.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG -DSTATIC_LIBGIT2 -I/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/include -DR_NO_REMAP -DSTRICT_R_HEADERS    -fvisibility=hidden -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c config.c -o config.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG -DSTATIC_LIBGIT2 -I/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/include -DR_NO_REMAP -DSTRICT_R_HEADERS    -fvisibility=hidden -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c conflicts.c -o conflicts.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG -DSTATIC_LIBGIT2 -I/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/include -DR_NO_REMAP -DSTRICT_R_HEADERS    -fvisibility=hidden -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c files.c -o files.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG -DSTATIC_LIBGIT2 -I/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/include -DR_NO_REMAP -DSTRICT_R_HEADERS    -fvisibility=hidden -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c init.c -o init.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG -DSTATIC_LIBGIT2 -I/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/include -DR_NO_REMAP -DSTRICT_R_HEADERS    -fvisibility=hidden -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c merge.c -o merge.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG -DSTATIC_LIBGIT2 -I/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/include -DR_NO_REMAP -DSTRICT_R_HEADERS    -fvisibility=hidden -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rebase.c -o rebase.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG -DSTATIC_LIBGIT2 -I/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/include -DR_NO_REMAP -DSTRICT_R_HEADERS    -fvisibility=hidden -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stash.c -o stash.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG -DSTATIC_LIBGIT2 -I/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/include -DR_NO_REMAP -DSTRICT_R_HEADERS    -fvisibility=hidden -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c submodules.c -o submodules.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG -DSTATIC_LIBGIT2 -I/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/include -DR_NO_REMAP -DSTRICT_R_HEADERS    -fvisibility=hidden -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c tag.c -o tag.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG -DSTATIC_LIBGIT2 -I/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/include -DR_NO_REMAP -DSTRICT_R_HEADERS    -fvisibility=hidden -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c utils.c -o utils.o
gcc -std=gnu99 -I"/usr/share/R/include" -DNDEBUG -DSTATIC_LIBGIT2 -I/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/include -DR_NO_REMAP -DSTRICT_R_HEADERS    -fvisibility=hidden -fpic  -g -O2 -ffile-prefix-map=/build/r-base-XqSJAD/r-base-4.0.4=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c version.c -o version.o
gcc -std=gnu99 -shared -L/usr/lib/R/lib -Wl,-z,relro -o gert.so branch.o clone.o commit.o config.o conflicts.o files.o init.o merge.o rebase.o stash.o submodules.o tag.o utils.o version.o -L/tmp/Rtmpu71soX/R.INSTALL96e317742612/gert/libgit2/lib -lgit2 -lrt -lpthread -lssh2 -lssl -lcrypto -ldl -lpcre -lz -L/usr/lib/R/lib -lR
installing to /home/nicolas/R/x86_64-pc-linux-gnu-library/4.0/00LOCK-gert/00new/gert/libs
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** checking absolute paths in shared objects and dynamic libraries
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (gert)
* installing *source* package ‘gh’ ...
** package ‘gh’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (gh)
* installing *source* package ‘rcmdcheck’ ...
** package ‘rcmdcheck’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** R
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (rcmdcheck)
* installing *source* package ‘usethis’ ...
** package ‘usethis’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
*** copying figures
** building package indices
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (usethis)
* installing *source* package ‘devtools’ ...
** package ‘devtools’ correctement décompressé et sommes MD5 vérifiées
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
*** copying figures
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (devtools)

Les packages source téléchargés sont dans
	‘/tmp/RtmpsBAlPo/downloaded_packages’
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> for (f in files.sources){
+     source(f)
+ }
> data = format_data("./data/train_V2.csv", "./data/test_V2.csv")
Error in format_data("./data/train_V2.csv", "./data/test_V2.csv") : 
  impossible de trouver la fonction "format_data"
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> for (f in files.sources){
+     source(f, echo = TRUE)
+ }

> days_to_numeric = function (data) {
+     data$WeekDays[data$WeekDays=='Monday']    = 1
+     data$WeekDays[data$WeekDays=='Tuesday']   = 2
+     da .... [TRUNCATED] 

> evaluate = function(test_label, predicted_set){
+     
+     return(rmse(test_label, predicted_set))
+ }

> format_data = function(file_train, file_test) {
+ 
+     print(paste("Load and format " , file_train, sep = " "))
+     train_set = read_csv(file_tr .... [TRUNCATED] 

> fourier = function(train, test, plt = FALSE){
+     total.time = c(1:(nrow(train)+nrow(test)))
+     length(total.time)
+     train$time = total.tim .... [TRUNCATED] 

> lstm = function(train_set, train_label, test_set){
+   N = length(train_set[,'Load.1'])
+   nb_var = 17
+   y = train_label
+   x = array(train_set, .... [TRUNCATED] 

> rm(list=objects())

> graphics.off()

> #http://rwanjohi.rbind.io/2018/04/05/time-series-forecasting-using-lstm-in-r/
> 
> scale_data = function(train, test, feature_range = c(0, 1)) {
+   .... [TRUNCATED] 

> invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
+   min = scaler[1]
+   max = scaler[2]
+   t = length(scaled)
+   mins = featur .... [TRUNCATED] 

> xgboost_rte = function(train_set, train_label, test_set){
+   param = list(booster = "gblinear", objective = "reg:squarederror", eval_metric = "rmse ..." ... [TRUNCATED] 
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> for (f in files.sources){
+     source(f, verbose = TRUE)
+ }
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 1 expressions; now eval(.)ing them:
has srcrefs:
List of 1
 $ : 'srcref' int [1:8] 1 1 11 1 1 1 1 11
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e221b11ee8> 

>>>> eval(expression_nr. 1 )
		 =================

> days_to_numeric = function (data) {
+     data$WeekDays[data$WeekDays=='Monday']    = 1
+     data$WeekDays[data$WeekDays=='Tuesday']   = 2
+     da .... [TRUNCATED] 
curr.fun: symbol =
 .. after ‘expression(days_to_numeric = function (data) {’‘    data$WeekDays[data$WeekDays=='Monday']    = 1’‘    data$WeekDays[data$WeekDays=='Tuesday']   = 2’‘    data$WeekDays[data$WeekDays=='Wednesday'] = 3’‘    data$WeekDays[data$WeekDays=='Thursday']  = 4’‘    data$WeekDays[data$WeekDays=='Friday']    = 5’‘    data$WeekDays[data$WeekDays=='Saturday']  = 6’‘    data$WeekDays[data$WeekDays=='Sunday']    = 7’‘    data$WeekDays = as.numeric(data$WeekDays)’‘    return (data$WeekDays)’‘})’
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 1 expressions; now eval(.)ing them:
has srcrefs:
List of 1
 $ : 'srcref' int [1:8] 1 1 4 1 1 1 1 4
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e221942678> 

>>>> eval(expression_nr. 1 )
		 =================

> evaluate = function(test_label, predicted_set){
+     
+     return(rmse(test_label, predicted_set))
+ }
curr.fun: symbol =
 .. after ‘expression(evaluate = function(test_label, predicted_set){’‘    ’‘    return(rmse(test_label, predicted_set))’‘})’
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 1 expressions; now eval(.)ing them:
has srcrefs:
List of 1
 $ : 'srcref' int [1:8] 1 1 52 1 1 1 1 52
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e22186f9a8> 

>>>> eval(expression_nr. 1 )
		 =================

> format_data = function(file_train, file_test) {
+ 
+     print(paste("Load and format " , file_train, sep = " "))
+     train_set = read_csv(file_tr .... [TRUNCATED] 
curr.fun: symbol =
 .. after ‘expression(format_data = function(file_train, file_test) {’‘’‘    print(paste("Load and format " , file_train, sep = " "))’‘    train_set = read_csv(file_train, col_types = cols())’‘’‘    train_set$WeekDays[train_set$WeekDays == "Monday"] <- 1’‘    train_set$WeekDays[train_set$WeekDays == "Tuesday"] <- 2’‘    train_set$WeekDays[train_set$WeekDays == "Wednesday"] <- 3’‘    train_set$WeekDays[train_set$WeekDays == "Thursday"] <- 4’‘    train_set$WeekDays[train_set$WeekDays == "Friday"] <- 5’‘    train_set$WeekDays[train_set$WeekDays == "Saturday"] <- 6’‘    train_set$WeekDays[train_set$WeekDays == "Sunday"] <- 7’‘    train_set$WeekDays = as.integer(train_set$WeekDays)’‘’‘    train_set$Year = NULL’‘    train_set$Date = NULL’‘    train_label = data.matrix(train_set$Load)’‘’‘’‘    train_set$Load = NULL’‘    ’‘    train_set = data.matrix(train_set)’‘’‘’‘    print(paste("Load and format " , file_test, sep = " "))    ’‘    test_set = read_csv(file_test, col_types = cols())’‘’‘    test_set$WeekDays[test_set$WeekDays == "Monday"] <- 1’‘    test_set$WeekDays[test_set$WeekDays == "Tuesday"] <- 2’‘    test_set$WeekDays[test_set$WeekDays == "Wednesday"] <- 3’‘    test_set$WeekDays[test_set$WeekDays == "Thursday"] <- 4’‘    test_set$WeekDays[test_set$WeekDays == "Friday"] <- 5’‘    test_set$WeekDays[test_set$WeekDays == "Saturday"] <- 6’‘    test_set$WeekDays[test_set$WeekDays == "Sunday"] <- 7’‘    test_set$WeekDays = as.integer(test_set$WeekDays)’‘’‘    test_label = test_set$Load.1’‘    tmp = test_set$Load.1’‘    for (i in c(1:(length(tmp)-1))){’‘        test_label[i] = tmp[i+1]’‘    }’‘    ’‘    test_set$Year = NULL’‘    test_set$Date = NULL’‘    test_set$Id = NULL’‘    test_set$Usage = NULL’‘    test_set = data.matrix(test_set)’‘    ’‘’‘    test_set = data.matrix(test_set)’‘    return(list("train_set" = train_set, "train_label" = train_label, "test_set" = test_set, "test_label" = test_label))’‘})’
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 1 expressions; now eval(.)ing them:
has srcrefs:
List of 1
 $ : 'srcref' int [1:8] 1 1 36 1 1 1 1 36
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e2217808a0> 

>>>> eval(expression_nr. 1 )
		 =================

> fourier = function(train, test, plt = FALSE){
+     total.time = c(1:(nrow(train)+nrow(test)))
+     length(total.time)
+     train$time = total.tim .... [TRUNCATED] 
curr.fun: symbol =
 .. after ‘expression(fourier = function(train, test, plt = FALSE){’‘    total.time = c(1:(nrow(train)+nrow(test)))’‘    length(total.time)’‘    train$time = total.time[1:nrow(train)]’‘    test$time = tail(total.time,nrow(test))’‘’‘    fourier.make.matrix = function (t, k, period) {’‘        w = 2*pi/period’‘        ret = cbind(cos(w*t), sin(w*t))’‘        ’‘        for(i in c(2:K))’‘        {’‘            ret = cbind(ret, cos(i*w*t), sin(i*w*t))’‘        }’‘        return (ret)’‘    }’‘’‘    K = 5; period = 365’‘    fourier.train = fourier.make.matrix(train$time, K, period)’‘    fourier.test = fourier.make.matrix(test$time, K, period)’‘’‘    fourier.train.df = data.frame(train$Load,fourier.train)’‘    fourier.test.df = data.frame(fourier.test)’‘’‘    reg = lm(train.Load ~., data=fourier.train.df)’‘’‘    pred.fourier = predict(reg, newdata=fourier.test.df)’‘    total.fourier = c(reg$fitted,pred.fourier)’‘    if (plt){’‘        par(mfrow=c(1,1))’‘        plot(train$Load,type='l', xlim=c(0,length(total.time)))’‘        lines(reg$fitted,col='red', lwd=2)’‘        lines(test$time,pred.fourier,col='green', lwd=2)’‘    }’‘    return(pred.fourier)’‘})’
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 1 expressions; now eval(.)ing them:
has srcrefs:
List of 1
 $ : 'srcref' int [1:8] 1 1 26 1 1 1 1 26
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e2216b1dd0> 

>>>> eval(expression_nr. 1 )
		 =================

> lstm = function(train_set, train_label, test_set){
+   N = length(train_set[,'Load.1'])
+   nb_var = 17
+   y = train_label
+   x = array(train_set, .... [TRUNCATED] 
curr.fun: symbol =
 .. after ‘expression(lstm = function(train_set, train_label, test_set){’‘  N = length(train_set[,'Load.1'])’‘  nb_var = 17’‘  y = train_label’‘  x = array(train_set, dim = c(N, nb_var, 1))’‘  x_test = array(test_set,dim = c(length(test_set[,'Load.1']), nb_var, 1))’‘  print("step1")’‘  model = keras_model_sequential() %>%   ’‘    #layer_lstm(units=128, input_shape=c(nb_var, 1), activation="relu", return_sequences = TRUE) %>% ’‘    layer_dense(units=64, activation = "relu") %>%  ’‘    layer_dense(units=32) %>%  ’‘    layer_dense(units=1, activation = "linear")’‘  print("step2")’‘  model %>% compile(loss = 'mse',’‘                    optimizer = 'adam',’‘                    metrics = list("mean_absolute_error")’‘  )’‘  print("step3")’‘  model %>% fit(x,y, epochs=100, batch_size=32, shuffle = FALSE)’‘  print("step4")’‘  print(length(x_test))’‘  pred = model %>% predict(x_test)’‘  print("step5")’‘  return(pred)’‘    ’‘})’
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 2 expressions; now eval(.)ing them:
has srcrefs:
List of 2
 $ : 'srcref' int [1:8] 1 1 1 18 1 18 1 1
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e2215b8608> 
 $ : 'srcref' int [1:8] 2 1 2 14 1 14 2 2
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e2215b8608> 

>>>> eval(expression_nr. 1 )
		 =================

> rm(list=objects())
curr.fun: symbol rm
 .. after ‘expression(rm(list=objects()))’

>>>> eval(expression_nr. 2 )
		 =================

> graphics.off()
curr.fun: symbol graphics.off
 .. after ‘expression(graphics.off())’
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 2 expressions; now eval(.)ing them:
has srcrefs:
List of 2
 $ : 'srcref' int [1:8] 3 1 15 1 1 1 3 15
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e22143cc98> 
 $ : 'srcref' int [1:8] 17 1 31 1 1 1 17 31
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e22143cc98> 

>>>> eval(expression_nr. 1 )
		 =================

> #http://rwanjohi.rbind.io/2018/04/05/time-series-forecasting-using-lstm-in-r/
> 
> scale_data = function(train, test, feature_range = c(0, 1)) {
+   .... [TRUNCATED] 
curr.fun: symbol =
 .. after ‘expression(scale_data = function(train, test, feature_range = c(0, 1)) {’‘  x = train’‘  fr_min = feature_range[1]’‘  fr_max = feature_range[2]’‘  std_train = ((x - min(x) ) / (max(x) - min(x)  ))’‘  std_test  = ((test - min(x) ) / (max(x) - min(x)  ))’‘  ’‘  scaled_train = std_train *(fr_max -fr_min) + fr_min’‘  scaled_test = std_test *(fr_max -fr_min) + fr_min’‘  ’‘  return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )’‘  ’‘})’

>>>> eval(expression_nr. 2 )
		 =================

> invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
+   min = scaler[1]
+   max = scaler[2]
+   t = length(scaled)
+   mins = featur .... [TRUNCATED] 
curr.fun: symbol =
 .. after ‘expression(invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){’‘  min = scaler[1]’‘  max = scaler[2]’‘  t = length(scaled)’‘  mins = feature_range[1]’‘  maxs = feature_range[2]’‘  inverted_dfs = numeric(t)’‘  ’‘  for( i in 1:t){’‘    X = (scaled[i]- mins)/(maxs - mins)’‘    rawValues = X *(max - min) + min’‘    inverted_dfs[i] <- rawValues’‘  }’‘  return(inverted_dfs)’‘})’
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 1 expressions; now eval(.)ing them:
has srcrefs:
List of 1
 $ : 'srcref' int [1:8] 1 1 11 1 1 1 1 11
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e2212c14d8> 

>>>> eval(expression_nr. 1 )
		 =================

> xgboost_rte = function(train_set, train_label, test_set){
+   param = list(booster = "gblinear", objective = "reg:squarederror", eval_metric = "rmse ..." ... [TRUNCATED] 
curr.fun: symbol =
 .. after ‘expression(xgboost_rte = function(train_set, train_label, test_set){’‘  param = list(booster = "gblinear", objective = "reg:squarederror", eval_metric = "rmse", lambda = 0.0003, alpha = 0.0003, nthread = 2, eta = 0.1)’‘  ’‘  print("Model : XGBOOST")’‘  ’‘  xgbmodel = xgboost(data = train_set, label = train_label, nrounds = 200, params = param, verbose = 0)’‘  ’‘  pred = predict(xgbmodel, test_set)’‘  ’‘  return(pred)’‘})’
> source("toto")
Error in file(filename, "r", encoding = encoding) : 
  impossible d'ouvrir la connexion
De plus : Warning message:
In file(filename, "r", encoding = encoding) :
  impossible d'ouvrir le fichier 'toto' : Aucun fichier ou dossier de ce type
> rm(list=objects())
> graphics.off()
> libs.to.load = c("icesTAF", "tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
     icesTAF    tidyverse    lubridate       ranger       pracma      Metrics 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
        mgcv        keras       visreg        caret         mc2d        opera 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       abind randomForest   tensorflow 
        TRUE         TRUE         TRUE 
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> for (f in files.sources){
+     source(f, verbose = TRUE)
+ }
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 1 expressions; now eval(.)ing them:
has srcrefs:
List of 1
 $ : 'srcref' int [1:8] 1 1 11 1 1 1 1 11
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e2216dc4b0> 

>>>> eval(expression_nr. 1 )
		 =================

> days_to_numeric = function (data) {
+     data$WeekDays[data$WeekDays=='Monday']    = 1
+     data$WeekDays[data$WeekDays=='Tuesday']   = 2
+     da .... [TRUNCATED] 
curr.fun: symbol =
 .. after ‘expression(days_to_numeric = function (data) {’‘    data$WeekDays[data$WeekDays=='Monday']    = 1’‘    data$WeekDays[data$WeekDays=='Tuesday']   = 2’‘    data$WeekDays[data$WeekDays=='Wednesday'] = 3’‘    data$WeekDays[data$WeekDays=='Thursday']  = 4’‘    data$WeekDays[data$WeekDays=='Friday']    = 5’‘    data$WeekDays[data$WeekDays=='Saturday']  = 6’‘    data$WeekDays[data$WeekDays=='Sunday']    = 7’‘    data$WeekDays = as.numeric(data$WeekDays)’‘    return (data$WeekDays)’‘})’
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 1 expressions; now eval(.)ing them:
has srcrefs:
List of 1
 $ : 'srcref' int [1:8] 1 1 4 1 1 1 1 4
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e2215e4490> 

>>>> eval(expression_nr. 1 )
		 =================

> evaluate = function(test_label, predicted_set){
+     
+     return(rmse(test_label, predicted_set))
+ }
curr.fun: symbol =
 .. after ‘expression(evaluate = function(test_label, predicted_set){’‘    ’‘    return(rmse(test_label, predicted_set))’‘})’
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 1 expressions; now eval(.)ing them:
has srcrefs:
List of 1
 $ : 'srcref' int [1:8] 1 1 52 1 1 1 1 52
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e2215181d0> 

>>>> eval(expression_nr. 1 )
		 =================

> format_data = function(file_train, file_test) {
+ 
+     print(paste("Load and format " , file_train, sep = " "))
+     train_set = read_csv(file_tr .... [TRUNCATED] 
curr.fun: symbol =
 .. after ‘expression(format_data = function(file_train, file_test) {’‘’‘    print(paste("Load and format " , file_train, sep = " "))’‘    train_set = read_csv(file_train, col_types = cols())’‘’‘    train_set$WeekDays[train_set$WeekDays == "Monday"] <- 1’‘    train_set$WeekDays[train_set$WeekDays == "Tuesday"] <- 2’‘    train_set$WeekDays[train_set$WeekDays == "Wednesday"] <- 3’‘    train_set$WeekDays[train_set$WeekDays == "Thursday"] <- 4’‘    train_set$WeekDays[train_set$WeekDays == "Friday"] <- 5’‘    train_set$WeekDays[train_set$WeekDays == "Saturday"] <- 6’‘    train_set$WeekDays[train_set$WeekDays == "Sunday"] <- 7’‘    train_set$WeekDays = as.integer(train_set$WeekDays)’‘’‘    train_set$Year = NULL’‘    train_set$Date = NULL’‘    train_label = data.matrix(train_set$Load)’‘’‘’‘    train_set$Load = NULL’‘    ’‘    train_set = data.matrix(train_set)’‘’‘’‘    print(paste("Load and format " , file_test, sep = " "))    ’‘    test_set = read_csv(file_test, col_types = cols())’‘’‘    test_set$WeekDays[test_set$WeekDays == "Monday"] <- 1’‘    test_set$WeekDays[test_set$WeekDays == "Tuesday"] <- 2’‘    test_set$WeekDays[test_set$WeekDays == "Wednesday"] <- 3’‘    test_set$WeekDays[test_set$WeekDays == "Thursday"] <- 4’‘    test_set$WeekDays[test_set$WeekDays == "Friday"] <- 5’‘    test_set$WeekDays[test_set$WeekDays == "Saturday"] <- 6’‘    test_set$WeekDays[test_set$WeekDays == "Sunday"] <- 7’‘    test_set$WeekDays = as.integer(test_set$WeekDays)’‘’‘    test_label = test_set$Load.1’‘    tmp = test_set$Load.1’‘    for (i in c(1:(length(tmp)-1))){’‘        test_label[i] = tmp[i+1]’‘    }’‘    ’‘    test_set$Year = NULL’‘    test_set$Date = NULL’‘    test_set$Id = NULL’‘    test_set$Usage = NULL’‘    test_set = data.matrix(test_set)’‘    ’‘’‘    test_set = data.matrix(test_set)’‘    return(list("train_set" = train_set, "train_label" = train_label, "test_set" = test_set, "test_label" = test_label))’‘})’
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 1 expressions; now eval(.)ing them:
has srcrefs:
List of 1
 $ : 'srcref' int [1:8] 1 1 36 1 1 1 1 36
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e22143c4f0> 

>>>> eval(expression_nr. 1 )
		 =================

> fourier = function(train, test, plt = FALSE){
+     total.time = c(1:(nrow(train)+nrow(test)))
+     length(total.time)
+     train$time = total.tim .... [TRUNCATED] 
curr.fun: symbol =
 .. after ‘expression(fourier = function(train, test, plt = FALSE){’‘    total.time = c(1:(nrow(train)+nrow(test)))’‘    length(total.time)’‘    train$time = total.time[1:nrow(train)]’‘    test$time = tail(total.time,nrow(test))’‘’‘    fourier.make.matrix = function (t, k, period) {’‘        w = 2*pi/period’‘        ret = cbind(cos(w*t), sin(w*t))’‘        ’‘        for(i in c(2:K))’‘        {’‘            ret = cbind(ret, cos(i*w*t), sin(i*w*t))’‘        }’‘        return (ret)’‘    }’‘’‘    K = 5; period = 365’‘    fourier.train = fourier.make.matrix(train$time, K, period)’‘    fourier.test = fourier.make.matrix(test$time, K, period)’‘’‘    fourier.train.df = data.frame(train$Load,fourier.train)’‘    fourier.test.df = data.frame(fourier.test)’‘’‘    reg = lm(train.Load ~., data=fourier.train.df)’‘’‘    pred.fourier = predict(reg, newdata=fourier.test.df)’‘    total.fourier = c(reg$fitted,pred.fourier)’‘    if (plt){’‘        par(mfrow=c(1,1))’‘        plot(train$Load,type='l', xlim=c(0,length(total.time)))’‘        lines(reg$fitted,col='red', lwd=2)’‘        lines(test$time,pred.fourier,col='green', lwd=2)’‘    }’‘    return(pred.fourier)’‘})’
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 1 expressions; now eval(.)ing them:
has srcrefs:
List of 1
 $ : 'srcref' int [1:8] 1 1 26 1 1 1 1 26
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e22135f180> 

>>>> eval(expression_nr. 1 )
		 =================

> lstm = function(train_set, train_label, test_set){
+   N = length(train_set[,'Load.1'])
+   nb_var = 17
+   y = train_label
+   x = array(train_set, .... [TRUNCATED] 
curr.fun: symbol =
 .. after ‘expression(lstm = function(train_set, train_label, test_set){’‘  N = length(train_set[,'Load.1'])’‘  nb_var = 17’‘  y = train_label’‘  x = array(train_set, dim = c(N, nb_var, 1))’‘  x_test = array(test_set,dim = c(length(test_set[,'Load.1']), nb_var, 1))’‘  print("step1")’‘  model = keras_model_sequential() %>%   ’‘    #layer_lstm(units=128, input_shape=c(nb_var, 1), activation="relu", return_sequences = TRUE) %>% ’‘    layer_dense(units=64, activation = "relu") %>%  ’‘    layer_dense(units=32) %>%  ’‘    layer_dense(units=1, activation = "linear")’‘  print("step2")’‘  model %>% compile(loss = 'mse',’‘                    optimizer = 'adam',’‘                    metrics = list("mean_absolute_error")’‘  )’‘  print("step3")’‘  model %>% fit(x,y, epochs=100, batch_size=32, shuffle = FALSE)’‘  print("step4")’‘  print(length(x_test))’‘  pred = model %>% predict(x_test)’‘  print("step5")’‘  return(pred)’‘    ’‘})’
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 2 expressions; now eval(.)ing them:
has srcrefs:
List of 2
 $ : 'srcref' int [1:8] 1 1 1 18 1 18 1 1
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e2212930b0> 
 $ : 'srcref' int [1:8] 2 1 2 14 1 14 2 2
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e2212930b0> 

>>>> eval(expression_nr. 1 )
		 =================

> rm(list=objects())
curr.fun: symbol rm
 .. after ‘expression(rm(list=objects()))’

>>>> eval(expression_nr. 2 )
		 =================

> graphics.off()
curr.fun: symbol graphics.off
 .. after ‘expression(graphics.off())’
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 2 expressions; now eval(.)ing them:
has srcrefs:
List of 2
 $ : 'srcref' int [1:8] 3 1 15 1 1 1 3 15
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e2211128d8> 
 $ : 'srcref' int [1:8] 17 1 31 1 1 1 17 31
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e2211128d8> 

>>>> eval(expression_nr. 1 )
		 =================

> #http://rwanjohi.rbind.io/2018/04/05/time-series-forecasting-using-lstm-in-r/
> 
> scale_data = function(train, test, feature_range = c(0, 1)) {
+   .... [TRUNCATED] 
curr.fun: symbol =
 .. after ‘expression(scale_data = function(train, test, feature_range = c(0, 1)) {’‘  x = train’‘  fr_min = feature_range[1]’‘  fr_max = feature_range[2]’‘  std_train = ((x - min(x) ) / (max(x) - min(x)  ))’‘  std_test  = ((test - min(x) ) / (max(x) - min(x)  ))’‘  ’‘  scaled_train = std_train *(fr_max -fr_min) + fr_min’‘  scaled_test = std_test *(fr_max -fr_min) + fr_min’‘  ’‘  return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )’‘  ’‘})’

>>>> eval(expression_nr. 2 )
		 =================

> invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){
+   min = scaler[1]
+   max = scaler[2]
+   t = length(scaled)
+   mins = featur .... [TRUNCATED] 
curr.fun: symbol =
 .. after ‘expression(invert_scaling = function(scaled, scaler, feature_range = c(0, 1)){’‘  min = scaler[1]’‘  max = scaler[2]’‘  t = length(scaled)’‘  mins = feature_range[1]’‘  maxs = feature_range[2]’‘  inverted_dfs = numeric(t)’‘  ’‘  for( i in 1:t){’‘    X = (scaled[i]- mins)/(maxs - mins)’‘    rawValues = X *(max - min) + min’‘    inverted_dfs[i] <- rawValues’‘  }’‘  return(inverted_dfs)’‘})’
'envir' chosen:<environment: R_GlobalEnv>
l'encodage 'encoding = "native.enc" est sélectionné
--> parsed 1 expressions; now eval(.)ing them:
has srcrefs:
List of 1
 $ : 'srcref' int [1:8] 1 1 11 1 1 1 1 11
  ..- attr(*, "srcfile")=Classes 'srcfilecopy', 'srcfile' <environment: 0x55e220f0f510> 

>>>> eval(expression_nr. 1 )
		 =================

> xgboost_rte = function(train_set, train_label, test_set){
+   param = list(booster = "gblinear", objective = "reg:squarederror", eval_metric = "rmse ..." ... [TRUNCATED] 
curr.fun: symbol =
 .. after ‘expression(xgboost_rte = function(train_set, train_label, test_set){’‘  param = list(booster = "gblinear", objective = "reg:squarederror", eval_metric = "rmse", lambda = 0.0003, alpha = 0.0003, nthread = 2, eta = 0.1)’‘  ’‘  print("Model : XGBOOST")’‘  ’‘  xgbmodel = xgboost(data = train_set, label = train_label, nrounds = 200, params = param, verbose = 0)’‘  ’‘  pred = predict(xgbmodel, test_set)’‘  ’‘  return(pred)’‘})’
> model = "lstm"
> data = format_data("./data/train_V2.csv", "./data/test_V2.csv")
Error in format_data("./data/train_V2.csv", "./data/test_V2.csv") : 
  impossible de trouver la fonction "format_data"
> sessionInfo()
R version 4.0.4 (2021-02-15)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Debian GNU/Linux bullseye/sid

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3
LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.13.so

locale:
 [1] LC_CTYPE=fr_FR.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=fr_FR.UTF-8        LC_COLLATE=fr_FR.UTF-8    
 [5] LC_MONETARY=fr_FR.UTF-8    LC_MESSAGES=fr_FR.UTF-8   
 [7] LC_PAPER=fr_FR.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=fr_FR.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] tensorflow_2.2.0    randomForest_4.6-14 abind_1.4-5        
 [4] opera_1.1           mc2d_0.1-18         mvtnorm_1.1-1      
 [7] caret_6.0-86        lattice_0.20-41     visreg_2.7.0       
[10] keras_2.3.0.0       mgcv_1.8-34         nlme_3.1-152       
[13] Metrics_0.1.4       pracma_2.3.3        ranger_0.12.1      
[16] lubridate_1.7.9.2   forcats_0.5.1       stringr_1.4.0      
[19] dplyr_1.0.3         purrr_0.3.4         readr_1.4.0        
[22] tidyr_1.1.2         tibble_3.0.5        ggplot2_3.3.3      
[25] tidyverse_1.3.0     icesTAF_3.6.0      

loaded via a namespace (and not attached):
 [1] fs_1.5.0             httr_1.4.2           tools_4.0.4         
 [4] backports_1.2.1      R6_2.5.0             rpart_4.1-15        
 [7] DBI_1.1.1            colorspace_2.0-0     nnet_7.3-15         
[10] withr_2.4.1          tidyselect_1.1.0     compiler_4.0.4      
[13] cli_2.2.0            rvest_0.3.6          xml2_1.3.2          
[16] scales_1.1.1         tfruns_1.4           base64enc_0.1-3     
[19] pkgconfig_2.0.3      dbplyr_2.0.0         rlang_0.4.10        
[22] readxl_1.3.1         rstudioapi_0.13      generics_0.1.0      
[25] jsonlite_1.7.2       ModelMetrics_1.2.2.2 magrittr_2.0.1      
[28] Matrix_1.3-2         Rcpp_1.0.6           munsell_0.5.0       
[31] fansi_0.4.2          reticulate_1.18      lifecycle_0.2.0     
[34] pROC_1.17.0.1        stringi_1.5.3        whisker_0.4         
[37] MASS_7.3-53.1        plyr_1.8.6           recipes_0.1.15      
[40] grid_4.0.4           crayon_1.3.4         haven_2.3.1         
[43] splines_4.0.4        hms_1.0.0            zeallot_0.1.0       
[46] knitr_1.31           ps_1.5.0             pillar_1.4.7        
[49] stats4_4.0.4         reshape2_1.4.4       codetools_0.2-18    
[52] reprex_1.0.0         glue_1.4.2           data.table_1.13.6   
[55] modelr_0.1.8         vctrs_0.3.6          foreach_1.5.1       
[58] cellranger_1.1.0     gtable_0.3.0         assertthat_0.2.1    
[61] xfun_0.20            gower_0.2.2          prodlim_2019.11.13  
[64] broom_0.7.3          roxygen2_7.1.1       class_7.3-18        
[67] survival_3.2-7       timeDate_3043.102    iterators_1.0.13    
[70] lava_1.6.8.1         ellipsis_0.3.1       ipred_0.9-9         
> rm(list=objects())
> graphics.off()
> libs.to.load = c("icesTAF", "tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
     icesTAF    tidyverse    lubridate       ranger       pracma      Metrics 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
        mgcv        keras       visreg        caret         mc2d        opera 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       abind randomForest   tensorflow 
        TRUE         TRUE         TRUE 
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> for (f in files.sources){
+     source(f)
+ }
> model = "lstm"
> data = format_data("./data/train_V2.csv", "./data/test_V2.csv")
[1] "Load and format  ./data/train_V2.csv"
[1] "Load and format  ./data/test_V2.csv"
> rm(list=objects())
> graphics.off()
> libs.to.load = c("icesTAF", "tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
     icesTAF    tidyverse    lubridate       ranger       pracma      Metrics 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
        mgcv        keras       visreg        caret         mc2d        opera 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       abind randomForest   tensorflow 
        TRUE         TRUE         TRUE 
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> sapply(files.sources, source)
$days_to_numeric.r
$days_to_numeric.r$value
function (data) {
    data$WeekDays[data$WeekDays=='Monday']    = 1
    data$WeekDays[data$WeekDays=='Tuesday']   = 2
    data$WeekDays[data$WeekDays=='Wednesday'] = 3
    data$WeekDays[data$WeekDays=='Thursday']  = 4
    data$WeekDays[data$WeekDays=='Friday']    = 5
    data$WeekDays[data$WeekDays=='Saturday']  = 6
    data$WeekDays[data$WeekDays=='Sunday']    = 7
    data$WeekDays = as.numeric(data$WeekDays)
    return (data$WeekDays)
}

$days_to_numeric.r$visible
[1] FALSE


$evaluate.r
$evaluate.r$value
function(test_label, predicted_set){
    
    return(rmse(test_label, predicted_set))
}

$evaluate.r$visible
[1] FALSE


$format_data.r
$format_data.r$value
function(file_train, file_test) {

    print(paste("Load and format " , file_train, sep = " "))
    train_set = read_csv(file_train, col_types = cols())

    train_set$WeekDays[train_set$WeekDays == "Monday"] <- 1
    train_set$WeekDays[train_set$WeekDays == "Tuesday"] <- 2
    train_set$WeekDays[train_set$WeekDays == "Wednesday"] <- 3
    train_set$WeekDays[train_set$WeekDays == "Thursday"] <- 4
    train_set$WeekDays[train_set$WeekDays == "Friday"] <- 5
    train_set$WeekDays[train_set$WeekDays == "Saturday"] <- 6
    train_set$WeekDays[train_set$WeekDays == "Sunday"] <- 7
    train_set$WeekDays = as.integer(train_set$WeekDays)

    train_set$Year = NULL
    train_set$Date = NULL
    train_label = data.matrix(train_set$Load)


    train_set$Load = NULL
    
    train_set = data.matrix(train_set)


    print(paste("Load and format " , file_test, sep = " "))    
    test_set = read_csv(file_test, col_types = cols())

    test_set$WeekDays[test_set$WeekDays == "Monday"] <- 1
    test_set$WeekDays[test_set$WeekDays == "Tuesday"] <- 2
    test_set$WeekDays[test_set$WeekDays == "Wednesday"] <- 3
    test_set$WeekDays[test_set$WeekDays == "Thursday"] <- 4
    test_set$WeekDays[test_set$WeekDays == "Friday"] <- 5
    test_set$WeekDays[test_set$WeekDays == "Saturday"] <- 6
    test_set$WeekDays[test_set$WeekDays == "Sunday"] <- 7
    test_set$WeekDays = as.integer(test_set$WeekDays)

    test_label = test_set$Load.1
    tmp = test_set$Load.1
    for (i in c(1:(length(tmp)-1))){
        test_label[i] = tmp[i+1]
    }
    
    test_set$Year = NULL
    test_set$Date = NULL
    test_set$Id = NULL
    test_set$Usage = NULL
    test_set = data.matrix(test_set)
    

    test_set = data.matrix(test_set)
    return(list("train_set" = train_set, "train_label" = train_label, "test_set" = test_set, "test_label" = test_label))
}

$format_data.r$visible
[1] FALSE


$fourier.r
$fourier.r$value
function(train, test, plt = FALSE){
    total.time = c(1:(nrow(train)+nrow(test)))
    length(total.time)
    train$time = total.time[1:nrow(train)]
    test$time = tail(total.time,nrow(test))

    fourier.make.matrix = function (t, k, period) {
        w = 2*pi/period
        ret = cbind(cos(w*t), sin(w*t))
        
        for(i in c(2:K))
        {
            ret = cbind(ret, cos(i*w*t), sin(i*w*t))
        }
        return (ret)
    }

    K = 5; period = 365
    fourier.train = fourier.make.matrix(train$time, K, period)
    fourier.test = fourier.make.matrix(test$time, K, period)

    fourier.train.df = data.frame(train$Load,fourier.train)
    fourier.test.df = data.frame(fourier.test)

    reg = lm(train.Load ~., data=fourier.train.df)

    pred.fourier = predict(reg, newdata=fourier.test.df)
    total.fourier = c(reg$fitted,pred.fourier)
    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(reg$fitted,col='red', lwd=2)
        lines(test$time,pred.fourier,col='green', lwd=2)
    }
    return(pred.fourier)
}

$fourier.r$visible
[1] FALSE


$lstm.r
$lstm.r$value
function(train_set, train_label, test_set){
  N = length(train_set[,'Load.1'])
  nb_var = 17
  y = train_label
  x = array(train_set, dim = c(N, nb_var, 1))
  x_test = array(test_set,dim = c(length(test_set[,'Load.1']), nb_var, 1))
  print("step1")
  model = keras_model_sequential() %>%   
    #layer_lstm(units=128, input_shape=c(nb_var, 1), activation="relu", return_sequences = TRUE) %>% 
    layer_dense(units=64, activation = "relu") %>%  
    layer_dense(units=32) %>%  
    layer_dense(units=1, activation = "linear")
  print("step2")
  model %>% compile(loss = 'mse',
                    optimizer = 'adam',
                    metrics = list("mean_absolute_error")
  )
  print("step3")
  model %>% fit(x,y, epochs=100, batch_size=32, shuffle = FALSE)
  print("step4")
  print(length(x_test))
  pred = model %>% predict(x_test)
  print("step5")
  return(pred)
    
}

$lstm.r$visible
[1] FALSE


$main.r
NULL

$scale.r
$scale.r$value
function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

$scale.r$visible
[1] FALSE


$xgboost.r
$xgboost.r$value
function(train_set, train_label, test_set){
  param = list(booster = "gblinear", objective = "reg:squarederror", eval_metric = "rmse", lambda = 0.0003, alpha = 0.0003, nthread = 2, eta = 0.1)
  
  print("Model : XGBOOST")
  
  xgbmodel = xgboost(data = train_set, label = train_label, nrounds = 200, params = param, verbose = 0)
  
  pred = predict(xgbmodel, test_set)
  
  return(pred)
}

$xgboost.r$visible
[1] FALSE


> model = "lstm"
> data = format_data("./data/train_V2.csv", "./data/test_V2.csv")
[1] "Load and format  ./data/train_V2.csv"
[1] "Load and format  ./data/test_V2.csv"
> setrain_set = data$train_set
> test_set = data$test_set
> train_label = data$train_label
> test_label = data$test_label
> if (model == "xgboost"){
+   pred = xgboost_rte(train_set, train_label, test_set)
+ } else if(model == "lstm"){
+   pred = lstm(train_set, train_label, test_set)
+ }
Error in lstm(train_set, train_label, test_set) : 
  objet 'train_set' introuvable
> train_set = data$train_set
> test_set = data$test_set
> train_label = data$train_label
> test_label = data$test_label
> if (model == "xgboost"){
+   pred = xgboost_rte(train_set, train_label, test_set)
+ } else if(model == "lstm"){
+   pred = lstm(train_set, train_label, test_set)
+ }
[1] "step1"
[1] "step2"
2021-02-24 16:36:34.795848: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: Ne peut ouvrir le fichier d'objet partagé: Aucun fichier ou dossier de ce type; LD_LIBRARY_PATH: /home/nicolas/.local/share/r-miniconda/envs/r-reticulate/lib:/usr/lib/R/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server
2021-02-24 16:36:34.795868: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)
2021-02-24 16:36:34.795885: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: requiem
2021-02-24 16:36:34.795891: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: requiem
2021-02-24 16:36:34.795933: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
2021-02-24 16:36:34.796125: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.39.0
2021-02-24 16:36:34.796318: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2021-02-24 16:36:34.826599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2499950000 Hz
2021-02-24 16:36:34.827467: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f19f0000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-02-24 16:36:34.827490: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
[1] "step3"
Epoch 1/100
95/95 [==============================] - 0s 1ms/step - loss: 2772186880.0000 - mean_absolute_error: 49378.9961
95/95 [==============================] - 0s 1ms/step - loss: 2772186880.0000 - mean_absolute_error: 49378.9961
Epoch 2/100
95/95 [==============================] - 0s 956us/step - loss: 2740427008.0000 - mean_absolute_error: 48555.8164
95/95 [==============================] - 0s 1ms/step - loss: 2740427008.0000 - mean_absolute_error: 48555.8164  
Epoch 3/100
95/95 [==============================] - 0s 970us/step - loss: 2739772416.0000 - mean_absolute_error: 48561.8242
95/95 [==============================] - 0s 988us/step - loss: 2739772416.0000 - mean_absolute_error: 48561.8242
Epoch 4/100
95/95 [==============================] - 0s 990us/step - loss: 2738284800.0000 - mean_absolute_error: 48556.8867
95/95 [==============================] - 0s 1ms/step - loss: 2738284800.0000 - mean_absolute_error: 48556.8867  
Epoch 5/100
95/95 [==============================] - 0s 974us/step - loss: 2736050176.0000 - mean_absolute_error: 48548.9297
95/95 [==============================] - 0s 993us/step - loss: 2736050176.0000 - mean_absolute_error: 48548.9297
Epoch 6/100
95/95 [==============================] - 0s 943us/step - loss: 2732848384.0000 - mean_absolute_error: 48532.6719
95/95 [==============================] - 0s 962us/step - loss: 2732848384.0000 - mean_absolute_error: 48532.6719
Epoch 7/100
95/95 [==============================] - 0s 944us/step - loss: 2728204544.0000 - mean_absolute_error: 48502.5820
95/95 [==============================] - 0s 964us/step - loss: 2728204544.0000 - mean_absolute_error: 48502.5820
Epoch 8/100
95/95 [==============================] - 0s 943us/step - loss: 2721454592.0000 - mean_absolute_error: 48444.3711
95/95 [==============================] - 0s 963us/step - loss: 2721454592.0000 - mean_absolute_error: 48444.3711
Epoch 9/100
95/95 [==============================] - 0s 942us/step - loss: 2710205952.0000 - mean_absolute_error: 48302.9102
95/95 [==============================] - 0s 961us/step - loss: 2710205952.0000 - mean_absolute_error: 48302.9102
Epoch 10/100
95/95 [==============================] - 0s 982us/step - loss: 2692740352.0000 - mean_absolute_error: 48139.8750
95/95 [==============================] - 0s 1ms/step - loss: 2692740352.0000 - mean_absolute_error: 48139.8750  
Epoch 11/100
95/95 [==============================] - 0s 1ms/step - loss: 2665235712.0000 - mean_absolute_error: 47926.9688
95/95 [==============================] - 0s 1ms/step - loss: 2665235712.0000 - mean_absolute_error: 47926.9688
Epoch 12/100
95/95 [==============================] - 0s 933us/step - loss: 2624095744.0000 - mean_absolute_error: 47578.6055
95/95 [==============================] - 0s 951us/step - loss: 2624095744.0000 - mean_absolute_error: 47578.6055
Epoch 13/100
95/95 [==============================] - 0s 934us/step - loss: 2563500288.0000 - mean_absolute_error: 46989.3828
95/95 [==============================] - 0s 953us/step - loss: 2563500288.0000 - mean_absolute_error: 46989.3828
Epoch 14/100
95/95 [==============================] - 0s 957us/step - loss: 2482389248.0000 - mean_absolute_error: 46359.7500
95/95 [==============================] - 0s 975us/step - loss: 2482389248.0000 - mean_absolute_error: 46359.7500
Epoch 15/100
95/95 [==============================] - 0s 956us/step - loss: 2385711360.0000 - mean_absolute_error: 45520.3008
95/95 [==============================] - 0s 974us/step - loss: 2385711360.0000 - mean_absolute_error: 45520.3008
Epoch 16/100
95/95 [==============================] - 0s 1ms/step - loss: 2270988032.0000 - mean_absolute_error: 44578.0781
95/95 [==============================] - 0s 1ms/step - loss: 2270988032.0000 - mean_absolute_error: 44578.0781
Epoch 17/100
95/95 [==============================] - 0s 934us/step - loss: 2128844928.0000 - mean_absolute_error: 42981.8398
95/95 [==============================] - 0s 953us/step - loss: 2128844928.0000 - mean_absolute_error: 42981.8398
Epoch 18/100
95/95 [==============================] - 0s 936us/step - loss: 1972098432.0000 - mean_absolute_error: 41327.9922
95/95 [==============================] - 0s 954us/step - loss: 1972098432.0000 - mean_absolute_error: 41327.9922
Epoch 19/100
95/95 [==============================] - 0s 929us/step - loss: 1804777856.0000 - mean_absolute_error: 39508.2109
95/95 [==============================] - 0s 947us/step - loss: 1804777856.0000 - mean_absolute_error: 39508.2109
Epoch 20/100
95/95 [==============================] - 0s 932us/step - loss: 1624414720.0000 - mean_absolute_error: 37319.6055
95/95 [==============================] - 0s 952us/step - loss: 1624414720.0000 - mean_absolute_error: 37319.6055
Epoch 21/100
95/95 [==============================] - 0s 932us/step - loss: 1455207040.0000 - mean_absolute_error: 35456.3242
95/95 [==============================] - 0s 950us/step - loss: 1455207040.0000 - mean_absolute_error: 35456.3242
Epoch 22/100
95/95 [==============================] - 0s 1ms/step - loss: 1274213632.0000 - mean_absolute_error: 33031.5312
95/95 [==============================] - 0s 1ms/step - loss: 1274213632.0000 - mean_absolute_error: 33031.5312
Epoch 23/100
95/95 [==============================] - 0s 959us/step - loss: 1095678848.0000 - mean_absolute_error: 30168.0020
95/95 [==============================] - 0s 977us/step - loss: 1095678848.0000 - mean_absolute_error: 30168.0020
Epoch 24/100
95/95 [==============================] - 0s 949us/step - loss: 939073600.0000 - mean_absolute_error: 27976.9355
95/95 [==============================] - 0s 967us/step - loss: 939073600.0000 - mean_absolute_error: 27976.9355
Epoch 25/100
95/95 [==============================] - 0s 922us/step - loss: 777980288.0000 - mean_absolute_error: 24929.2148
95/95 [==============================] - 0s 941us/step - loss: 777980288.0000 - mean_absolute_error: 24929.2148
Epoch 26/100
95/95 [==============================] - 0s 924us/step - loss: 684038592.0000 - mean_absolute_error: 23373.6406
95/95 [==============================] - 0s 942us/step - loss: 684038592.0000 - mean_absolute_error: 23373.6406
Epoch 27/100
95/95 [==============================] - 0s 921us/step - loss: 522352224.0000 - mean_absolute_error: 19609.2617
95/95 [==============================] - 0s 939us/step - loss: 522352224.0000 - mean_absolute_error: 19609.2617
Epoch 28/100
95/95 [==============================] - 0s 942us/step - loss: 429569952.0000 - mean_absolute_error: 17475.0215
95/95 [==============================] - 0s 1ms/step - loss: 429569952.0000 - mean_absolute_error: 17475.0215  
Epoch 29/100
95/95 [==============================] - 0s 971us/step - loss: 348318080.0000 - mean_absolute_error: 15228.9736
95/95 [==============================] - 0s 989us/step - loss: 348318080.0000 - mean_absolute_error: 15228.9736
Epoch 30/100
95/95 [==============================] - 0s 932us/step - loss: 327572800.0000 - mean_absolute_error: 14217.6182
95/95 [==============================] - 0s 950us/step - loss: 327572800.0000 - mean_absolute_error: 14217.6182
Epoch 31/100
95/95 [==============================] - 0s 967us/step - loss: 226518576.0000 - mean_absolute_error: 11349.5557
95/95 [==============================] - 0s 984us/step - loss: 226518576.0000 - mean_absolute_error: 11349.5557
Epoch 32/100
95/95 [==============================] - 0s 972us/step - loss: 220188640.0000 - mean_absolute_error: 10928.2988
95/95 [==============================] - 0s 991us/step - loss: 220188640.0000 - mean_absolute_error: 10928.2988
Epoch 33/100
95/95 [==============================] - 0s 928us/step - loss: 184859824.0000 - mean_absolute_error: 9914.7861
95/95 [==============================] - 0s 946us/step - loss: 184859824.0000 - mean_absolute_error: 9914.7861
Epoch 34/100
95/95 [==============================] - 0s 966us/step - loss: 143862160.0000 - mean_absolute_error: 8746.4746
95/95 [==============================] - 0s 993us/step - loss: 143862160.0000 - mean_absolute_error: 8746.4746
Epoch 35/100
95/95 [==============================] - 0s 994us/step - loss: 131478368.0000 - mean_absolute_error: 8571.7188
95/95 [==============================] - 0s 1ms/step - loss: 131478368.0000 - mean_absolute_error: 8571.7188  
Epoch 36/100
95/95 [==============================] - 0s 928us/step - loss: 122612672.0000 - mean_absolute_error: 8484.3008
95/95 [==============================] - 0s 947us/step - loss: 122612672.0000 - mean_absolute_error: 8484.3008
Epoch 37/100
95/95 [==============================] - 0s 932us/step - loss: 118054384.0000 - mean_absolute_error: 8533.7373
95/95 [==============================] - 0s 951us/step - loss: 118054384.0000 - mean_absolute_error: 8533.7373
Epoch 38/100
95/95 [==============================] - 0s 923us/step - loss: 118340320.0000 - mean_absolute_error: 8729.5869
95/95 [==============================] - 0s 941us/step - loss: 118340320.0000 - mean_absolute_error: 8729.5869
Epoch 39/100
95/95 [==============================] - 0s 922us/step - loss: 117383240.0000 - mean_absolute_error: 8802.2666
95/95 [==============================] - 0s 941us/step - loss: 117383240.0000 - mean_absolute_error: 8802.2666
Epoch 40/100
95/95 [==============================] - 0s 948us/step - loss: 116076184.0000 - mean_absolute_error: 8826.4131
95/95 [==============================] - 0s 967us/step - loss: 116076184.0000 - mean_absolute_error: 8826.4131
Epoch 41/100
95/95 [==============================] - 0s 1ms/step - loss: 115166792.0000 - mean_absolute_error: 8843.8359
95/95 [==============================] - 0s 1ms/step - loss: 115166792.0000 - mean_absolute_error: 8843.8359
Epoch 42/100
95/95 [==============================] - 0s 959us/step - loss: 114478656.0000 - mean_absolute_error: 8853.9180
95/95 [==============================] - 0s 978us/step - loss: 114478656.0000 - mean_absolute_error: 8853.9180
Epoch 43/100
95/95 [==============================] - 0s 968us/step - loss: 113932624.0000 - mean_absolute_error: 8858.1758
95/95 [==============================] - 0s 987us/step - loss: 113932624.0000 - mean_absolute_error: 8858.1758
Epoch 44/100
95/95 [==============================] - 0s 922us/step - loss: 113476600.0000 - mean_absolute_error: 8857.7568
95/95 [==============================] - 0s 940us/step - loss: 113476600.0000 - mean_absolute_error: 8857.7568
Epoch 45/100
95/95 [==============================] - 0s 936us/step - loss: 113077984.0000 - mean_absolute_error: 8853.8926
95/95 [==============================] - 0s 954us/step - loss: 113077984.0000 - mean_absolute_error: 8853.8926
Epoch 46/100
95/95 [==============================] - 0s 1ms/step - loss: 112716184.0000 - mean_absolute_error: 8847.3848
95/95 [==============================] - 0s 1ms/step - loss: 112716184.0000 - mean_absolute_error: 8847.3848
Epoch 47/100
95/95 [==============================] - 0s 978us/step - loss: 112377968.0000 - mean_absolute_error: 8838.7793
95/95 [==============================] - 0s 997us/step - loss: 112377968.0000 - mean_absolute_error: 8838.7793
Epoch 48/100
95/95 [==============================] - 0s 989us/step - loss: 112054688.0000 - mean_absolute_error: 8828.5664
95/95 [==============================] - 0s 1ms/step - loss: 112054688.0000 - mean_absolute_error: 8828.5664  
Epoch 49/100
95/95 [==============================] - 0s 930us/step - loss: 111740560.0000 - mean_absolute_error: 8817.0811
95/95 [==============================] - 0s 949us/step - loss: 111740560.0000 - mean_absolute_error: 8817.0811
Epoch 50/100
95/95 [==============================] - 0s 950us/step - loss: 111431824.0000 - mean_absolute_error: 8804.6035
95/95 [==============================] - 0s 968us/step - loss: 111431824.0000 - mean_absolute_error: 8804.6035
Epoch 51/100
95/95 [==============================] - 0s 932us/step - loss: 111126024.0000 - mean_absolute_error: 8791.3418
95/95 [==============================] - 0s 950us/step - loss: 111126024.0000 - mean_absolute_error: 8791.3418
Epoch 52/100
95/95 [==============================] - 0s 935us/step - loss: 110821864.0000 - mean_absolute_error: 8777.4688
95/95 [==============================] - 0s 953us/step - loss: 110821864.0000 - mean_absolute_error: 8777.4688
Epoch 53/100
95/95 [==============================] - 0s 962us/step - loss: 110518184.0000 - mean_absolute_error: 8763.0088
95/95 [==============================] - 0s 981us/step - loss: 110518184.0000 - mean_absolute_error: 8763.0088
Epoch 54/100
95/95 [==============================] - 0s 995us/step - loss: 110214472.0000 - mean_absolute_error: 8748.0361
95/95 [==============================] - 0s 1ms/step - loss: 110214472.0000 - mean_absolute_error: 8748.0361  
Epoch 55/100
95/95 [==============================] - 0s 927us/step - loss: 109910600.0000 - mean_absolute_error: 8732.6865
95/95 [==============================] - 0s 946us/step - loss: 109910600.0000 - mean_absolute_error: 8732.6865
Epoch 56/100
95/95 [==============================] - 0s 931us/step - loss: 109606912.0000 - mean_absolute_error: 8716.9824
95/95 [==============================] - 0s 949us/step - loss: 109606912.0000 - mean_absolute_error: 8716.9824
Epoch 57/100
95/95 [==============================] - 0s 943us/step - loss: 109303728.0000 - mean_absolute_error: 8700.9961
95/95 [==============================] - 0s 961us/step - loss: 109303728.0000 - mean_absolute_error: 8700.9961
Epoch 58/100
95/95 [==============================] - 0s 932us/step - loss: 109001360.0000 - mean_absolute_error: 8684.7217
95/95 [==============================] - 0s 950us/step - loss: 109001360.0000 - mean_absolute_error: 8684.7217
Epoch 59/100
95/95 [==============================] - 0s 921us/step - loss: 108700680.0000 - mean_absolute_error: 8668.1914
95/95 [==============================] - 0s 939us/step - loss: 108700680.0000 - mean_absolute_error: 8668.1914
Epoch 60/100
95/95 [==============================] - 0s 981us/step - loss: 108402128.0000 - mean_absolute_error: 8651.5186
95/95 [==============================] - 0s 1000us/step - loss: 108402128.0000 - mean_absolute_error: 8651.5186
Epoch 61/100
95/95 [==============================] - 0s 936us/step - loss: 108106536.0000 - mean_absolute_error: 8634.6904
95/95 [==============================] - 0s 956us/step - loss: 108106536.0000 - mean_absolute_error: 8634.6904
Epoch 62/100
95/95 [==============================] - 0s 929us/step - loss: 107814888.0000 - mean_absolute_error: 8617.7666
95/95 [==============================] - 0s 947us/step - loss: 107814888.0000 - mean_absolute_error: 8617.7666
Epoch 63/100
95/95 [==============================] - 0s 990us/step - loss: 107527896.0000 - mean_absolute_error: 8600.7959
95/95 [==============================] - 0s 1ms/step - loss: 107527896.0000 - mean_absolute_error: 8600.7959  
Epoch 64/100
95/95 [==============================] - 0s 978us/step - loss: 107246680.0000 - mean_absolute_error: 8583.8154
95/95 [==============================] - 0s 997us/step - loss: 107246680.0000 - mean_absolute_error: 8583.8154
Epoch 65/100
95/95 [==============================] - 0s 942us/step - loss: 106971816.0000 - mean_absolute_error: 8566.8574
95/95 [==============================] - 0s 961us/step - loss: 106971816.0000 - mean_absolute_error: 8566.8574
Epoch 66/100
95/95 [==============================] - 0s 973us/step - loss: 106704320.0000 - mean_absolute_error: 8549.9883
95/95 [==============================] - 0s 1ms/step - loss: 106704320.0000 - mean_absolute_error: 8549.9883  
Epoch 67/100
95/95 [==============================] - 0s 965us/step - loss: 106444520.0000 - mean_absolute_error: 8533.2891
95/95 [==============================] - 0s 983us/step - loss: 106444520.0000 - mean_absolute_error: 8533.2891
Epoch 68/100
95/95 [==============================] - 0s 950us/step - loss: 106193144.0000 - mean_absolute_error: 8516.7852
95/95 [==============================] - 0s 969us/step - loss: 106193144.0000 - mean_absolute_error: 8516.7852
Epoch 69/100
95/95 [==============================] - 0s 924us/step - loss: 105951120.0000 - mean_absolute_error: 8500.6006
95/95 [==============================] - 0s 942us/step - loss: 105951120.0000 - mean_absolute_error: 8500.6006
Epoch 70/100
95/95 [==============================] - 0s 920us/step - loss: 105718648.0000 - mean_absolute_error: 8484.7764
95/95 [==============================] - 0s 938us/step - loss: 105718648.0000 - mean_absolute_error: 8484.7764
Epoch 71/100
95/95 [==============================] - 0s 921us/step - loss: 105494800.0000 - mean_absolute_error: 8469.1934
95/95 [==============================] - 0s 939us/step - loss: 105494800.0000 - mean_absolute_error: 8469.1934
Epoch 72/100
95/95 [==============================] - 0s 940us/step - loss: 105281672.0000 - mean_absolute_error: 8454.0557
95/95 [==============================] - 0s 969us/step - loss: 105281672.0000 - mean_absolute_error: 8454.0557
Epoch 73/100
95/95 [==============================] - 0s 1ms/step - loss: 105079392.0000 - mean_absolute_error: 8439.4688
95/95 [==============================] - 0s 1ms/step - loss: 105079392.0000 - mean_absolute_error: 8439.4688
Epoch 74/100
95/95 [==============================] - 0s 958us/step - loss: 104888136.0000 - mean_absolute_error: 8425.4668
95/95 [==============================] - 0s 976us/step - loss: 104888136.0000 - mean_absolute_error: 8425.4668
Epoch 75/100
95/95 [==============================] - 0s 967us/step - loss: 104708128.0000 - mean_absolute_error: 8412.0537
95/95 [==============================] - 0s 985us/step - loss: 104708128.0000 - mean_absolute_error: 8412.0537
Epoch 76/100
95/95 [==============================] - 0s 928us/step - loss: 104539408.0000 - mean_absolute_error: 8399.2461
95/95 [==============================] - 0s 947us/step - loss: 104539408.0000 - mean_absolute_error: 8399.2461
Epoch 77/100
95/95 [==============================] - 0s 926us/step - loss: 104382104.0000 - mean_absolute_error: 8387.0781
95/95 [==============================] - 0s 944us/step - loss: 104382104.0000 - mean_absolute_error: 8387.0781
Epoch 78/100
95/95 [==============================] - 0s 930us/step - loss: 104236112.0000 - mean_absolute_error: 8375.6318
95/95 [==============================] - 0s 949us/step - loss: 104236112.0000 - mean_absolute_error: 8375.6318
Epoch 79/100
95/95 [==============================] - 0s 1ms/step - loss: 104101448.0000 - mean_absolute_error: 8364.8574
95/95 [==============================] - 0s 1ms/step - loss: 104101448.0000 - mean_absolute_error: 8364.8574
Epoch 80/100
95/95 [==============================] - 0s 928us/step - loss: 103977280.0000 - mean_absolute_error: 8354.7598
95/95 [==============================] - 0s 947us/step - loss: 103977280.0000 - mean_absolute_error: 8354.7598
Epoch 81/100
95/95 [==============================] - 0s 1ms/step - loss: 103862800.0000 - mean_absolute_error: 8345.1973
95/95 [==============================] - 0s 1ms/step - loss: 103862800.0000 - mean_absolute_error: 8345.1973
Epoch 82/100
95/95 [==============================] - 0s 925us/step - loss: 103758488.0000 - mean_absolute_error: 8336.4199
95/95 [==============================] - 0s 943us/step - loss: 103758488.0000 - mean_absolute_error: 8336.4199
Epoch 83/100
95/95 [==============================] - 0s 959us/step - loss: 103662192.0000 - mean_absolute_error: 8328.2402
95/95 [==============================] - 0s 978us/step - loss: 103662192.0000 - mean_absolute_error: 8328.2402
Epoch 84/100
95/95 [==============================] - 0s 929us/step - loss: 103573056.0000 - mean_absolute_error: 8320.6299
95/95 [==============================] - 0s 948us/step - loss: 103573056.0000 - mean_absolute_error: 8320.6299
Epoch 85/100
95/95 [==============================] - 0s 1ms/step - loss: 103490112.0000 - mean_absolute_error: 8313.3252
95/95 [==============================] - 0s 1ms/step - loss: 103490112.0000 - mean_absolute_error: 8313.3252
Epoch 86/100
95/95 [==============================] - 0s 939us/step - loss: 103415568.0000 - mean_absolute_error: 8306.7666
95/95 [==============================] - 0s 957us/step - loss: 103415568.0000 - mean_absolute_error: 8306.7666
Epoch 87/100
95/95 [==============================] - 0s 936us/step - loss: 103347936.0000 - mean_absolute_error: 8300.8271
95/95 [==============================] - 0s 954us/step - loss: 103347936.0000 - mean_absolute_error: 8300.8271
Epoch 88/100
95/95 [==============================] - 0s 931us/step - loss: 103286144.0000 - mean_absolute_error: 8295.4375
95/95 [==============================] - 0s 951us/step - loss: 103286144.0000 - mean_absolute_error: 8295.4375
Epoch 89/100
95/95 [==============================] - 0s 931us/step - loss: 103229360.0000 - mean_absolute_error: 8290.5312
95/95 [==============================] - 0s 949us/step - loss: 103229360.0000 - mean_absolute_error: 8290.5312
Epoch 90/100
95/95 [==============================] - 0s 923us/step - loss: 103176672.0000 - mean_absolute_error: 8285.8838
95/95 [==============================] - 0s 942us/step - loss: 103176672.0000 - mean_absolute_error: 8285.8838
Epoch 91/100
95/95 [==============================] - 0s 974us/step - loss: 103129160.0000 - mean_absolute_error: 8281.7275
95/95 [==============================] - 0s 995us/step - loss: 103129160.0000 - mean_absolute_error: 8281.7275
Epoch 92/100
95/95 [==============================] - 0s 969us/step - loss: 103085944.0000 - mean_absolute_error: 8277.9033
95/95 [==============================] - 0s 987us/step - loss: 103085944.0000 - mean_absolute_error: 8277.9033
Epoch 93/100
95/95 [==============================] - 0s 925us/step - loss: 103046672.0000 - mean_absolute_error: 8274.5068
95/95 [==============================] - 0s 943us/step - loss: 103046672.0000 - mean_absolute_error: 8274.5068
Epoch 94/100
95/95 [==============================] - 0s 929us/step - loss: 103010008.0000 - mean_absolute_error: 8271.3438
95/95 [==============================] - 0s 948us/step - loss: 103010008.0000 - mean_absolute_error: 8271.3438
Epoch 95/100
95/95 [==============================] - 0s 931us/step - loss: 102976904.0000 - mean_absolute_error: 8268.5752
95/95 [==============================] - 0s 949us/step - loss: 102976904.0000 - mean_absolute_error: 8268.5752
Epoch 96/100
95/95 [==============================] - 0s 979us/step - loss: 102946168.0000 - mean_absolute_error: 8266.1211
95/95 [==============================] - 0s 997us/step - loss: 102946168.0000 - mean_absolute_error: 8266.1211
Epoch 97/100
95/95 [==============================] - 0s 940us/step - loss: 102917624.0000 - mean_absolute_error: 8263.9365
95/95 [==============================] - 0s 959us/step - loss: 102917624.0000 - mean_absolute_error: 8263.9365
Epoch 98/100
95/95 [==============================] - 0s 1ms/step - loss: 102891056.0000 - mean_absolute_error: 8261.9727
95/95 [==============================] - 0s 1ms/step - loss: 102891056.0000 - mean_absolute_error: 8261.9727
Epoch 99/100
95/95 [==============================] - 0s 941us/step - loss: 102866064.0000 - mean_absolute_error: 8260.2051
95/95 [==============================] - 0s 959us/step - loss: 102866064.0000 - mean_absolute_error: 8260.2051
Epoch 100/100
95/95 [==============================] - 0s 958us/step - loss: 102842776.0000 - mean_absolute_error: 8258.6064
95/95 [==============================] - 0s 978us/step - loss: 102842776.0000 - mean_absolute_error: 8258.6064
[1] "step4"
[1] 4675
[1] "step5"
> print(paste("Score final : ", evaluate(test_label, pred), sep=""))
[1] "Score final : 11531.1946582175"
> train <- read_delim(file="data/train_V2.csv",delim=',')

── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character()
)
ℹ Use `spec()` for the full column specifications.

> test <- read_delim(file="data/test_V2.csv",delim=',')

── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character(),
  Usage = col_character()
)
ℹ Use `spec()` for the full column specifications.

> rm(list=objects())
+ graphics.off()
+ 
+ ##pour load tout en 2 lignes, il faut juste rajouter les librairies dans libs.to.load
+ 
+ libs.to.load = c("icesTAF", "tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
+ suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
+ 
+ ##setwd("C:/Users/CM/code/M1/R")
+ 
+ ##load tous les fichiers en sources
+ files.sources = list.files(pattern = "*.r$")
+ files.sources = files.sources[files.sources != "main_matthieu.r"]
+ sapply(files.sources, source)
+ 
+ ##
+ plt = FALSE #(si on veut plot, mettre à TRUE)
+ 
+ ##MAIN NICOLAS
+ model = "lstm"
+ 
+ data = format_data("./data/train_V2.csv", "./data/test_V2.csv")
+ train_set = data$train_set
+ test_set = data$test_set
+ train_label = data$train_label
+ test_label = data$test_label
+ 
+ if (model == "xgboost"){
+   pred = xgboost_rte(train_set, train_label, test_set)
+ } else if(model == "lstm"){
+   pred = lstm(train_set, train_label, test_set)
+ }
+ 
+ 
+ print(paste("Score final : ", evaluate(test_label, pred), sep=""))
+ 
+ if (plt) {
+     plot(c(train_label, pred))
+ }
+ 
+ ##FIN MAIN NICOLAS 
+ 
+ 
+ train <- read_delim(file="data/train_V2.csv",delim=',')
+ test <- read_delim(file="data/test_V2.csv",delim=',')
+ 
+ reg = lm(Load~Temp, data=train)
+ 
+ if (plt){
+     plot(train$Date, train$Load, type='l')
+     par(new=T)
+     plot(train$Date, train$Temp, type='l', col='red')
+ 
+     plot(train$Temp,train$Load, col='red')
+     points(train$Temp ,reg$fitted.values)
+ }
+ 
+ train$WeekDays = days_to_numeric(train)
+ test$WeekDays = days_to_numeric(test)
+ 
+ train$Year = train$Year - 2012
+ test$Year = test$Year - 2012
+ 
+ #saisonalite : annuelle
+ 
+ if (plt){
+     plot(train$Date, train$Load, type='l')
+ }
+ 
+ MA <- stats::filter(train$Load, filter = rep(1/365,365),
+              method = c("convolution"), sides = 2, circular = FALSE)
+ if (plt){
+     plot(train$Date, train$Load, type = "l", xlab = "",
+          ylab = "consumption (kw)", col = "seagreen4", lwd = 1)
+     lines(train$Date, MA, col = "red", lwd = 2)
+ }
+ 
+ ##estimation avec fourier
+ fourier(train, test, plt = TRUE)
+ 
+ ##saisonalite : hebdomadaire
+ 
+ num.years = 0
+ average = 0
+ N = 8; a = 0.7 #exponential weight
+ while (365*(num.years+1) <= length(train$Date)) {
+     par(mfrow = c(1, 1))
+     dateyear = train$Date[(365*num.years+1):(365*(num.years+1))]
+     loadyear = train$Load[(365*num.years+1):(365*(num.years+1))]
+ 
+     ##plot(dateyear,loadyear,type='l')
+   
+     MAw <- stats::filter(loadyear, filter = rep(1/52,52),
+                       method = c("convolution"), sides = 2, circular = T)
+     ##plot(dateyear,loadyear, type = "l", xlab = "",
+     ##    ylab = "consumption (kw)", col = "seagreen4", lwd = 1)
+     ##lines(dateyear, MAw, col = "red", lwd = 2)
+   
+     ##plot(dateyear, loadyear - MAw, type="l")
+     expweight = (((1-a)/(1-a^8))*a^(N-(num.years+1)))
+     average = expweight*(average + (loadyear - MAw))
+     num.years = num.years + 1
+ }
+ if (plt){
+     plot(average)
+ }
+ 
+ train.to.day = train$time %% 365 + 1
+ test.to.day = test$time %% 365 + 1
+ 
+ pred.hebdo.train = average[train.to.day]
+ pred.hebdo.test = average[test.to.day]
+ 
+ pred.total.train = reg$fitted + pred.hebdo.train
+ pred.total.test = pred.fourier + pred.hebdo.test
+ 
+ 
+ if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(train$time,pred.total.train, col='red', lwd=1)
+     lines(test$time,pred.total.test, col='green', lwd=1)
+ }
+ 
+ Load = pred.total.test
+ Id = train$Id
+ submission = data.frame(Load, Id)
+ 
+ write.csv(submission, file ="submissions/submission.csv", row.names=F)
+ 
+ 
+ MAw.train.total <- stats::filter(train$Load.1, filter = rep(1/52,52),
+                      method = c("convolution"), sides = 2, circular = T)
+ MAw.test.total <- stats::filter(test$Load.1, filter = rep(1/52,52),
+                                  method = c("convolution"), sides = 2, circular = T)
+ 
+ train$seasonal.tendancy = train$Load.1 - MAw.train.total
+ test$seasonal.tendancy = test$Load.1 - MAw.test.total
+ 
+ train$fourier.fitted = reg$fitted
+ test$fourier.fitted = pred.fourier
+ 
+ reg2 = lm(Load~Load.1+Load.7+Temp
+ +Temp_s95+WeekDays+GovernmentResponseIndex,data=train)
+ summary(reg2)
+ 
+ Gam <- gam(Load~s(Load.1)+s(Load.7)+s(Temp)
+            +s(Temp_s95)+s(WeekDays,k=7)
+            +s(GovernmentResponseIndex)
+            ,data=train)
+ summary(Gam)
+ 
+ 
+ gam.train = predict(Gam, newdata=train)
+ gam.test = predict(Gam, newdata=test)
+ 
+ if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(train$time,Gam$fit, col='red', lwd=1)
+     lines(test$time,gam.test, col='green', lwd=1)
+ }
+ ##visreg(Gam,"Temp_s95")
+ 
+ tmp = format_data("./data/train_V2.csv", "./data/test_V2.csv")
+ test_label = tmp$test_label
+ 
+ Gam.rmse = evaluate(test_label, gam.test)
+ 
+ ##cross-validation
+ 
+ cross.validation = function (train, test) {
+   set.seed(123)
+   to.extract = rbern(n=length(train$Load),p=0.8)==T
+   to.train = train[F,]
+   for (j in 1:length(to.extract)) {
+     if (to.extract[j]) {
+       to.train[nrow(to.train)+1,] = train[j,]
+     }
+   }
+   
+   cross.train  <- train[to.train$time, ]
+   cross.test <- train[-to.train$time, ]
+   
+   model = gam(Load~s(Load.1)+s(Load.7)+s(Temp)
+                +s(Temp_s95)+s(WeekDays,k=7)
+                ,data=cross.train)
+   
+   pred.test = predict(model,test)
+   print(length(pred.test))
+   N = length(test$Load.1)
+   RMSE = rmse(pred.test[-N], test$Load.1[2:N]) #on connait pas le dernier
+   
+   print(RMSE)
+   return(list("model"=model,"pred.test"=pred.test,"RMSE"=RMSE))
+ }
+ 
+ cv = cross.validation(train, test)
+ model = cv$model; pred.test = cv$pred.test; RMSE = cv$RMSE
+ 
+ if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(train$time,predict(model,train), col='red', lwd=1)
+     lines(test$time,pred.test, col='green', lwd=1)
+ }
+ 
+ Load = pred.test
+ Id = 1:length(Load)
+ submission = data.frame(Load, Id)
+ 
+ write.csv(submission, file ="submission.csv", row.names=F)
+ 
+ ##lstm
+ 
+ labels = train$Load
+ train.lstm = train[,-c(1,2,22,23)]
+ test.lstm = test[,-c(1,20,21,23,24)]
+ 
+ lstm = function(train_set, train_label, test_set) {
+   y = train_label
+   x = data.matrix(train_set)
+   x_test = data.matrix(test_set)
+   model = keras_model_sequential() %>%
+     layer_dense(units = 19, activation = 'relu', input_shape = c(19)) %>%
+     layer_dense(units = 12, activation = 'relu') %>%
+     layer_dense(units = 6, activation = 'relu') %>%
+     layer_dense(units=1, activation = "linear")
+   
+   model %>% compile(loss = 'mse',
+                     optimizer = optimizer_adam(0.0005))
+   
+   model %>% fit(x,y, epochs=15)
+ 
+   pred.train = model %>% predict(x)
+   pred.test = model %>% predict(x_test)
+ 
+   return(list("train"=pred.train,"test"=pred.test))
+ }
+ 
+ 
+ pred.lstm = lstm(train.lstm, labels, test.lstm)
+ 
+ if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(test$time,pred.lstm$test, col='green', lwd=1)
+ }
+ 
+ N = length(test$Load.1)
+ RMSE = rmse(pred.lstm$test[-N], test$Load.1[2:N])
+ RMSE
+ 
+ 
+ ##random forest
+ 
+ rf = randomForest(Load ~ ., data=train, mtry=3,
+                          importance=TRUE, na.action=na.omit)
+ pred.test.rf = predict(rf,test)
+ 
+ if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(test$time,pred.test.rf, col='green', lwd=1)
+ }
+ 
+ N = length(test$Load.1)
+ RMSE = rmse(pred.test.rf[-N], test$Load.1[2:N])
+ RMSE
+ 
+ 
+ ##aggregation
+ 
     icesTAF    tidyverse    lubridate       ranger       pracma      Metrics 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
        mgcv        keras       visreg        caret         mc2d        opera 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       abind randomForest   tensorflow 
        TRUE         TRUE         TRUE 
> > > > > > > $days_to_numeric.r
$days_to_numeric.r$value
function (data) {
    data$WeekDays[data$WeekDays=='Monday']    = 1
    data$WeekDays[data$WeekDays=='Tuesday']   = 2
    data$WeekDays[data$WeekDays=='Wednesday'] = 3
    data$WeekDays[data$WeekDays=='Thursday']  = 4
    data$WeekDays[data$WeekDays=='Friday']    = 5
    data$WeekDays[data$WeekDays=='Saturday']  = 6
    data$WeekDays[data$WeekDays=='Sunday']    = 7
    data$WeekDays = as.numeric(data$WeekDays)
    return (data$WeekDays)
}

$days_to_numeric.r$visible
[1] FALSE


$evaluate.r
$evaluate.r$value
function(test_label, predicted_set){
    
    return(rmse(test_label, predicted_set))
}

$evaluate.r$visible
[1] FALSE


$format_data.r
$format_data.r$value
function(file_train, file_test) {

    print(paste("Load and format " , file_train, sep = " "))
    train_set = read_csv(file_train, col_types = cols())

    train_set$WeekDays[train_set$WeekDays == "Monday"] <- 1
    train_set$WeekDays[train_set$WeekDays == "Tuesday"] <- 2
    train_set$WeekDays[train_set$WeekDays == "Wednesday"] <- 3
    train_set$WeekDays[train_set$WeekDays == "Thursday"] <- 4
    train_set$WeekDays[train_set$WeekDays == "Friday"] <- 5
    train_set$WeekDays[train_set$WeekDays == "Saturday"] <- 6
    train_set$WeekDays[train_set$WeekDays == "Sunday"] <- 7
    train_set$WeekDays = as.integer(train_set$WeekDays)

    train_set$Year = NULL
    train_set$Date = NULL
    train_label = data.matrix(train_set$Load)


    train_set$Load = NULL
    
    train_set = data.matrix(train_set)


    print(paste("Load and format " , file_test, sep = " "))    
    test_set = read_csv(file_test, col_types = cols())

    test_set$WeekDays[test_set$WeekDays == "Monday"] <- 1
    test_set$WeekDays[test_set$WeekDays == "Tuesday"] <- 2
    test_set$WeekDays[test_set$WeekDays == "Wednesday"] <- 3
    test_set$WeekDays[test_set$WeekDays == "Thursday"] <- 4
    test_set$WeekDays[test_set$WeekDays == "Friday"] <- 5
    test_set$WeekDays[test_set$WeekDays == "Saturday"] <- 6
    test_set$WeekDays[test_set$WeekDays == "Sunday"] <- 7
    test_set$WeekDays = as.integer(test_set$WeekDays)

    test_label = test_set$Load.1
    tmp = test_set$Load.1
    for (i in c(1:(length(tmp)-1))){
        test_label[i] = tmp[i+1]
    }
    
    test_set$Year = NULL
    test_set$Date = NULL
    test_set$Id = NULL
    test_set$Usage = NULL
    test_set = data.matrix(test_set)
    

    test_set = data.matrix(test_set)
    return(list("train_set" = train_set, "train_label" = train_label, "test_set" = test_set, "test_label" = test_label))
}

$format_data.r$visible
[1] FALSE


$fourier.r
$fourier.r$value
function(train, test, plt = FALSE){
    total.time = c(1:(nrow(train)+nrow(test)))
    length(total.time)
    train$time = total.time[1:nrow(train)]
    test$time = tail(total.time,nrow(test))

    fourier.make.matrix = function (t, k, period) {
        w = 2*pi/period
        ret = cbind(cos(w*t), sin(w*t))
        
        for(i in c(2:K))
        {
            ret = cbind(ret, cos(i*w*t), sin(i*w*t))
        }
        return (ret)
    }

    K = 5; period = 365
    fourier.train = fourier.make.matrix(train$time, K, period)
    fourier.test = fourier.make.matrix(test$time, K, period)

    fourier.train.df = data.frame(train$Load,fourier.train)
    fourier.test.df = data.frame(fourier.test)

    reg = lm(train.Load ~., data=fourier.train.df)

    pred.fourier = predict(reg, newdata=fourier.test.df)
    total.fourier = c(reg$fitted,pred.fourier)
    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(reg$fitted,col='red', lwd=2)
        lines(test$time,pred.fourier,col='green', lwd=2)
    }
    return(pred.fourier)
}

$fourier.r$visible
[1] FALSE


$lstm.r
$lstm.r$value
function(train_set, train_label, test_set){
  N = length(train_set[,'Load.1'])
  nb_var = 17
  y = train_label
  x = array(train_set, dim = c(N, nb_var, 1))
  x_test = array(test_set,dim = c(length(test_set[,'Load.1']), nb_var, 1))
  print("step1")
  model = keras_model_sequential() %>%   
    #layer_lstm(units=128, input_shape=c(nb_var, 1), activation="relu", return_sequences = TRUE) %>% 
    layer_dense(units=64, activation = "relu") %>%  
    layer_dense(units=32) %>%  
    layer_dense(units=1, activation = "linear")
  print("step2")
  model %>% compile(loss = 'mse',
                    optimizer = 'adam',
                    metrics = list("mean_absolute_error")
  )
  print("step3")
  model %>% fit(x,y, epochs=100, batch_size=32, shuffle = FALSE)
  print("step4")
  print(length(x_test))
  pred = model %>% predict(x_test)
  print("step5")
  return(pred)
    
}

$lstm.r$visible
[1] FALSE


$main.r
NULL

$scale.r
$scale.r$value
function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

$scale.r$visible
[1] FALSE


$xgboost.r
$xgboost.r$value
function(train_set, train_label, test_set){
  param = list(booster = "gblinear", objective = "reg:squarederror", eval_metric = "rmse", lambda = 0.0003, alpha = 0.0003, nthread = 2, eta = 0.1)
  
  print("Model : XGBOOST")
  
  xgbmodel = xgboost(data = train_set, label = train_label, nrounds = 200, params = param, verbose = 0)
  
  pred = predict(xgbmodel, test_set)
  
  return(pred)
}

$xgboost.r$visible
[1] FALSE


> > > > [1] "Load and format  ./data/train_V2.csv"
[1] "Load and format  ./data/test_V2.csv"
> [1] "step1"
[1] "step2"
[1] "step3"
Epoch 1/100
95/95 [==============================] - 0s 999us/step - loss: 2813465856.0000 - mean_absolute_error: 49980.8984
95/95 [==============================] - 0s 1ms/step - loss: 2813465856.0000 - mean_absolute_error: 49980.8984  
Epoch 2/100
95/95 [==============================] - 0s 974us/step - loss: 2740388864.0000 - mean_absolute_error: 48558.7461
95/95 [==============================] - 0s 993us/step - loss: 2740388864.0000 - mean_absolute_error: 48558.7461
Epoch 3/100
95/95 [==============================] - 0s 970us/step - loss: 2739704832.0000 - mean_absolute_error: 48560.1680
95/95 [==============================] - 0s 989us/step - loss: 2739704832.0000 - mean_absolute_error: 48560.1680
Epoch 4/100
95/95 [==============================] - 0s 1ms/step - loss: 2738216960.0000 - mean_absolute_error: 48553.0625
95/95 [==============================] - 0s 1ms/step - loss: 2738216960.0000 - mean_absolute_error: 48553.0625
Epoch 5/100
95/95 [==============================] - 0s 1ms/step - loss: 2735991808.0000 - mean_absolute_error: 48550.3398
95/95 [==============================] - 0s 1ms/step - loss: 2735991808.0000 - mean_absolute_error: 48550.3398
Epoch 6/100
95/95 [==============================] - 0s 1ms/step - loss: 2733393152.0000 - mean_absolute_error: 48550.7070
95/95 [==============================] - 0s 1ms/step - loss: 2733393152.0000 - mean_absolute_error: 48550.7070
Epoch 7/100
95/95 [==============================] - 0s 1ms/step - loss: 2729206784.0000 - mean_absolute_error: 48481.3516
95/95 [==============================] - 0s 1ms/step - loss: 2729206784.0000 - mean_absolute_error: 48481.3516
Epoch 8/100
95/95 [==============================] - 0s 990us/step - loss: 2724094208.0000 - mean_absolute_error: 48440.4531
95/95 [==============================] - 0s 1ms/step - loss: 2724094208.0000 - mean_absolute_error: 48440.4531  
Epoch 9/100
95/95 [==============================] - 0s 986us/step - loss: 2716955392.0000 - mean_absolute_error: 48402.0625
95/95 [==============================] - 0s 1ms/step - loss: 2716955392.0000 - mean_absolute_error: 48402.0625  
Epoch 10/100
95/95 [==============================] - 0s 1ms/step - loss: 2705216256.0000 - mean_absolute_error: 48252.1680
95/95 [==============================] - 0s 1ms/step - loss: 2705216256.0000 - mean_absolute_error: 48252.1680
Epoch 11/100
95/95 [==============================] - 0s 966us/step - loss: 2687944192.0000 - mean_absolute_error: 48120.6406
95/95 [==============================] - 0s 994us/step - loss: 2687944192.0000 - mean_absolute_error: 48120.6406
Epoch 12/100
95/95 [==============================] - 0s 966us/step - loss: 2660759808.0000 - mean_absolute_error: 47861.3086
95/95 [==============================] - 0s 985us/step - loss: 2660759808.0000 - mean_absolute_error: 47861.3086
Epoch 13/100
95/95 [==============================] - 0s 985us/step - loss: 2621736192.0000 - mean_absolute_error: 47515.9258
95/95 [==============================] - 0s 1ms/step - loss: 2621736192.0000 - mean_absolute_error: 47515.9258  
Epoch 14/100
95/95 [==============================] - 0s 939us/step - loss: 2565542400.0000 - mean_absolute_error: 47036.4805
95/95 [==============================] - 0s 956us/step - loss: 2565542400.0000 - mean_absolute_error: 47036.4805
Epoch 15/100
95/95 [==============================] - 0s 932us/step - loss: 2489311488.0000 - mean_absolute_error: 46354.3711
95/95 [==============================] - 0s 950us/step - loss: 2489311488.0000 - mean_absolute_error: 46354.3711
Epoch 16/100
95/95 [==============================] - 0s 1ms/step - loss: 2388197888.0000 - mean_absolute_error: 45386.6250
95/95 [==============================] - 0s 1ms/step - loss: 2388197888.0000 - mean_absolute_error: 45386.6250
Epoch 17/100
95/95 [==============================] - 0s 975us/step - loss: 2271472640.0000 - mean_absolute_error: 44409.5234
95/95 [==============================] - 0s 994us/step - loss: 2271472640.0000 - mean_absolute_error: 44409.5234
Epoch 18/100
95/95 [==============================] - 0s 929us/step - loss: 2121492480.0000 - mean_absolute_error: 42702.5703
95/95 [==============================] - 0s 948us/step - loss: 2121492480.0000 - mean_absolute_error: 42702.5703
Epoch 19/100
95/95 [==============================] - 0s 932us/step - loss: 1957775360.0000 - mean_absolute_error: 41038.9609
95/95 [==============================] - 0s 950us/step - loss: 1957775360.0000 - mean_absolute_error: 41038.9609
Epoch 20/100
95/95 [==============================] - 0s 972us/step - loss: 1784023936.0000 - mean_absolute_error: 39266.3906
95/95 [==============================] - 0s 991us/step - loss: 1784023936.0000 - mean_absolute_error: 39266.3906
Epoch 21/100
95/95 [==============================] - 0s 1ms/step - loss: 1605364352.0000 - mean_absolute_error: 37242.4336
95/95 [==============================] - 0s 1ms/step - loss: 1605364352.0000 - mean_absolute_error: 37242.4336
Epoch 22/100
95/95 [==============================] - 0s 955us/step - loss: 1413633536.0000 - mean_absolute_error: 34675.0742
95/95 [==============================] - 0s 973us/step - loss: 1413633536.0000 - mean_absolute_error: 34675.0742
Epoch 23/100
95/95 [==============================] - 0s 922us/step - loss: 1239333760.0000 - mean_absolute_error: 32396.0156
95/95 [==============================] - 0s 941us/step - loss: 1239333760.0000 - mean_absolute_error: 32396.0156
Epoch 24/100
95/95 [==============================] - 0s 931us/step - loss: 1072074944.0000 - mean_absolute_error: 30049.9688
95/95 [==============================] - 0s 950us/step - loss: 1072074944.0000 - mean_absolute_error: 30049.9688
Epoch 25/100
95/95 [==============================] - 0s 929us/step - loss: 904235136.0000 - mean_absolute_error: 27428.6973
95/95 [==============================] - 0s 947us/step - loss: 904235136.0000 - mean_absolute_error: 27428.6973
Epoch 26/100
95/95 [==============================] - 0s 997us/step - loss: 762888064.0000 - mean_absolute_error: 24802.1484
95/95 [==============================] - 0s 1ms/step - loss: 762888064.0000 - mean_absolute_error: 24802.1484  
Epoch 27/100
95/95 [==============================] - 0s 980us/step - loss: 636680064.0000 - mean_absolute_error: 22195.5664
95/95 [==============================] - 0s 998us/step - loss: 636680064.0000 - mean_absolute_error: 22195.5664
Epoch 28/100
95/95 [==============================] - 0s 979us/step - loss: 516200032.0000 - mean_absolute_error: 19459.6895
95/95 [==============================] - 0s 997us/step - loss: 516200032.0000 - mean_absolute_error: 19459.6895
Epoch 29/100
95/95 [==============================] - 0s 976us/step - loss: 407519648.0000 - mean_absolute_error: 16854.9746
95/95 [==============================] - 0s 995us/step - loss: 407519648.0000 - mean_absolute_error: 16854.9746
Epoch 30/100
95/95 [==============================] - 0s 972us/step - loss: 391449216.0000 - mean_absolute_error: 16030.8076
95/95 [==============================] - 0s 991us/step - loss: 391449216.0000 - mean_absolute_error: 16030.8076
Epoch 31/100
95/95 [==============================] - 0s 932us/step - loss: 275583680.0000 - mean_absolute_error: 12899.1211
95/95 [==============================] - 0s 950us/step - loss: 275583680.0000 - mean_absolute_error: 12899.1211
Epoch 32/100
95/95 [==============================] - 0s 929us/step - loss: 228366320.0000 - mean_absolute_error: 11402.8096
95/95 [==============================] - 0s 947us/step - loss: 228366320.0000 - mean_absolute_error: 11402.8096
Epoch 33/100
95/95 [==============================] - 0s 980us/step - loss: 187398464.0000 - mean_absolute_error: 10165.8809
95/95 [==============================] - 0s 1ms/step - loss: 187398464.0000 - mean_absolute_error: 10165.8809  
Epoch 34/100
95/95 [==============================] - 0s 1ms/step - loss: 169545664.0000 - mean_absolute_error: 9612.3027
95/95 [==============================] - 0s 1ms/step - loss: 169545664.0000 - mean_absolute_error: 9612.3027
Epoch 35/100
95/95 [==============================] - 0s 974us/step - loss: 159348320.0000 - mean_absolute_error: 9362.5850
95/95 [==============================] - 0s 993us/step - loss: 159348320.0000 - mean_absolute_error: 9362.5850
Epoch 36/100
95/95 [==============================] - 0s 974us/step - loss: 133400896.0000 - mean_absolute_error: 8745.2969
95/95 [==============================] - 0s 992us/step - loss: 133400896.0000 - mean_absolute_error: 8745.2969
Epoch 37/100
95/95 [==============================] - 0s 962us/step - loss: 128607464.0000 - mean_absolute_error: 8815.7012
95/95 [==============================] - 0s 981us/step - loss: 128607464.0000 - mean_absolute_error: 8815.7012
Epoch 38/100
95/95 [==============================] - 0s 1ms/step - loss: 123811008.0000 - mean_absolute_error: 8818.4668
95/95 [==============================] - 0s 1ms/step - loss: 123811008.0000 - mean_absolute_error: 8818.4668
Epoch 39/100
95/95 [==============================] - 0s 952us/step - loss: 120865624.0000 - mean_absolute_error: 8842.6680
95/95 [==============================] - 0s 971us/step - loss: 120865624.0000 - mean_absolute_error: 8842.6680
Epoch 40/100
95/95 [==============================] - 0s 966us/step - loss: 118940944.0000 - mean_absolute_error: 8869.1602
95/95 [==============================] - 0s 985us/step - loss: 118940944.0000 - mean_absolute_error: 8869.1602
Epoch 41/100
95/95 [==============================] - 0s 968us/step - loss: 117643440.0000 - mean_absolute_error: 8891.3945
95/95 [==============================] - 0s 986us/step - loss: 117643440.0000 - mean_absolute_error: 8891.3945
Epoch 42/100
95/95 [==============================] - 0s 953us/step - loss: 116725264.0000 - mean_absolute_error: 8907.5430
95/95 [==============================] - 0s 981us/step - loss: 116725264.0000 - mean_absolute_error: 8907.5430
Epoch 43/100
95/95 [==============================] - 0s 984us/step - loss: 116039616.0000 - mean_absolute_error: 8917.5000
95/95 [==============================] - 0s 1ms/step - loss: 116039616.0000 - mean_absolute_error: 8917.5000  
Epoch 44/100
95/95 [==============================] - 0s 995us/step - loss: 115498272.0000 - mean_absolute_error: 8922.1904
95/95 [==============================] - 0s 1ms/step - loss: 115498272.0000 - mean_absolute_error: 8922.1904  
Epoch 45/100
95/95 [==============================] - 0s 927us/step - loss: 115048392.0000 - mean_absolute_error: 8922.5439
95/95 [==============================] - 0s 945us/step - loss: 115048392.0000 - mean_absolute_error: 8922.5439
Epoch 46/100
95/95 [==============================] - 0s 960us/step - loss: 114658384.0000 - mean_absolute_error: 8919.5742
95/95 [==============================] - 0s 979us/step - loss: 114658384.0000 - mean_absolute_error: 8919.5742
Epoch 47/100
95/95 [==============================] - 0s 1ms/step - loss: 114308600.0000 - mean_absolute_error: 8914.1533
95/95 [==============================] - 0s 1ms/step - loss: 114308600.0000 - mean_absolute_error: 8914.1533
Epoch 48/100
95/95 [==============================] - 0s 974us/step - loss: 113986472.0000 - mean_absolute_error: 8906.8174
95/95 [==============================] - 0s 992us/step - loss: 113986472.0000 - mean_absolute_error: 8906.8174
Epoch 49/100
95/95 [==============================] - 0s 923us/step - loss: 113683480.0000 - mean_absolute_error: 8898.0391
95/95 [==============================] - 0s 942us/step - loss: 113683480.0000 - mean_absolute_error: 8898.0391
Epoch 50/100
95/95 [==============================] - 0s 932us/step - loss: 113393120.0000 - mean_absolute_error: 8888.1250
95/95 [==============================] - 0s 950us/step - loss: 113393120.0000 - mean_absolute_error: 8888.1250
Epoch 51/100
95/95 [==============================] - 0s 949us/step - loss: 113111168.0000 - mean_absolute_error: 8877.3359
95/95 [==============================] - 0s 967us/step - loss: 113111168.0000 - mean_absolute_error: 8877.3359
Epoch 52/100
95/95 [==============================] - 0s 1ms/step - loss: 112834312.0000 - mean_absolute_error: 8865.8564
95/95 [==============================] - 0s 1ms/step - loss: 112834312.0000 - mean_absolute_error: 8865.8564
Epoch 53/100
95/95 [==============================] - 0s 956us/step - loss: 112560224.0000 - mean_absolute_error: 8853.7959
95/95 [==============================] - 0s 974us/step - loss: 112560224.0000 - mean_absolute_error: 8853.7959
Epoch 54/100
95/95 [==============================] - 0s 943us/step - loss: 112286936.0000 - mean_absolute_error: 8841.2578
95/95 [==============================] - 0s 962us/step - loss: 112286936.0000 - mean_absolute_error: 8841.2578
Epoch 55/100
95/95 [==============================] - 0s 1ms/step - loss: 112014120.0000 - mean_absolute_error: 8828.2949
95/95 [==============================] - 0s 1ms/step - loss: 112014120.0000 - mean_absolute_error: 8828.2949
Epoch 56/100
95/95 [==============================] - 0s 927us/step - loss: 111740592.0000 - mean_absolute_error: 8814.9102
95/95 [==============================] - 0s 945us/step - loss: 111740592.0000 - mean_absolute_error: 8814.9102
Epoch 57/100
95/95 [==============================] - 0s 944us/step - loss: 111466416.0000 - mean_absolute_error: 8801.1562
95/95 [==============================] - 0s 965us/step - loss: 111466416.0000 - mean_absolute_error: 8801.1562
Epoch 58/100
95/95 [==============================] - 0s 970us/step - loss: 111191144.0000 - mean_absolute_error: 8787.1035
95/95 [==============================] - 0s 989us/step - loss: 111191144.0000 - mean_absolute_error: 8787.1035
Epoch 59/100
95/95 [==============================] - 0s 960us/step - loss: 110915176.0000 - mean_absolute_error: 8772.7549
95/95 [==============================] - 0s 978us/step - loss: 110915176.0000 - mean_absolute_error: 8772.7549
Epoch 60/100
95/95 [==============================] - 0s 959us/step - loss: 110638672.0000 - mean_absolute_error: 8758.1514
95/95 [==============================] - 0s 977us/step - loss: 110638672.0000 - mean_absolute_error: 8758.1514
Epoch 61/100
95/95 [==============================] - 0s 945us/step - loss: 110362136.0000 - mean_absolute_error: 8743.2920
95/95 [==============================] - 0s 963us/step - loss: 110362136.0000 - mean_absolute_error: 8743.2920
Epoch 62/100
95/95 [==============================] - 0s 934us/step - loss: 110086096.0000 - mean_absolute_error: 8728.1953
95/95 [==============================] - 0s 954us/step - loss: 110086096.0000 - mean_absolute_error: 8728.1953
Epoch 63/100
95/95 [==============================] - 0s 943us/step - loss: 109811248.0000 - mean_absolute_error: 8712.9238
95/95 [==============================] - 0s 962us/step - loss: 109811248.0000 - mean_absolute_error: 8712.9238
Epoch 64/100
95/95 [==============================] - 0s 939us/step - loss: 109538488.0000 - mean_absolute_error: 8697.5254
95/95 [==============================] - 0s 959us/step - loss: 109538488.0000 - mean_absolute_error: 8697.5254
Epoch 65/100
95/95 [==============================] - 0s 968us/step - loss: 109268208.0000 - mean_absolute_error: 8682.0156
95/95 [==============================] - 0s 988us/step - loss: 109268208.0000 - mean_absolute_error: 8682.0156
Epoch 66/100
95/95 [==============================] - 0s 925us/step - loss: 109000784.0000 - mean_absolute_error: 8666.3809
95/95 [==============================] - 0s 943us/step - loss: 109000784.0000 - mean_absolute_error: 8666.3809
Epoch 67/100
95/95 [==============================] - 0s 936us/step - loss: 108735552.0000 - mean_absolute_error: 8650.5830
95/95 [==============================] - 0s 955us/step - loss: 108735552.0000 - mean_absolute_error: 8650.5830
Epoch 68/100
95/95 [==============================] - 0s 984us/step - loss: 108473504.0000 - mean_absolute_error: 8634.6211
95/95 [==============================] - 0s 1ms/step - loss: 108473504.0000 - mean_absolute_error: 8634.6211  
Epoch 69/100
95/95 [==============================] - 0s 931us/step - loss: 108215616.0000 - mean_absolute_error: 8618.6055
95/95 [==============================] - 0s 949us/step - loss: 108215616.0000 - mean_absolute_error: 8618.6055
Epoch 70/100
95/95 [==============================] - 0s 933us/step - loss: 107961136.0000 - mean_absolute_error: 8602.4404
95/95 [==============================] - 0s 951us/step - loss: 107961136.0000 - mean_absolute_error: 8602.4404
Epoch 71/100
95/95 [==============================] - 0s 989us/step - loss: 107712888.0000 - mean_absolute_error: 8586.3633
95/95 [==============================] - 0s 1ms/step - loss: 107712888.0000 - mean_absolute_error: 8586.3633  
Epoch 72/100
95/95 [==============================] - 0s 1ms/step - loss: 107470368.0000 - mean_absolute_error: 8570.3770
95/95 [==============================] - 0s 1ms/step - loss: 107470368.0000 - mean_absolute_error: 8570.3770
Epoch 73/100
95/95 [==============================] - 0s 939us/step - loss: 107234048.0000 - mean_absolute_error: 8554.4971
95/95 [==============================] - 0s 957us/step - loss: 107234048.0000 - mean_absolute_error: 8554.4971
Epoch 74/100
95/95 [==============================] - 0s 921us/step - loss: 107004864.0000 - mean_absolute_error: 8538.8564
95/95 [==============================] - 0s 939us/step - loss: 107004864.0000 - mean_absolute_error: 8538.8564
Epoch 75/100
95/95 [==============================] - 0s 918us/step - loss: 106782792.0000 - mean_absolute_error: 8523.4014
95/95 [==============================] - 0s 936us/step - loss: 106782792.0000 - mean_absolute_error: 8523.4014
Epoch 76/100
95/95 [==============================] - 0s 956us/step - loss: 106569632.0000 - mean_absolute_error: 8508.3027
95/95 [==============================] - 0s 975us/step - loss: 106569632.0000 - mean_absolute_error: 8508.3027
Epoch 77/100
95/95 [==============================] - 0s 1ms/step - loss: 106365568.0000 - mean_absolute_error: 8493.6504
95/95 [==============================] - 0s 1ms/step - loss: 106365568.0000 - mean_absolute_error: 8493.6504
Epoch 78/100
95/95 [==============================] - 0s 944us/step - loss: 106171016.0000 - mean_absolute_error: 8479.5244
95/95 [==============================] - 0s 962us/step - loss: 106171016.0000 - mean_absolute_error: 8479.5244
Epoch 79/100
95/95 [==============================] - 0s 971us/step - loss: 105986368.0000 - mean_absolute_error: 8465.9082
95/95 [==============================] - 0s 990us/step - loss: 105986368.0000 - mean_absolute_error: 8465.9082
Epoch 80/100
95/95 [==============================] - 0s 929us/step - loss: 105811712.0000 - mean_absolute_error: 8452.8301
95/95 [==============================] - 0s 948us/step - loss: 105811712.0000 - mean_absolute_error: 8452.8301
Epoch 81/100
95/95 [==============================] - 0s 922us/step - loss: 105647192.0000 - mean_absolute_error: 8440.3086
95/95 [==============================] - 0s 940us/step - loss: 105647192.0000 - mean_absolute_error: 8440.3086
Epoch 82/100
95/95 [==============================] - 0s 932us/step - loss: 105492776.0000 - mean_absolute_error: 8428.4375
95/95 [==============================] - 0s 950us/step - loss: 105492776.0000 - mean_absolute_error: 8428.4375
Epoch 83/100
95/95 [==============================] - 0s 988us/step - loss: 105348512.0000 - mean_absolute_error: 8417.1641
95/95 [==============================] - 0s 1ms/step - loss: 105348512.0000 - mean_absolute_error: 8417.1641  
Epoch 84/100
95/95 [==============================] - 0s 950us/step - loss: 105214312.0000 - mean_absolute_error: 8406.4951
95/95 [==============================] - 0s 970us/step - loss: 105214312.0000 - mean_absolute_error: 8406.4951
Epoch 85/100
95/95 [==============================] - 0s 948us/step - loss: 105089712.0000 - mean_absolute_error: 8396.4102
95/95 [==============================] - 0s 967us/step - loss: 105089712.0000 - mean_absolute_error: 8396.4102
Epoch 86/100
95/95 [==============================] - 0s 941us/step - loss: 104974584.0000 - mean_absolute_error: 8387.0312
95/95 [==============================] - 0s 959us/step - loss: 104974584.0000 - mean_absolute_error: 8387.0312
Epoch 87/100
95/95 [==============================] - 0s 942us/step - loss: 104868616.0000 - mean_absolute_error: 8378.3877
95/95 [==============================] - 0s 960us/step - loss: 104868616.0000 - mean_absolute_error: 8378.3877
Epoch 88/100
95/95 [==============================] - 0s 934us/step - loss: 104771272.0000 - mean_absolute_error: 8370.4277
95/95 [==============================] - 0s 953us/step - loss: 104771272.0000 - mean_absolute_error: 8370.4277
Epoch 89/100
95/95 [==============================] - 0s 1ms/step - loss: 104680832.0000 - mean_absolute_error: 8362.9580
95/95 [==============================] - 0s 1ms/step - loss: 104680832.0000 - mean_absolute_error: 8362.9580
Epoch 90/100
95/95 [==============================] - 0s 979us/step - loss: 104598560.0000 - mean_absolute_error: 8356.1465
95/95 [==============================] - 0s 1ms/step - loss: 104598560.0000 - mean_absolute_error: 8356.1465  
Epoch 91/100
95/95 [==============================] - 0s 922us/step - loss: 104523456.0000 - mean_absolute_error: 8349.9062
95/95 [==============================] - 0s 941us/step - loss: 104523456.0000 - mean_absolute_error: 8349.9062
Epoch 92/100
95/95 [==============================] - 0s 930us/step - loss: 104454936.0000 - mean_absolute_error: 8344.2324
95/95 [==============================] - 0s 949us/step - loss: 104454936.0000 - mean_absolute_error: 8344.2324
Epoch 93/100
95/95 [==============================] - 0s 959us/step - loss: 104392472.0000 - mean_absolute_error: 8339.0957
95/95 [==============================] - 0s 978us/step - loss: 104392472.0000 - mean_absolute_error: 8339.0957
Epoch 94/100
95/95 [==============================] - 0s 956us/step - loss: 104335296.0000 - mean_absolute_error: 8334.4580
95/95 [==============================] - 0s 975us/step - loss: 104335296.0000 - mean_absolute_error: 8334.4580
Epoch 95/100
95/95 [==============================] - 0s 947us/step - loss: 104283184.0000 - mean_absolute_error: 8330.2773
95/95 [==============================] - 0s 965us/step - loss: 104283184.0000 - mean_absolute_error: 8330.2773
Epoch 96/100
95/95 [==============================] - 0s 1ms/step - loss: 104235448.0000 - mean_absolute_error: 8326.5146
95/95 [==============================] - 0s 1ms/step - loss: 104235448.0000 - mean_absolute_error: 8326.5146
Epoch 97/100
95/95 [==============================] - 0s 930us/step - loss: 104191712.0000 - mean_absolute_error: 8323.1533
95/95 [==============================] - 0s 948us/step - loss: 104191712.0000 - mean_absolute_error: 8323.1533
Epoch 98/100
95/95 [==============================] - 0s 922us/step - loss: 104151216.0000 - mean_absolute_error: 8320.1318
95/95 [==============================] - 0s 941us/step - loss: 104151216.0000 - mean_absolute_error: 8320.1318
Epoch 99/100
95/95 [==============================] - 0s 928us/step - loss: 104113784.0000 - mean_absolute_error: 8317.3926
95/95 [==============================] - 0s 948us/step - loss: 104113784.0000 - mean_absolute_error: 8317.3926
Epoch 100/100
95/95 [==============================] - 0s 1ms/step - loss: 104079080.0000 - mean_absolute_error: 8314.9043
95/95 [==============================] - 0s 1ms/step - loss: 104079080.0000 - mean_absolute_error: 8314.9043
[1] "step4"
[1] 4675
[1] "step5"
> > > [1] "Score final : 11093.7846361791"
> > + + > > > > > 
── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character()
)
ℹ Use `spec()` for the full column specifications.

> 
── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character(),
  Usage = col_character()
)
ℹ Use `spec()` for the full column specifications.

> > > > . + > > > > > > > > > > + + > > + >        1        2        3        4        5        6        7        8 
51880.11 51703.77 51524.16 51341.04 51154.24 50963.64 50769.21 50570.97 
       9       10       11       12       13       14       15       16 
50369.02 50163.54 49954.77 49742.99 49528.59 49311.98 49093.64 48874.12 
      17       18       19       20       21       22       23       24 
48653.99 48433.88 48214.44 47996.38 47780.41 47567.27 47357.70 47152.48 
      25       26       27       28       29       30       31       32 
46952.34 46758.05 46570.35 46389.94 46217.53 46053.77 45899.27 45754.63 
      33       34       35       36       37       38       39       40 
45620.35 45496.93 45384.76 45284.21 45195.54 45118.98 45054.67 45002.67 
      41       42       43       44       45       46       47       48 
44962.97 44935.49 44920.05 44916.41 44924.26 44943.20 44972.77 45012.42 
      49       50       51       52       53       54       55       56 
45061.56 45119.52 45185.56 45258.92 45338.76 45424.21 45514.37 45608.27 
      57       58       59       60       61       62       63       64 
45704.97 45803.47 45902.79 46001.91 46099.84 46195.59 46288.19 46376.70 
      65       66       67       68       69       70       71       72 
46460.19 46537.78 46608.64 46671.98 46727.07 46773.23 46809.87 46836.44 
      73       74       75       76       77       78       79       80 
46852.48 46857.60 46851.51 46833.98 46804.86 46764.12 46711.77 46647.93 
      81       82       83       84       85       86       87       88 
46572.82 46486.71 46389.96 46283.04 46166.45 46040.78 45906.71 45764.95 
      89       90       91       92       93       94       95       96 
45616.28 45461.54 45301.60 45137.39 44969.85 44799.96 44628.73 44457.16 
      97       98       99      100      101      102      103      104 
44286.26 44117.06 43950.54 43787.71 43629.52 43476.90 43330.75 43191.93 
     105      106      107      108      109      110      111      112 
43061.23 42939.41 42827.15 42725.09 42633.77 42553.68 42485.22 42428.74 
     113      114      115      116      117      118      119      120 
42384.47 42352.59 42333.19 42326.27 42331.75 42349.49 42379.25 42420.73 
     121      122      123      124      125      126      127      128 
42473.53 42537.21 42611.26 42695.09 42788.09 42889.56 42998.78 43114.99 
     129      130      131      132      133      134      135      136 
43237.41 43365.19 43497.53 43633.56 43772.44 43913.32 44055.38 44197.78 
     137      138      139      140      141      142      143      144 
44339.75 44480.53 44619.39 44755.66 44888.72 45017.99 45142.96 45263.21 
     145      146      147      148      149      150      151      152 
45378.34 45488.06 45592.16 45690.47 45782.94 45869.58 45950.49 46025.85 
     153      154      155      156      157      158      159      160 
46095.92 46161.03 46221.60 46278.12 46331.14 46381.30 46429.28 46475.82 
     161      162      163      164      165      166      167      168 
46521.73 46567.84 46615.04 46664.23 46716.37 46772.39 46833.28 46900.00 
     169      170      171      172      173      174      175      176 
46973.52 47054.78 47144.72 47244.25 47354.22 47475.47 47608.77 47754.84 
     177      178      179      180      181      182      183      184 
47914.32 48087.80 48275.80 48478.73 48696.96 48930.72 49180.19 49445.44 
     185      186      187      188      189      190      191      192 
49726.43 50023.05 50335.07 50662.18 51003.94 51359.86 51729.31 52111.61 
     193      194      195      196      197      198      199      200 
52505.96 52911.49 53327.27 53752.25 54185.36 54625.45 55071.31 55521.70 
     201      202      203      204      205      206      207      208 
55975.32 56430.86 56886.99 57342.33 57795.55 58245.27 58690.16 59128.90 
     209      210      211      212      213      214      215      216 
59560.19 59982.78 60395.46 60797.08 61186.56 61562.85 61925.03 62272.23 
     217      218      219      220      221      222      223      224 
62603.67 62918.67 63216.65 63497.12 63759.69 64004.11 64230.21 64437.93 
     225      226      227      228      229      230      231      232 
64627.33 64798.57 64951.93 65087.79 65206.63 65309.03 65395.67 65467.32 
     233      234      235      236      237      238      239      240 
65524.83 65569.13 65601.23 65622.20 65633.16 65635.30 65629.82 65617.98 
     241      242      243      244      245      246      247      248 
65601.06 65580.33 65557.10 65532.65 65508.26 65485.18 65464.64 65447.81 
     249      250      251      252      253      254      255      256 
65435.82 65429.76 65430.64 65439.38 65456.87 65483.85 65521.04 65569.00 
     257      258      259      260      261      262      263      264 
65628.23 65699.11 65781.91 65876.81 65983.85 66102.98 66234.04 66376.74 
     265      266      267      268      269      270      271      272 
66530.69 66695.39 66870.26 67054.57 67247.55 67448.30 67655.85 67869.15 
     273      274      275 
68087.08 68308.45 68532.02 
> > > > > > > . + > + + > > Warning message:
Unknown or uninitialised column: `time`. 
> Warning message:
Unknown or uninitialised column: `time`. 
> > > > > > Erreur : objet 'pred.fourier' introuvable
> > > . + > > Erreur : objet 'pred.total.test' introuvable
> Warning message:
Unknown or uninitialised column: `Id`. 
> Error in data.frame(Load, Id) : objet 'Load' introuvable
> > Error in is.data.frame(x) : objet 'submission' introuvable
> > > + > + > > > > > > Erreur : objet 'pred.fourier' introuvable
> > + > 
Call:
lm(formula = Load ~ Load.1 + Load.7 + Temp + Temp_s95 + WeekDays + 
    GovernmentResponseIndex, data = train)

Residuals:
     Min       1Q   Median       3Q      Max 
-10526.2  -1304.4    147.1   1369.5   6430.4 

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
(Intercept)              1.957e+04  5.333e+02  36.687  < 2e-16 ***
Load.1                   7.261e-01  9.519e-03  76.273  < 2e-16 ***
Load.7                   8.780e-02  8.258e-03  10.633  < 2e-16 ***
Temp                    -6.524e+02  6.980e+01  -9.346  < 2e-16 ***
Temp_s95                 3.229e+02  7.229e+01   4.467 8.24e-06 ***
WeekDays                -1.309e+03  2.290e+01 -57.167  < 2e-16 ***
GovernmentResponseIndex -3.480e+01  5.039e+00  -6.906 6.05e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2134 on 3021 degrees of freedom
Multiple R-squared:  0.9626,	Adjusted R-squared:  0.9626 
F-statistic: 1.297e+04 on 6 and 3021 DF,  p-value: < 2.2e-16

> > . + > 
Family: gaussian 
Link function: identity 

Formula:
Load ~ s(Load.1) + s(Load.7) + s(Temp) + s(Temp_s95) + s(WeekDays, 
    k = 7) + s(GovernmentResponseIndex)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 54614.19      25.79    2118   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
                             edf Ref.df       F  p-value    
s(Load.1)                  4.853  6.077 646.680  < 2e-16 ***
s(Load.7)                  7.619  8.534  16.211  < 2e-16 ***
s(Temp)                    6.900  7.787  40.091  < 2e-16 ***
s(Temp_s95)                6.006  7.103   3.964 0.000244 ***
s(WeekDays)                5.984  6.000 937.534  < 2e-16 ***
s(GovernmentResponseIndex) 2.199  2.552  41.304  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.983   Deviance explained = 98.4%
GCV = 2.0365e+06  Scale est. = 2.0132e+06  n = 3028
> > > > [1] "Load and format  ./data/train_V2.csv"
[1] "Load and format  ./data/test_V2.csv"
> > + + > Erreur : objet 'cv' introuvable
> > + + Erreur : objet 'pred.test' introuvable
> Erreur : objet 'Load' introuvable
> Error in data.frame(Load, Id) : objet 'Load' introuvable
> > Error in is.data.frame(x) : objet 'submission' introuvable
> > > > > Erreur : Can't negate columns that don't exist.
✖ Location 23 doesn't exist.
ℹ There are only 22 columns.
Run `rlang::last_error()` to see where the error occurred.
> Erreur : Can't negate columns that don't exist.
✖ Locations 23 and 24 don't exist.
ℹ There are only 22 columns.
Run `rlang::last_error()` to see where the error occurred.
> > . + > > > Error in is.data.frame(frame) : objet 'train.lstm' introuvable
> > . + Error in se(actual, predicted) : objet 'pred.lstm' introuvable
> function (pred, obs, na.rm = FALSE) 
sqrt(mean((pred - obs)^2, na.rm = na.rm))
<bytecode: 0x55e21f1ece48>
<environment: namespace:caret>
> > > > > + > Error in eval(predvars, data, env) : objet 'fourier.fitted' introuvable
> > . + > > > Error in se(actual, predicted) : objet 'pred.test.rf' introuvable
> function (pred, obs, na.rm = FALSE) 
sqrt(mean((pred - obs)^2, na.rm = na.rm))
<bytecode: 0x55e21f1ece48>
<environment: namespace:caret>
> > > > > rm(list=objects())
> graphics.off()
> libs.to.load = c("icesTAF", "tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
     icesTAF    tidyverse    lubridate       ranger       pracma      Metrics 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
        mgcv        keras       visreg        caret         mc2d        opera 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       abind randomForest   tensorflow 
        TRUE         TRUE         TRUE 
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> sapply(files.sources, source)
$days_to_numeric.r
$days_to_numeric.r$value
function (data) {
    data$WeekDays[data$WeekDays=='Monday']    = 1
    data$WeekDays[data$WeekDays=='Tuesday']   = 2
    data$WeekDays[data$WeekDays=='Wednesday'] = 3
    data$WeekDays[data$WeekDays=='Thursday']  = 4
    data$WeekDays[data$WeekDays=='Friday']    = 5
    data$WeekDays[data$WeekDays=='Saturday']  = 6
    data$WeekDays[data$WeekDays=='Sunday']    = 7
    data$WeekDays = as.numeric(data$WeekDays)
    return (data$WeekDays)
}

$days_to_numeric.r$visible
[1] FALSE


$evaluate.r
$evaluate.r$value
function(test_label, predicted_set){
    
    return(rmse(test_label, predicted_set))
}

$evaluate.r$visible
[1] FALSE


$format_data.r
$format_data.r$value
function(file_train, file_test) {

    print(paste("Load and format " , file_train, sep = " "))
    train_set = read_csv(file_train, col_types = cols())

    train_set$WeekDays[train_set$WeekDays == "Monday"] <- 1
    train_set$WeekDays[train_set$WeekDays == "Tuesday"] <- 2
    train_set$WeekDays[train_set$WeekDays == "Wednesday"] <- 3
    train_set$WeekDays[train_set$WeekDays == "Thursday"] <- 4
    train_set$WeekDays[train_set$WeekDays == "Friday"] <- 5
    train_set$WeekDays[train_set$WeekDays == "Saturday"] <- 6
    train_set$WeekDays[train_set$WeekDays == "Sunday"] <- 7
    train_set$WeekDays = as.integer(train_set$WeekDays)

    train_set$Year = NULL
    train_set$Date = NULL
    train_label = data.matrix(train_set$Load)


    train_set$Load = NULL
    
    train_set = data.matrix(train_set)


    print(paste("Load and format " , file_test, sep = " "))    
    test_set = read_csv(file_test, col_types = cols())

    test_set$WeekDays[test_set$WeekDays == "Monday"] <- 1
    test_set$WeekDays[test_set$WeekDays == "Tuesday"] <- 2
    test_set$WeekDays[test_set$WeekDays == "Wednesday"] <- 3
    test_set$WeekDays[test_set$WeekDays == "Thursday"] <- 4
    test_set$WeekDays[test_set$WeekDays == "Friday"] <- 5
    test_set$WeekDays[test_set$WeekDays == "Saturday"] <- 6
    test_set$WeekDays[test_set$WeekDays == "Sunday"] <- 7
    test_set$WeekDays = as.integer(test_set$WeekDays)

    test_label = test_set$Load.1
    tmp = test_set$Load.1
    for (i in c(1:(length(tmp)-1))){
        test_label[i] = tmp[i+1]
    }
    
    test_set$Year = NULL
    test_set$Date = NULL
    test_set$Id = NULL
    test_set$Usage = NULL
    test_set = data.matrix(test_set)
    

    test_set = data.matrix(test_set)
    return(list("train_set" = train_set, "train_label" = train_label, "test_set" = test_set, "test_label" = test_label))
}

$format_data.r$visible
[1] FALSE


$fourier.r
$fourier.r$value
function(train, test, plt = FALSE){
    total.time = c(1:(nrow(train)+nrow(test)))
    length(total.time)
    train$time = total.time[1:nrow(train)]
    test$time = tail(total.time,nrow(test))

    fourier.make.matrix = function (t, k, period) {
        w = 2*pi/period
        ret = cbind(cos(w*t), sin(w*t))
        
        for(i in c(2:K))
        {
            ret = cbind(ret, cos(i*w*t), sin(i*w*t))
        }
        return (ret)
    }

    K = 5; period = 365
    fourier.train = fourier.make.matrix(train$time, K, period)
    fourier.test = fourier.make.matrix(test$time, K, period)

    fourier.train.df = data.frame(train$Load,fourier.train)
    fourier.test.df = data.frame(fourier.test)

    reg = lm(train.Load ~., data=fourier.train.df)

    pred.fourier = predict(reg, newdata=fourier.test.df)
    total.fourier = c(reg$fitted,pred.fourier)
    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(reg$fitted,col='red', lwd=2)
        lines(test$time,pred.fourier,col='green', lwd=2)
    }
    return(pred.fourier)
}

$fourier.r$visible
[1] FALSE


$lstm.r
$lstm.r$value
function(train_set, train_label, test_set){
  N = length(train_set[,'Load.1'])
  nb_var = 17
  y = train_label
  x = array(train_set, dim = c(N, nb_var, 1))
  x_test = array(test_set,dim = c(length(test_set[,'Load.1']), nb_var, 1))
  print("step1")
  model = keras_model_sequential() %>%   
    #layer_lstm(units=128, input_shape=c(nb_var, 1), activation="relu", return_sequences = TRUE) %>% 
    layer_dense(units=64, activation = "relu") %>%  
    layer_dense(units=32) %>%  
    layer_dense(units=1, activation = "linear")
  print("step2")
  model %>% compile(loss = 'mse',
                    optimizer = 'adam',
                    metrics = list("mean_absolute_error")
  )
  print("step3")
  model %>% fit(x,y, epochs=100, batch_size=32, shuffle = FALSE)
  print("step4")
  print(length(x_test))
  pred = model %>% predict(x_test)
  print("step5")
  return(pred)
    
}

$lstm.r$visible
[1] FALSE


$main.r
NULL

$scale.r
$scale.r$value
function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

$scale.r$visible
[1] FALSE


$xgboost.r
$xgboost.r$value
function(train_set, train_label, test_set){
  param = list(booster = "gblinear", objective = "reg:squarederror", eval_metric = "rmse", lambda = 0.0003, alpha = 0.0003, nthread = 2, eta = 0.1)
  
  print("Model : XGBOOST")
  
  xgbmodel = xgboost(data = train_set, label = train_label, nrounds = 200, params = param, verbose = 0)
  
  pred = predict(xgbmodel, test_set)
  
  return(pred)
}

$xgboost.r$visible
[1] FALSE


> plt = FALSE #(si on veut plot, mettre à TRUE)
> model = "lstm"
> data = format_data("./data/train_V2.csv", "./data/test_V2.csv")
[1] "Load and format  ./data/train_V2.csv"
[1] "Load and format  ./data/test_V2.csv"
> train_set = data$train_set
> test_set = data$test_set
> train_label = data$train_label
> test_label = data$test_label
> if (model == "xgboost"){
+   pred = xgboost_rte(train_set, train_label, test_set)
+ } else if(model == "lstm"){
+   pred = lstm(train_set, train_label, test_set)
+ }
[1] "step1"
[1] "step2"
[1] "step3"
Epoch 1/100
95/95 [==============================] - 0s 995us/step - loss: 2762843648.0000 - mean_absolute_error: 49189.0586
95/95 [==============================] - 0s 1ms/step - loss: 2762843648.0000 - mean_absolute_error: 49189.0586  
Epoch 2/100
95/95 [==============================] - 0s 1ms/step - loss: 2740456192.0000 - mean_absolute_error: 48555.6602
95/95 [==============================] - 0s 1ms/step - loss: 2740456192.0000 - mean_absolute_error: 48555.6602
Epoch 3/100
95/95 [==============================] - 0s 977us/step - loss: 2739889664.0000 - mean_absolute_error: 48563.1406
95/95 [==============================] - 0s 995us/step - loss: 2739889664.0000 - mean_absolute_error: 48563.1406
Epoch 4/100
95/95 [==============================] - 0s 969us/step - loss: 2738458368.0000 - mean_absolute_error: 48561.1094
95/95 [==============================] - 0s 989us/step - loss: 2738458368.0000 - mean_absolute_error: 48561.1094
Epoch 5/100
95/95 [==============================] - 0s 1ms/step - loss: 2736604160.0000 - mean_absolute_error: 48579.5352
95/95 [==============================] - 0s 1ms/step - loss: 2736604160.0000 - mean_absolute_error: 48579.5352
Epoch 6/100
95/95 [==============================] - 0s 1ms/step - loss: 2734507264.0000 - mean_absolute_error: 48576.7656
95/95 [==============================] - 0s 1ms/step - loss: 2734507264.0000 - mean_absolute_error: 48576.7656
Epoch 7/100
95/95 [==============================] - 0s 965us/step - loss: 2731869696.0000 - mean_absolute_error: 48575.4023
95/95 [==============================] - 0s 988us/step - loss: 2731869696.0000 - mean_absolute_error: 48575.4023
Epoch 8/100
95/95 [==============================] - 0s 968us/step - loss: 2728173312.0000 - mean_absolute_error: 48534.6914
95/95 [==============================] - 0s 989us/step - loss: 2728173312.0000 - mean_absolute_error: 48534.6914
Epoch 9/100
95/95 [==============================] - 0s 966us/step - loss: 2722643456.0000 - mean_absolute_error: 48457.0234
95/95 [==============================] - 0s 984us/step - loss: 2722643456.0000 - mean_absolute_error: 48457.0234
Epoch 10/100
95/95 [==============================] - 0s 964us/step - loss: 2714194944.0000 - mean_absolute_error: 48351.5234
95/95 [==============================] - 0s 982us/step - loss: 2714194944.0000 - mean_absolute_error: 48351.5234
Epoch 11/100
95/95 [==============================] - 0s 973us/step - loss: 2701398016.0000 - mean_absolute_error: 48235.8164
95/95 [==============================] - 0s 993us/step - loss: 2701398016.0000 - mean_absolute_error: 48235.8164
Epoch 12/100
95/95 [==============================] - 0s 1ms/step - loss: 2680871168.0000 - mean_absolute_error: 48043.2305
95/95 [==============================] - 0s 1ms/step - loss: 2680871168.0000 - mean_absolute_error: 48043.2305
Epoch 13/100
95/95 [==============================] - 0s 975us/step - loss: 2650467328.0000 - mean_absolute_error: 47822.2422
95/95 [==============================] - 0s 994us/step - loss: 2650467328.0000 - mean_absolute_error: 47822.2422
Epoch 14/100
95/95 [==============================] - 0s 1ms/step - loss: 2606683392.0000 - mean_absolute_error: 47415.9844
95/95 [==============================] - 0s 1ms/step - loss: 2606683392.0000 - mean_absolute_error: 47415.9844
Epoch 15/100
95/95 [==============================] - 0s 998us/step - loss: 2546751744.0000 - mean_absolute_error: 46897.5898
95/95 [==============================] - 0s 1ms/step - loss: 2546751744.0000 - mean_absolute_error: 46897.5898  
Epoch 16/100
95/95 [==============================] - 0s 958us/step - loss: 2472935168.0000 - mean_absolute_error: 46266.5742
95/95 [==============================] - 0s 976us/step - loss: 2472935168.0000 - mean_absolute_error: 46266.5742
Epoch 17/100
95/95 [==============================] - 0s 967us/step - loss: 2402603520.0000 - mean_absolute_error: 45785.0625
95/95 [==============================] - 0s 985us/step - loss: 2402603520.0000 - mean_absolute_error: 45785.0625
Epoch 18/100
95/95 [==============================] - 0s 1ms/step - loss: 2287712768.0000 - mean_absolute_error: 44352.5508
95/95 [==============================] - 0s 1ms/step - loss: 2287712768.0000 - mean_absolute_error: 44352.5508
Epoch 19/100
95/95 [==============================] - 0s 994us/step - loss: 2173000192.0000 - mean_absolute_error: 43268.0430
95/95 [==============================] - 0s 1ms/step - loss: 2173000192.0000 - mean_absolute_error: 43268.0430  
Epoch 20/100
95/95 [==============================] - 0s 963us/step - loss: 2038336640.0000 - mean_absolute_error: 41845.8945
95/95 [==============================] - 0s 983us/step - loss: 2038336640.0000 - mean_absolute_error: 41845.8945
Epoch 21/100
95/95 [==============================] - 0s 969us/step - loss: 1893930880.0000 - mean_absolute_error: 40325.1406
95/95 [==============================] - 0s 987us/step - loss: 1893930880.0000 - mean_absolute_error: 40325.1406
Epoch 22/100
95/95 [==============================] - 0s 941us/step - loss: 1754636928.0000 - mean_absolute_error: 39066.5508
95/95 [==============================] - 0s 963us/step - loss: 1754636928.0000 - mean_absolute_error: 39066.5508
Epoch 23/100
95/95 [==============================] - 0s 991us/step - loss: 1595763456.0000 - mean_absolute_error: 37106.9609
95/95 [==============================] - 0s 1ms/step - loss: 1595763456.0000 - mean_absolute_error: 37106.9609  
Epoch 24/100
95/95 [==============================] - 0s 1ms/step - loss: 1427120896.0000 - mean_absolute_error: 34708.2031
95/95 [==============================] - 0s 1ms/step - loss: 1427120896.0000 - mean_absolute_error: 34708.2031
Epoch 25/100
95/95 [==============================] - 0s 960us/step - loss: 1394913152.0000 - mean_absolute_error: 35055.5273
95/95 [==============================] - 0s 978us/step - loss: 1394913152.0000 - mean_absolute_error: 35055.5273
Epoch 26/100
95/95 [==============================] - 0s 994us/step - loss: 1159300992.0000 - mean_absolute_error: 31089.3535
95/95 [==============================] - 0s 1ms/step - loss: 1159300992.0000 - mean_absolute_error: 31089.3535  
Epoch 27/100
95/95 [==============================] - 0s 945us/step - loss: 1033781824.0000 - mean_absolute_error: 29343.4590
95/95 [==============================] - 0s 963us/step - loss: 1033781824.0000 - mean_absolute_error: 29343.4590
Epoch 28/100
95/95 [==============================] - 0s 954us/step - loss: 910351552.0000 - mean_absolute_error: 27110.1484
95/95 [==============================] - 0s 972us/step - loss: 910351552.0000 - mean_absolute_error: 27110.1484
Epoch 29/100
95/95 [==============================] - 0s 961us/step - loss: 814320064.0000 - mean_absolute_error: 25664.9336
95/95 [==============================] - 0s 982us/step - loss: 814320064.0000 - mean_absolute_error: 25664.9336
Epoch 30/100
95/95 [==============================] - 0s 983us/step - loss: 715294656.0000 - mean_absolute_error: 23494.3535
95/95 [==============================] - 0s 1ms/step - loss: 715294656.0000 - mean_absolute_error: 23494.3535  
Epoch 31/100
95/95 [==============================] - 0s 980us/step - loss: 583289856.0000 - mean_absolute_error: 21021.2598
95/95 [==============================] - 0s 998us/step - loss: 583289856.0000 - mean_absolute_error: 21021.2598
Epoch 32/100
95/95 [==============================] - 0s 973us/step - loss: 486974656.0000 - mean_absolute_error: 18797.4258
95/95 [==============================] - 0s 991us/step - loss: 486974656.0000 - mean_absolute_error: 18797.4258
Epoch 33/100
95/95 [==============================] - 0s 950us/step - loss: 406852128.0000 - mean_absolute_error: 16685.0000
95/95 [==============================] - 0s 968us/step - loss: 406852128.0000 - mean_absolute_error: 16685.0000
Epoch 34/100
95/95 [==============================] - 0s 941us/step - loss: 340182848.0000 - mean_absolute_error: 14816.8535
95/95 [==============================] - 0s 959us/step - loss: 340182848.0000 - mean_absolute_error: 14816.8535
Epoch 35/100
95/95 [==============================] - 0s 946us/step - loss: 283781024.0000 - mean_absolute_error: 13100.2295
95/95 [==============================] - 0s 964us/step - loss: 283781024.0000 - mean_absolute_error: 13100.2295
Epoch 36/100
95/95 [==============================] - 0s 1ms/step - loss: 243637792.0000 - mean_absolute_error: 11950.8213
95/95 [==============================] - 0s 1ms/step - loss: 243637792.0000 - mean_absolute_error: 11950.8213
Epoch 37/100
95/95 [==============================] - 0s 968us/step - loss: 210830240.0000 - mean_absolute_error: 10953.3906
95/95 [==============================] - 0s 986us/step - loss: 210830240.0000 - mean_absolute_error: 10953.3906
Epoch 38/100
95/95 [==============================] - 0s 947us/step - loss: 184038528.0000 - mean_absolute_error: 10081.1094
95/95 [==============================] - 0s 965us/step - loss: 184038528.0000 - mean_absolute_error: 10081.1094
Epoch 39/100
95/95 [==============================] - 0s 947us/step - loss: 164290432.0000 - mean_absolute_error: 9466.0146
95/95 [==============================] - 0s 965us/step - loss: 164290432.0000 - mean_absolute_error: 9466.0146
Epoch 40/100
95/95 [==============================] - 0s 968us/step - loss: 146963936.0000 - mean_absolute_error: 8957.9404
95/95 [==============================] - 0s 989us/step - loss: 146963936.0000 - mean_absolute_error: 8957.9404
Epoch 41/100
95/95 [==============================] - 0s 957us/step - loss: 138869296.0000 - mean_absolute_error: 8922.4795
95/95 [==============================] - 0s 975us/step - loss: 138869296.0000 - mean_absolute_error: 8922.4795
Epoch 42/100
95/95 [==============================] - 0s 1ms/step - loss: 132017488.0000 - mean_absolute_error: 8881.8262
95/95 [==============================] - 0s 1ms/step - loss: 132017488.0000 - mean_absolute_error: 8881.8262
Epoch 43/100
95/95 [==============================] - 0s 954us/step - loss: 127570192.0000 - mean_absolute_error: 8893.1543
95/95 [==============================] - 0s 972us/step - loss: 127570192.0000 - mean_absolute_error: 8893.1543
Epoch 44/100
95/95 [==============================] - 0s 945us/step - loss: 124693840.0000 - mean_absolute_error: 8923.0986
95/95 [==============================] - 0s 965us/step - loss: 124693840.0000 - mean_absolute_error: 8923.0986
Epoch 45/100
95/95 [==============================] - 0s 945us/step - loss: 122206568.0000 - mean_absolute_error: 8938.1807
95/95 [==============================] - 0s 963us/step - loss: 122206568.0000 - mean_absolute_error: 8938.1807
Epoch 46/100
95/95 [==============================] - 0s 973us/step - loss: 120088064.0000 - mean_absolute_error: 8938.8232
95/95 [==============================] - 0s 992us/step - loss: 120088064.0000 - mean_absolute_error: 8938.8232
Epoch 47/100
95/95 [==============================] - 0s 976us/step - loss: 118670384.0000 - mean_absolute_error: 8943.7256
95/95 [==============================] - 0s 994us/step - loss: 118670384.0000 - mean_absolute_error: 8943.7256
Epoch 48/100
95/95 [==============================] - 0s 982us/step - loss: 117682008.0000 - mean_absolute_error: 8949.2354
95/95 [==============================] - 0s 1ms/step - loss: 117682008.0000 - mean_absolute_error: 8949.2354  
Epoch 49/100
95/95 [==============================] - 0s 1ms/step - loss: 116947216.0000 - mean_absolute_error: 8953.3096
95/95 [==============================] - 0s 1ms/step - loss: 116947216.0000 - mean_absolute_error: 8953.3096
Epoch 50/100
95/95 [==============================] - 0s 962us/step - loss: 116375224.0000 - mean_absolute_error: 8955.1641
95/95 [==============================] - 0s 980us/step - loss: 116375224.0000 - mean_absolute_error: 8955.1641
Epoch 51/100
95/95 [==============================] - 0s 944us/step - loss: 115911088.0000 - mean_absolute_error: 8954.6689
95/95 [==============================] - 0s 963us/step - loss: 115911088.0000 - mean_absolute_error: 8954.6689
Epoch 52/100
95/95 [==============================] - 0s 947us/step - loss: 115519448.0000 - mean_absolute_error: 8952.1006
95/95 [==============================] - 0s 965us/step - loss: 115519448.0000 - mean_absolute_error: 8952.1006
Epoch 53/100
95/95 [==============================] - 0s 941us/step - loss: 115177024.0000 - mean_absolute_error: 8947.7988
95/95 [==============================] - 0s 960us/step - loss: 115177024.0000 - mean_absolute_error: 8947.7988
Epoch 54/100
95/95 [==============================] - 0s 1ms/step - loss: 114867176.0000 - mean_absolute_error: 8942.0674
95/95 [==============================] - 0s 1ms/step - loss: 114867176.0000 - mean_absolute_error: 8942.0674
Epoch 55/100
95/95 [==============================] - 0s 998us/step - loss: 114579008.0000 - mean_absolute_error: 8935.1426
95/95 [==============================] - 0s 1ms/step - loss: 114579008.0000 - mean_absolute_error: 8935.1426  
Epoch 56/100
95/95 [==============================] - 0s 949us/step - loss: 114304600.0000 - mean_absolute_error: 8927.2051
95/95 [==============================] - 0s 969us/step - loss: 114304600.0000 - mean_absolute_error: 8927.2051
Epoch 57/100
95/95 [==============================] - 0s 994us/step - loss: 114038504.0000 - mean_absolute_error: 8918.3682
95/95 [==============================] - 0s 1ms/step - loss: 114038504.0000 - mean_absolute_error: 8918.3682  
Epoch 58/100
95/95 [==============================] - 0s 965us/step - loss: 113777224.0000 - mean_absolute_error: 8908.7695
95/95 [==============================] - 0s 983us/step - loss: 113777224.0000 - mean_absolute_error: 8908.7695
Epoch 59/100
95/95 [==============================] - 0s 956us/step - loss: 113518456.0000 - mean_absolute_error: 8898.5293
95/95 [==============================] - 0s 974us/step - loss: 113518456.0000 - mean_absolute_error: 8898.5293
Epoch 60/100
95/95 [==============================] - 0s 966us/step - loss: 113259768.0000 - mean_absolute_error: 8887.7148
95/95 [==============================] - 0s 985us/step - loss: 113259768.0000 - mean_absolute_error: 8887.7148
Epoch 61/100
95/95 [==============================] - 0s 959us/step - loss: 113000440.0000 - mean_absolute_error: 8876.3779
95/95 [==============================] - 0s 978us/step - loss: 113000440.0000 - mean_absolute_error: 8876.3779
Epoch 62/100
95/95 [==============================] - 0s 950us/step - loss: 112739440.0000 - mean_absolute_error: 8864.5684
95/95 [==============================] - 0s 968us/step - loss: 112739440.0000 - mean_absolute_error: 8864.5684
Epoch 63/100
95/95 [==============================] - 0s 947us/step - loss: 112476496.0000 - mean_absolute_error: 8852.3467
95/95 [==============================] - 0s 965us/step - loss: 112476496.0000 - mean_absolute_error: 8852.3467
Epoch 64/100
95/95 [==============================] - 0s 952us/step - loss: 112210896.0000 - mean_absolute_error: 8839.7031
95/95 [==============================] - 0s 972us/step - loss: 112210896.0000 - mean_absolute_error: 8839.7031
Epoch 65/100
95/95 [==============================] - 0s 979us/step - loss: 111943128.0000 - mean_absolute_error: 8826.7197
95/95 [==============================] - 0s 997us/step - loss: 111943128.0000 - mean_absolute_error: 8826.7197
Epoch 66/100
95/95 [==============================] - 0s 1ms/step - loss: 111673040.0000 - mean_absolute_error: 8813.4287
95/95 [==============================] - 0s 1ms/step - loss: 111673040.0000 - mean_absolute_error: 8813.4287
Epoch 67/100
95/95 [==============================] - 0s 1ms/step - loss: 111400680.0000 - mean_absolute_error: 8799.7822
95/95 [==============================] - 0s 1ms/step - loss: 111400680.0000 - mean_absolute_error: 8799.7822
Epoch 68/100
95/95 [==============================] - 0s 950us/step - loss: 111126232.0000 - mean_absolute_error: 8785.7744
95/95 [==============================] - 0s 969us/step - loss: 111126232.0000 - mean_absolute_error: 8785.7744
Epoch 69/100
95/95 [==============================] - 0s 952us/step - loss: 110850272.0000 - mean_absolute_error: 8771.4932
95/95 [==============================] - 0s 972us/step - loss: 110850272.0000 - mean_absolute_error: 8771.4932
Epoch 70/100
95/95 [==============================] - 0s 946us/step - loss: 110573016.0000 - mean_absolute_error: 8756.9365
95/95 [==============================] - 0s 967us/step - loss: 110573016.0000 - mean_absolute_error: 8756.9365
Epoch 71/100
95/95 [==============================] - 0s 1ms/step - loss: 110295040.0000 - mean_absolute_error: 8742.1230
95/95 [==============================] - 0s 1ms/step - loss: 110295040.0000 - mean_absolute_error: 8742.1230
Epoch 72/100
95/95 [==============================] - 0s 1ms/step - loss: 110016880.0000 - mean_absolute_error: 8727.0986
95/95 [==============================] - 0s 1ms/step - loss: 110016880.0000 - mean_absolute_error: 8727.0986
Epoch 73/100
95/95 [==============================] - 0s 948us/step - loss: 109738968.0000 - mean_absolute_error: 8711.8281
95/95 [==============================] - 0s 966us/step - loss: 109738968.0000 - mean_absolute_error: 8711.8281
Epoch 74/100
95/95 [==============================] - 0s 935us/step - loss: 109461984.0000 - mean_absolute_error: 8696.3447
95/95 [==============================] - 0s 953us/step - loss: 109461984.0000 - mean_absolute_error: 8696.3447
Epoch 75/100
95/95 [==============================] - 0s 966us/step - loss: 109186720.0000 - mean_absolute_error: 8680.7559
95/95 [==============================] - 0s 985us/step - loss: 109186720.0000 - mean_absolute_error: 8680.7559
Epoch 76/100
95/95 [==============================] - 0s 945us/step - loss: 108913680.0000 - mean_absolute_error: 8665.0254
95/95 [==============================] - 0s 965us/step - loss: 108913680.0000 - mean_absolute_error: 8665.0254
Epoch 77/100
95/95 [==============================] - 0s 944us/step - loss: 108643528.0000 - mean_absolute_error: 8649.2109
95/95 [==============================] - 0s 962us/step - loss: 108643528.0000 - mean_absolute_error: 8649.2109
Epoch 78/100
95/95 [==============================] - 0s 1ms/step - loss: 108376880.0000 - mean_absolute_error: 8633.3125
95/95 [==============================] - 0s 1ms/step - loss: 108376880.0000 - mean_absolute_error: 8633.3125
Epoch 79/100
95/95 [==============================] - 0s 976us/step - loss: 108112920.0000 - mean_absolute_error: 8617.2314
95/95 [==============================] - 0s 994us/step - loss: 108112920.0000 - mean_absolute_error: 8617.2314
Epoch 80/100
95/95 [==============================] - 0s 945us/step - loss: 107853928.0000 - mean_absolute_error: 8601.1455
95/95 [==============================] - 0s 964us/step - loss: 107853928.0000 - mean_absolute_error: 8601.1455
Epoch 81/100
95/95 [==============================] - 0s 947us/step - loss: 107600136.0000 - mean_absolute_error: 8585.0684
95/95 [==============================] - 0s 966us/step - loss: 107600136.0000 - mean_absolute_error: 8585.0684
Epoch 82/100
95/95 [==============================] - 0s 967us/step - loss: 107351792.0000 - mean_absolute_error: 8569.0156
95/95 [==============================] - 0s 987us/step - loss: 107351792.0000 - mean_absolute_error: 8569.0156
Epoch 83/100
95/95 [==============================] - 0s 976us/step - loss: 107109640.0000 - mean_absolute_error: 8553.0928
95/95 [==============================] - 0s 996us/step - loss: 107109640.0000 - mean_absolute_error: 8553.0928
Epoch 84/100
95/95 [==============================] - 0s 984us/step - loss: 106873880.0000 - mean_absolute_error: 8537.2744
95/95 [==============================] - 0s 1ms/step - loss: 106873880.0000 - mean_absolute_error: 8537.2744  
Epoch 85/100
95/95 [==============================] - 0s 955us/step - loss: 106645816.0000 - mean_absolute_error: 8521.7314
95/95 [==============================] - 0s 973us/step - loss: 106645816.0000 - mean_absolute_error: 8521.7314
Epoch 86/100
95/95 [==============================] - 0s 945us/step - loss: 106426416.0000 - mean_absolute_error: 8506.5547
95/95 [==============================] - 0s 965us/step - loss: 106426416.0000 - mean_absolute_error: 8506.5547
Epoch 87/100
95/95 [==============================] - 0s 945us/step - loss: 106215456.0000 - mean_absolute_error: 8491.7197
95/95 [==============================] - 0s 966us/step - loss: 106215456.0000 - mean_absolute_error: 8491.7197
Epoch 88/100
95/95 [==============================] - 0s 1ms/step - loss: 106013896.0000 - mean_absolute_error: 8477.3242
95/95 [==============================] - 0s 1ms/step - loss: 106013896.0000 - mean_absolute_error: 8477.3242
Epoch 89/100
95/95 [==============================] - 0s 950us/step - loss: 105821440.0000 - mean_absolute_error: 8463.3975
95/95 [==============================] - 0s 969us/step - loss: 105821440.0000 - mean_absolute_error: 8463.3975
Epoch 90/100
95/95 [==============================] - 0s 1000us/step - loss: 105636488.0000 - mean_absolute_error: 8449.8047
95/95 [==============================] - 0s 1ms/step - loss: 105636488.0000 - mean_absolute_error: 8449.8047   
Epoch 91/100
95/95 [==============================] - 0s 955us/step - loss: 105460432.0000 - mean_absolute_error: 8436.5889
95/95 [==============================] - 0s 975us/step - loss: 105460432.0000 - mean_absolute_error: 8436.5889
Epoch 92/100
95/95 [==============================] - 0s 940us/step - loss: 105295152.0000 - mean_absolute_error: 8423.9941
95/95 [==============================] - 0s 958us/step - loss: 105295152.0000 - mean_absolute_error: 8423.9941
Epoch 93/100
95/95 [==============================] - 0s 974us/step - loss: 105140296.0000 - mean_absolute_error: 8412.0850
95/95 [==============================] - 0s 992us/step - loss: 105140296.0000 - mean_absolute_error: 8412.0850
Epoch 94/100
95/95 [==============================] - 0s 942us/step - loss: 104995944.0000 - mean_absolute_error: 8400.8369
95/95 [==============================] - 0s 961us/step - loss: 104995944.0000 - mean_absolute_error: 8400.8369
Epoch 95/100
95/95 [==============================] - 0s 926us/step - loss: 104862304.0000 - mean_absolute_error: 8390.2881
95/95 [==============================] - 0s 945us/step - loss: 104862304.0000 - mean_absolute_error: 8390.2881
Epoch 96/100
95/95 [==============================] - 0s 977us/step - loss: 104739024.0000 - mean_absolute_error: 8380.4033
95/95 [==============================] - 0s 996us/step - loss: 104739024.0000 - mean_absolute_error: 8380.4033
Epoch 97/100
95/95 [==============================] - 0s 998us/step - loss: 104625488.0000 - mean_absolute_error: 8371.2412
95/95 [==============================] - 0s 1ms/step - loss: 104625488.0000 - mean_absolute_error: 8371.2412  
Epoch 98/100
95/95 [==============================] - 0s 940us/step - loss: 104521536.0000 - mean_absolute_error: 8362.8193
95/95 [==============================] - 0s 958us/step - loss: 104521536.0000 - mean_absolute_error: 8362.8193
Epoch 99/100
95/95 [==============================] - 0s 988us/step - loss: 104426408.0000 - mean_absolute_error: 8355.1611
95/95 [==============================] - 0s 1ms/step - loss: 104426408.0000 - mean_absolute_error: 8355.1611  
Epoch 100/100
95/95 [==============================] - 0s 939us/step - loss: 104339296.0000 - mean_absolute_error: 8348.1162
95/95 [==============================] - 0s 957us/step - loss: 104339296.0000 - mean_absolute_error: 8348.1162
[1] "step4"
[1] 4675
[1] "step5"
> print(paste("Score final : ", evaluate(test_label, pred), sep=""))
[1] "Score final : 11092.0706226001"
> if (plt) {
+     plot(c(train_label, pred))
+ }
> train <- read_delim(file="data/train_V2.csv",delim=',')

── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character()
)
ℹ Use `spec()` for the full column specifications.

> test <- read_delim(file="data/test_V2.csv",delim=',')

── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character(),
  Usage = col_character()
)
ℹ Use `spec()` for the full column specifications.

> reg = lm(Load~Temp, data=train)
> if (plt){
+     plot(train$Date, train$Load, type='l')
+     par(new=T)
+     plot(train$Date, train$Temp, type='l', col='red')
+     plot(train$Temp,train$Load, col='red')
+     points(train$Temp ,reg$fitted.values)
+ }
> train$WeekDays = days_to_numeric(train)
> test$WeekDays = days_to_numeric(test)
> train$Year = train$Year - 2012
> test$Year = test$Year - 2012
> if (plt){
+     plot(train$Date, train$Load, type='l')
+ }
> MA <- stats::filter(train$Load, filter = rep(1/365,365),
+              method = c("convolution"), sides = 2, circular = FALSE)
> if (plt){
+     plot(train$Date, train$Load, type = "l", xlab = "",
+          ylab = "consumption (kw)", col = "seagreen4", lwd = 1)
+     lines(train$Date, MA, col = "red", lwd = 2)
+ }
> fourier(train, test, plt = TRUE)
       1        2        3        4        5        6        7        8 
51880.11 51703.77 51524.16 51341.04 51154.24 50963.64 50769.21 50570.97 
       9       10       11       12       13       14       15       16 
50369.02 50163.54 49954.77 49742.99 49528.59 49311.98 49093.64 48874.12 
      17       18       19       20       21       22       23       24 
48653.99 48433.88 48214.44 47996.38 47780.41 47567.27 47357.70 47152.48 
      25       26       27       28       29       30       31       32 
46952.34 46758.05 46570.35 46389.94 46217.53 46053.77 45899.27 45754.63 
      33       34       35       36       37       38       39       40 
45620.35 45496.93 45384.76 45284.21 45195.54 45118.98 45054.67 45002.67 
      41       42       43       44       45       46       47       48 
44962.97 44935.49 44920.05 44916.41 44924.26 44943.20 44972.77 45012.42 
      49       50       51       52       53       54       55       56 
45061.56 45119.52 45185.56 45258.92 45338.76 45424.21 45514.37 45608.27 
      57       58       59       60       61       62       63       64 
45704.97 45803.47 45902.79 46001.91 46099.84 46195.59 46288.19 46376.70 
      65       66       67       68       69       70       71       72 
46460.19 46537.78 46608.64 46671.98 46727.07 46773.23 46809.87 46836.44 
      73       74       75       76       77       78       79       80 
46852.48 46857.60 46851.51 46833.98 46804.86 46764.12 46711.77 46647.93 
      81       82       83       84       85       86       87       88 
46572.82 46486.71 46389.96 46283.04 46166.45 46040.78 45906.71 45764.95 
      89       90       91       92       93       94       95       96 
45616.28 45461.54 45301.60 45137.39 44969.85 44799.96 44628.73 44457.16 
      97       98       99      100      101      102      103      104 
44286.26 44117.06 43950.54 43787.71 43629.52 43476.90 43330.75 43191.93 
     105      106      107      108      109      110      111      112 
43061.23 42939.41 42827.15 42725.09 42633.77 42553.68 42485.22 42428.74 
     113      114      115      116      117      118      119      120 
42384.47 42352.59 42333.19 42326.27 42331.75 42349.49 42379.25 42420.73 
     121      122      123      124      125      126      127      128 
42473.53 42537.21 42611.26 42695.09 42788.09 42889.56 42998.78 43114.99 
     129      130      131      132      133      134      135      136 
43237.41 43365.19 43497.53 43633.56 43772.44 43913.32 44055.38 44197.78 
     137      138      139      140      141      142      143      144 
44339.75 44480.53 44619.39 44755.66 44888.72 45017.99 45142.96 45263.21 
     145      146      147      148      149      150      151      152 
45378.34 45488.06 45592.16 45690.47 45782.94 45869.58 45950.49 46025.85 
     153      154      155      156      157      158      159      160 
46095.92 46161.03 46221.60 46278.12 46331.14 46381.30 46429.28 46475.82 
     161      162      163      164      165      166      167      168 
46521.73 46567.84 46615.04 46664.23 46716.37 46772.39 46833.28 46900.00 
     169      170      171      172      173      174      175      176 
46973.52 47054.78 47144.72 47244.25 47354.22 47475.47 47608.77 47754.84 
     177      178      179      180      181      182      183      184 
47914.32 48087.80 48275.80 48478.73 48696.96 48930.72 49180.19 49445.44 
     185      186      187      188      189      190      191      192 
49726.43 50023.05 50335.07 50662.18 51003.94 51359.86 51729.31 52111.61 
     193      194      195      196      197      198      199      200 
52505.96 52911.49 53327.27 53752.25 54185.36 54625.45 55071.31 55521.70 
     201      202      203      204      205      206      207      208 
55975.32 56430.86 56886.99 57342.33 57795.55 58245.27 58690.16 59128.90 
     209      210      211      212      213      214      215      216 
59560.19 59982.78 60395.46 60797.08 61186.56 61562.85 61925.03 62272.23 
     217      218      219      220      221      222      223      224 
62603.67 62918.67 63216.65 63497.12 63759.69 64004.11 64230.21 64437.93 
     225      226      227      228      229      230      231      232 
64627.33 64798.57 64951.93 65087.79 65206.63 65309.03 65395.67 65467.32 
     233      234      235      236      237      238      239      240 
65524.83 65569.13 65601.23 65622.20 65633.16 65635.30 65629.82 65617.98 
     241      242      243      244      245      246      247      248 
65601.06 65580.33 65557.10 65532.65 65508.26 65485.18 65464.64 65447.81 
     249      250      251      252      253      254      255      256 
65435.82 65429.76 65430.64 65439.38 65456.87 65483.85 65521.04 65569.00 
     257      258      259      260      261      262      263      264 
65628.23 65699.11 65781.91 65876.81 65983.85 66102.98 66234.04 66376.74 
     265      266      267      268      269      270      271      272 
66530.69 66695.39 66870.26 67054.57 67247.55 67448.30 67655.85 67869.15 
     273      274      275 
68087.08 68308.45 68532.02 
> num.years = 0
> average = 0
> N = 8; a = 0.7 #exponential weight
> while (365*(num.years+1) <= length(train$Date)) {
+     par(mfrow = c(1, 1))
+     dateyear = train$Date[(365*num.years+1):(365*(num.years+1))]
+     loadyear = train$Load[(365*num.years+1):(365*(num.years+1))]
+     MAw <- stats::filter(loadyear, filter = rep(1/52,52),
+                       method = c("convolution"), sides = 2, circular = T)
+     expweight = (((1-a)/(1-a^8))*a^(N-(num.years+1)))
+     average = expweight*(average + (loadyear - MAw))
+     num.years = num.years + 1
+ }
> if (plt){
+     plot(average)
+ }
> train.to.day = train$time %% 365 + 1
Warning message:
Unknown or uninitialised column: `time`. 
> test.to.day = test$time %% 365 + 1
Warning message:
Unknown or uninitialised column: `time`. 
> total.time = c(1:(nrow(train)+nrow(test)))
> length(total.time)
[1] 3303
> train$time = total.time[1:nrow(train)]
> test$time = tail(total.time,nrow(test))
> fourier(train, test, plt = TRUE)
       1        2        3        4        5        6        7        8 
51880.11 51703.77 51524.16 51341.04 51154.24 50963.64 50769.21 50570.97 
       9       10       11       12       13       14       15       16 
50369.02 50163.54 49954.77 49742.99 49528.59 49311.98 49093.64 48874.12 
      17       18       19       20       21       22       23       24 
48653.99 48433.88 48214.44 47996.38 47780.41 47567.27 47357.70 47152.48 
      25       26       27       28       29       30       31       32 
46952.34 46758.05 46570.35 46389.94 46217.53 46053.77 45899.27 45754.63 
      33       34       35       36       37       38       39       40 
45620.35 45496.93 45384.76 45284.21 45195.54 45118.98 45054.67 45002.67 
      41       42       43       44       45       46       47       48 
44962.97 44935.49 44920.05 44916.41 44924.26 44943.20 44972.77 45012.42 
      49       50       51       52       53       54       55       56 
45061.56 45119.52 45185.56 45258.92 45338.76 45424.21 45514.37 45608.27 
      57       58       59       60       61       62       63       64 
45704.97 45803.47 45902.79 46001.91 46099.84 46195.59 46288.19 46376.70 
      65       66       67       68       69       70       71       72 
46460.19 46537.78 46608.64 46671.98 46727.07 46773.23 46809.87 46836.44 
      73       74       75       76       77       78       79       80 
46852.48 46857.60 46851.51 46833.98 46804.86 46764.12 46711.77 46647.93 
      81       82       83       84       85       86       87       88 
46572.82 46486.71 46389.96 46283.04 46166.45 46040.78 45906.71 45764.95 
      89       90       91       92       93       94       95       96 
45616.28 45461.54 45301.60 45137.39 44969.85 44799.96 44628.73 44457.16 
      97       98       99      100      101      102      103      104 
44286.26 44117.06 43950.54 43787.71 43629.52 43476.90 43330.75 43191.93 
     105      106      107      108      109      110      111      112 
43061.23 42939.41 42827.15 42725.09 42633.77 42553.68 42485.22 42428.74 
     113      114      115      116      117      118      119      120 
42384.47 42352.59 42333.19 42326.27 42331.75 42349.49 42379.25 42420.73 
     121      122      123      124      125      126      127      128 
42473.53 42537.21 42611.26 42695.09 42788.09 42889.56 42998.78 43114.99 
     129      130      131      132      133      134      135      136 
43237.41 43365.19 43497.53 43633.56 43772.44 43913.32 44055.38 44197.78 
     137      138      139      140      141      142      143      144 
44339.75 44480.53 44619.39 44755.66 44888.72 45017.99 45142.96 45263.21 
     145      146      147      148      149      150      151      152 
45378.34 45488.06 45592.16 45690.47 45782.94 45869.58 45950.49 46025.85 
     153      154      155      156      157      158      159      160 
46095.92 46161.03 46221.60 46278.12 46331.14 46381.30 46429.28 46475.82 
     161      162      163      164      165      166      167      168 
46521.73 46567.84 46615.04 46664.23 46716.37 46772.39 46833.28 46900.00 
     169      170      171      172      173      174      175      176 
46973.52 47054.78 47144.72 47244.25 47354.22 47475.47 47608.77 47754.84 
     177      178      179      180      181      182      183      184 
47914.32 48087.80 48275.80 48478.73 48696.96 48930.72 49180.19 49445.44 
     185      186      187      188      189      190      191      192 
49726.43 50023.05 50335.07 50662.18 51003.94 51359.86 51729.31 52111.61 
     193      194      195      196      197      198      199      200 
52505.96 52911.49 53327.27 53752.25 54185.36 54625.45 55071.31 55521.70 
     201      202      203      204      205      206      207      208 
55975.32 56430.86 56886.99 57342.33 57795.55 58245.27 58690.16 59128.90 
     209      210      211      212      213      214      215      216 
59560.19 59982.78 60395.46 60797.08 61186.56 61562.85 61925.03 62272.23 
     217      218      219      220      221      222      223      224 
62603.67 62918.67 63216.65 63497.12 63759.69 64004.11 64230.21 64437.93 
     225      226      227      228      229      230      231      232 
64627.33 64798.57 64951.93 65087.79 65206.63 65309.03 65395.67 65467.32 
     233      234      235      236      237      238      239      240 
65524.83 65569.13 65601.23 65622.20 65633.16 65635.30 65629.82 65617.98 
     241      242      243      244      245      246      247      248 
65601.06 65580.33 65557.10 65532.65 65508.26 65485.18 65464.64 65447.81 
     249      250      251      252      253      254      255      256 
65435.82 65429.76 65430.64 65439.38 65456.87 65483.85 65521.04 65569.00 
     257      258      259      260      261      262      263      264 
65628.23 65699.11 65781.91 65876.81 65983.85 66102.98 66234.04 66376.74 
     265      266      267      268      269      270      271      272 
66530.69 66695.39 66870.26 67054.57 67247.55 67448.30 67655.85 67869.15 
     273      274      275 
68087.08 68308.45 68532.02 
> num.years = 0
> average = 0
> N = 8; a = 0.7 #exponential weight
> while (365*(num.years+1) <= length(train$Date)) {
+     par(mfrow = c(1, 1))
+     dateyear = train$Date[(365*num.years+1):(365*(num.years+1))]
+     loadyear = train$Load[(365*num.years+1):(365*(num.years+1))]
+     MAw <- stats::filter(loadyear, filter = rep(1/52,52),
+                       method = c("convolution"), sides = 2, circular = T)
+     expweight = (((1-a)/(1-a^8))*a^(N-(num.years+1)))
+     average = expweight*(average + (loadyear - MAw))
+     num.years = num.years + 1
+ }
> if (plt){
+     plot(average)
+ }
> train.to.day = train$time %% 365 + 1
> test.to.day = test$time %% 365 + 1
> pred.hebdo.train = average[train.to.day]
> pred.hebdo.test = average[test.to.day]
> pred.total.train = reg$fitted + pred.hebdo.train
> pred.total.test = pred.fourier + pred.hebdo.test
Erreur : objet 'pred.fourier' introuvable
> pred.fourier = (train, test, plt = TRUE)
Erreur : ',' inattendu(e) in "pred.fourier = (train,"
> pred.fourier = fourier(train, test, plt = TRUE)
> pred.total.train = reg$fitted + pred.hebdo.train
> pred.total.test = pred.fourier + pred.hebdo.test
> if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(train$time,pred.total.train, col='red', lwd=1)
+     lines(test$time,pred.total.test, col='green', lwd=1)
+ }
> Load = pred.total.test
> Id = train$Id
Warning message:
Unknown or uninitialised column: `Id`. 
> Load = pred.total.test
> Id = test$Id
> submission = data.frame(Load, Id)
> write.csv(submission, file ="submissions/submission.csv", row.names=F)
> MAw.train.total <- stats::filter(train$Load.1, filter = rep(1/52,52),
+                      method = c("convolution"), sides = 2, circular = T)
> MAw.test.total <- stats::filter(test$Load.1, filter = rep(1/52,52),
+                                  method = c("convolution"), sides = 2, circular = T)
> train$seasonal.tendancy = train$Load.1 - MAw.train.total
> test$seasonal.tendancy = test$Load.1 - MAw.test.total
> train$fourier.fitted = reg$fitted
> test$fourier.fitted = pred.fourier
> reg2 = lm(Load~Load.1+Load.7+Temp
+ +Temp_s95+WeekDays+GovernmentResponseIndex,data=train)
> summary(reg2)

Call:
lm(formula = Load ~ Load.1 + Load.7 + Temp + Temp_s95 + WeekDays + 
    GovernmentResponseIndex, data = train)

Residuals:
     Min       1Q   Median       3Q      Max 
-10526.2  -1304.4    147.1   1369.5   6430.4 

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
(Intercept)              1.957e+04  5.333e+02  36.687  < 2e-16 ***
Load.1                   7.261e-01  9.519e-03  76.273  < 2e-16 ***
Load.7                   8.780e-02  8.258e-03  10.633  < 2e-16 ***
Temp                    -6.524e+02  6.980e+01  -9.346  < 2e-16 ***
Temp_s95                 3.229e+02  7.229e+01   4.467 8.24e-06 ***
WeekDays                -1.309e+03  2.290e+01 -57.167  < 2e-16 ***
GovernmentResponseIndex -3.480e+01  5.039e+00  -6.906 6.05e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2134 on 3021 degrees of freedom
Multiple R-squared:  0.9626,	Adjusted R-squared:  0.9626 
F-statistic: 1.297e+04 on 6 and 3021 DF,  p-value: < 2.2e-16

> Gam <- gam(Load~s(Load.1)+s(Load.7)+s(Temp)
+            +s(Temp_s95)+s(WeekDays,k=7)
+            +s(GovernmentResponseIndex)
+            ,data=train)
> summary(Gam)

Family: gaussian 
Link function: identity 

Formula:
Load ~ s(Load.1) + s(Load.7) + s(Temp) + s(Temp_s95) + s(WeekDays, 
    k = 7) + s(GovernmentResponseIndex)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 54614.19      25.79    2118   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
                             edf Ref.df       F  p-value    
s(Load.1)                  4.853  6.077 646.680  < 2e-16 ***
s(Load.7)                  7.619  8.534  16.211  < 2e-16 ***
s(Temp)                    6.900  7.787  40.091  < 2e-16 ***
s(Temp_s95)                6.006  7.103   3.964 0.000244 ***
s(WeekDays)                5.984  6.000 937.534  < 2e-16 ***
s(GovernmentResponseIndex) 2.199  2.552  41.304  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.983   Deviance explained = 98.4%
GCV = 2.0365e+06  Scale est. = 2.0132e+06  n = 3028
> gam.train = predict(Gam, newdata=train)
> gam.test = predict(Gam, newdata=test)
> if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(train$time,Gam$fit, col='red', lwd=1)
+     lines(test$time,gam.test, col='green', lwd=1)
+ }
> tmp = format_data("./data/train_V2.csv", "./data/test_V2.csv")
[1] "Load and format  ./data/train_V2.csv"
[1] "Load and format  ./data/test_V2.csv"
> test_label = tmp$test_label
> Gam.rmse = evaluate(test_label, gam.test)
> cross.validation = function (train, test) {
+   set.seed(123)
+   to.extract = rbern(n=length(train$Load),p=0.8)==T
+   to.train = train[F,]
+   for (j in 1:length(to.extract)) {
+     if (to.extract[j]) {
+       to.train[nrow(to.train)+1,] = train[j,]
+     }
+   }
+   cross.train  <- train[to.train$time, ]
+   cross.test <- train[-to.train$time, ]
+   model = gam(Load~s(Load.1)+s(Load.7)+s(Temp)
+                +s(Temp_s95)+s(WeekDays,k=7)
+                ,data=cross.train)
+   pred.test = predict(model,test)
+   print(length(pred.test))
+   N = length(test$Load.1)
+   RMSE = rmse(pred.test[-N], test$Load.1[2:N]) #on connait pas le dernier
+   print(RMSE)
+   return(list("model"=model,"pred.test"=pred.test,"RMSE"=RMSE))
+ }
> cv = cross.validation(train, test)
[1] 275
[1] 1398.02
> model = cv$model; pred.test = cv$pred.test; RMSE = cv$RMSE
> if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(train$time,predict(model,train), col='red', lwd=1)
+     lines(test$time,pred.test, col='green', lwd=1)
+ }
> Load = pred.test
> Id = 1:length(Load)
> submission = data.frame(Load, Id)
> write.csv(submission, file ="submission.csv", row.names=F)
> labels = train$Load
> train.lstm = train[,-c(1,2,22,23)]
> test.lstm = test[,-c(1,20,21,23,24)]
> lstm = function(train_set, train_label, test_set) {
+   y = train_label
+   x = data.matrix(train_set)
+   x_test = data.matrix(test_set)
+   model = keras_model_sequential() %>%
+     layer_dense(units = 19, activation = 'relu', input_shape = c(19)) %>%
+     layer_dense(units = 12, activation = 'relu') %>%
+     layer_dense(units = 6, activation = 'relu') %>%
+     layer_dense(units=1, activation = "linear")
+   model %>% compile(loss = 'mse',
+                     optimizer = optimizer_adam(0.0005))
+   model %>% fit(x,y, epochs=15)
+   pred.train = model %>% predict(x)
+   pred.test = model %>% predict(x_test)
+   return(list("train"=pred.train,"test"=pred.test))
+ }
> pred.lstm = lstm(train.lstm, labels, test.lstm)
Epoch 1/15
95/95 [==============================] - 0s 766us/step - loss: 2795025664.0000
95/95 [==============================] - 0s 784us/step - loss: 2795025664.0000
Epoch 2/15
95/95 [==============================] - 0s 756us/step - loss: 639263104.0000
95/95 [==============================] - 0s 775us/step - loss: 639263104.0000
Epoch 3/15
95/95 [==============================] - 0s 733us/step - loss: 16259793.0000
95/95 [==============================] - 0s 751us/step - loss: 16259793.0000
Epoch 4/15
95/95 [==============================] - 0s 751us/step - loss: 15734385.0000
95/95 [==============================] - 0s 771us/step - loss: 15734385.0000
Epoch 5/15
95/95 [==============================] - 0s 739us/step - loss: 15052251.0000
95/95 [==============================] - 0s 757us/step - loss: 15052251.0000
Epoch 6/15
95/95 [==============================] - 0s 713us/step - loss: 14254108.0000
95/95 [==============================] - 0s 730us/step - loss: 14254108.0000
Epoch 7/15
95/95 [==============================] - 0s 708us/step - loss: 13586797.0000
95/95 [==============================] - 0s 725us/step - loss: 13586797.0000
Epoch 8/15
95/95 [==============================] - 0s 905us/step - loss: 13194220.0000
95/95 [==============================] - 0s 923us/step - loss: 13194220.0000
Epoch 9/15
95/95 [==============================] - 0s 740us/step - loss: 12877920.0000
95/95 [==============================] - 0s 757us/step - loss: 12877920.0000
Epoch 10/15
95/95 [==============================] - 0s 736us/step - loss: 12610613.0000
95/95 [==============================] - 0s 753us/step - loss: 12610613.0000
Epoch 11/15
95/95 [==============================] - 0s 723us/step - loss: 12426884.0000
95/95 [==============================] - 0s 740us/step - loss: 12426884.0000
Epoch 12/15
95/95 [==============================] - 0s 710us/step - loss: 12248947.0000
95/95 [==============================] - 0s 727us/step - loss: 12248947.0000
Epoch 13/15
95/95 [==============================] - 0s 706us/step - loss: 12046076.0000
95/95 [==============================] - 0s 723us/step - loss: 12046076.0000
Epoch 14/15
95/95 [==============================] - 0s 717us/step - loss: 11893991.0000
95/95 [==============================] - 0s 734us/step - loss: 11893991.0000
Epoch 15/15
95/95 [==============================] - 0s 737us/step - loss: 11753195.0000
95/95 [==============================] - 0s 754us/step - loss: 11753195.0000
> if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(test$time,pred.lstm$test, col='green', lwd=1)
+ }
> N = length(test$Load.1)
> RMSE = rmse(pred.lstm$test[-N], test$Load.1[2:N])
> RMSE
[1] 2740.86
> rf = randomForest(Load ~ ., data=train, mtry=3,
+                          importance=TRUE, na.action=na.omit)
pred.test.rf = predict(rf,test)
if (plt){
    par(mfrow=c(1,1))
    plot(train$Load,type='l', xlim=c(0,length(total.time)))
    lines(test$time,pred.test.rf, col='green', lwd=1)
}
> > + + + + > N = length(test$Load.1)
> RMSE = rmse(pred.test.rf[-N], test$Load.1[2:N])
> RMSE
[1] 1732.246
> add_expert = function(new_train, new_test, experts.train, experts.test){
+     experts.train = array_reshape(abind(experts.train, new_train, along=2), c(3028,1,length(experts.train)+1))
+     experts.test = cbind(experts.test, new_test)
+     return(experts.train, experts.test)
+ }
> expert1.train  = predict(model,train)
> expert2.train  = pred.lstm$train
> expert3.train  = rf$predicted
> expert1.test  = pred.test
> expert2.test = pred.lstm$test
> expert3.test  = pred.test.rf
> experts.train = array_reshape(abind(expert1.train,expert2.train,expert3.train,
+                               along=2), c(3028,1,3))
> experts.test = cbind(expert1.test,expert2.test,expert3.test)
> oracle1 = mixture(
+             Y = train$Load,
+             experts = experts.train,
+             model = "EWA",
+             loss.type = "square",
+             coefficients = "Uniform",
+ )
coeffs1 = oracle1$coefficients
> > print(oracle1)
Aggregation rule: EWA 
Loss function:  square loss 
Gradient trick:  TRUE 
Coefficients: 
  X1 X2  X3
 0.5  0 0.5
> oracle2 <- oracle(Y = train$Load,
+                         experts = experts.train,
+                         loss.type = "square", model = "convex"
+ )
Warning message:
In bestConvex(Y, experts, awake = awake, loss.type = loss.type,  :
  The best convex oracle is only approximated (using optim).
> plot(oracle2)
> coeffs2 = oracle2$coefficients
> coeffs2 = as.vector(coeffs2)
> print(coeffs2)
[1] 5.255350e-01 5.218119e-21 4.744650e-01
> pred.oracle = experts.test %*% coeffs1;
> par(mfrow=c(1,1))
> plot(train$Load,type='l', xlim=c(0,length(total.time)))
> lines(test$time,pred.oracle, col='green', lwd=1)
> N = length(test$Load.1)
> RMSE = rmse(pred.oracle[-N], test$Load.1[2:N])
> RMSE
[1] 1277.229
> Load = pred.oracle
> Id = 1:length(test$Load.1)
> submission = data.frame(Load, Id)
> write.csv(submission, file ="submission.csv", row.names=F)
> rm(list=objects())
> graphics.off()
> libs.to.load = c("tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
   tidyverse    lubridate       ranger       pracma      Metrics         mgcv 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       keras       visreg        caret         mc2d        opera        abind 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
randomForest   tensorflow 
        TRUE         TRUE 
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> sapply(files.sources, source)
Error in FUN(X[[i]], ...) : GAM.r:19:12: unexpected symbol
18: 
19:     return gam.test
               ^
> plt = FALSE #(si on veut plot, mettre à TRUE)
> path_submission = "./submissions/submission.csv"
> rm(list=objects())
> graphics.off()
> libs.to.load = c("tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
   tidyverse    lubridate       ranger       pracma      Metrics         mgcv 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       keras       visreg        caret         mc2d        opera        abind 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
randomForest   tensorflow 
        TRUE         TRUE 
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> sapply(files.sources, source)
Error in FUN(X[[i]], ...) : neural_network.r:39:12: unexpected symbol
38:     RMSE
39:     return pred.lstm
               ^
> rm(list=objects())
> graphics.off()
> libs.to.load = c("tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
> suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
   tidyverse    lubridate       ranger       pracma      Metrics         mgcv 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       keras       visreg        caret         mc2d        opera        abind 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
randomForest   tensorflow 
        TRUE         TRUE 
> files.sources = list.files(pattern = "*.r$")
> files.sources = files.sources[files.sources != "main_matthieu.r"]
> sapply(files.sources, source)
$cross_validation.r
$cross_validation.r$value
function (train, test) {
  set.seed(123)
  to.extract = rbern(n=length(train$Load),p=0.8)==T
  to.train = train[F,]
  for (j in 1:length(to.extract)) {
    if (to.extract[j]) {
      to.train[nrow(to.train)+1,] = train[j,]
    }
  }
  
  cross.train  <- train[to.train$time, ]
  cross.test <- train[-to.train$time, ]
  
  model = gam(Load~s(Load.1)+s(Load.7)+s(Temp)
               +s(Temp_s95)+s(WeekDays,k=7)
               ,data=cross.train)
  
  pred.test = predict(model,test)
  print(length(pred.test))
  N = length(test$Load.1)
  RMSE = rmse(pred.test[-N], test$Load.1[2:N]) #on connait pas le dernier
  
  print(RMSE)
  return(list("model"=model,"pred.test"=pred.test,"RMSE"=RMSE))
}

$cross_validation.r$visible
[1] FALSE


$days_to_numeric.r
$days_to_numeric.r$value
function (data) {
    data$WeekDays[data$WeekDays=='Monday']    = 1
    data$WeekDays[data$WeekDays=='Tuesday']   = 2
    data$WeekDays[data$WeekDays=='Wednesday'] = 3
    data$WeekDays[data$WeekDays=='Thursday']  = 4
    data$WeekDays[data$WeekDays=='Friday']    = 5
    data$WeekDays[data$WeekDays=='Saturday']  = 6
    data$WeekDays[data$WeekDays=='Sunday']    = 7
    data$WeekDays = as.numeric(data$WeekDays)
    return (data$WeekDays)
}

$days_to_numeric.r$visible
[1] FALSE


$evaluate.r
$evaluate.r$value
function(test_label, predicted_set){
    
    return(rmse(test_label, predicted_set))
}

$evaluate.r$visible
[1] FALSE


$format_data.r
$format_data.r$value
function(file_train, file_test) {

    print(paste("Load and format " , file_train, sep = " "))
    train_set = read_csv(file_train, col_types = cols())

    train_set$WeekDays[train_set$WeekDays == "Monday"] <- 1
    train_set$WeekDays[train_set$WeekDays == "Tuesday"] <- 2
    train_set$WeekDays[train_set$WeekDays == "Wednesday"] <- 3
    train_set$WeekDays[train_set$WeekDays == "Thursday"] <- 4
    train_set$WeekDays[train_set$WeekDays == "Friday"] <- 5
    train_set$WeekDays[train_set$WeekDays == "Saturday"] <- 6
    train_set$WeekDays[train_set$WeekDays == "Sunday"] <- 7
    train_set$WeekDays = as.integer(train_set$WeekDays)

    train_set$Year = NULL
    train_set$Date = NULL
    train_label = data.matrix(train_set$Load)


    train_set$Load = NULL
    
    train_set = data.matrix(train_set)


    print(paste("Load and format " , file_test, sep = " "))    
    test_set = read_csv(file_test, col_types = cols())

    test_set$WeekDays[test_set$WeekDays == "Monday"] <- 1
    test_set$WeekDays[test_set$WeekDays == "Tuesday"] <- 2
    test_set$WeekDays[test_set$WeekDays == "Wednesday"] <- 3
    test_set$WeekDays[test_set$WeekDays == "Thursday"] <- 4
    test_set$WeekDays[test_set$WeekDays == "Friday"] <- 5
    test_set$WeekDays[test_set$WeekDays == "Saturday"] <- 6
    test_set$WeekDays[test_set$WeekDays == "Sunday"] <- 7
    test_set$WeekDays = as.integer(test_set$WeekDays)

    test_label = test_set$Load.1
    tmp = test_set$Load.1
    for (i in c(1:(length(tmp)-1))){
        test_label[i] = tmp[i+1]
    }
    
    test_set$Year = NULL
    test_set$Date = NULL
    test_set$Id = NULL
    test_set$Usage = NULL
    test_set = data.matrix(test_set)
    

    test_set = data.matrix(test_set)
    return(list("train_set" = train_set, "train_label" = train_label, "test_set" = test_set, "test_label" = test_label))
}

$format_data.r$visible
[1] FALSE


$fourier.r
$fourier.r$value
function(train, test, plt = FALSE){
    total.time = c(1:(nrow(train)+nrow(test)))
    length(total.time)
    train$time = total.time[1:nrow(train)]
    test$time = tail(total.time,nrow(test))

    fourier.make.matrix = function (t, k, period) {
        w = 2*pi/period
        ret = cbind(cos(w*t), sin(w*t))
        
        for(i in c(2:K))
        {
            ret = cbind(ret, cos(i*w*t), sin(i*w*t))
        }
        return (ret)
    }

    K = 5; period = 365
    fourier.train = fourier.make.matrix(train$time, K, period)
    fourier.test = fourier.make.matrix(test$time, K, period)

    fourier.train.df = data.frame(train$Load,fourier.train)
    fourier.test.df = data.frame(fourier.test)

    reg = lm(train.Load ~., data=fourier.train.df)

    pred.fourier = predict(reg, newdata=fourier.test.df)
    total.fourier = c(reg$fitted,pred.fourier)
    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(reg$fitted,col='red', lwd=2)
        lines(test$time,pred.fourier,col='green', lwd=2)
    }
    return(pred.fourier)
}

$fourier.r$visible
[1] FALSE


$GAM.r
$GAM.r$value
function(train, test, plt = FALSE){
    Gam <- gam(Load~s(Load.1)+s(Load.7)+s(Temp)
               +s(Temp_s95)+s(WeekDays,k=7)
               +s(GovernmentResponseIndex)
              ,data=train)
    summary(Gam)


    gam.train = predict(Gam, newdata=train)
    gam.test = predict(Gam, newdata=test)

    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(train$time,Gam$fit, col='red', lwd=1)
        lines(test$time,gam.test, col='green', lwd=1)
    }

    return(gam.test)
}

$GAM.r$visible
[1] FALSE


$lstm.r
$lstm.r$value
function(train_set, train_label, test_set){
  N = length(train_set[,'Load.1'])
  nb_var = 17
  y = train_label
  x = array(train_set, dim = c(N, nb_var, 1))
  x_test = array(test_set,dim = c(length(test_set[,'Load.1']), nb_var, 1))
  print("step1")
  model = keras_model_sequential() %>%   
    #layer_lstm(units=128, input_shape=c(nb_var, 1), activation="relu", return_sequences = TRUE) %>% 
    layer_dense(units=64, activation = "relu") %>%  
    layer_dense(units=32) %>%  
    layer_dense(units=1, activation = "linear")
  print("step2")
  model %>% compile(loss = 'mse',
                    optimizer = 'adam',
                    metrics = list("mean_absolute_error")
  )
  print("step3")
  model %>% fit(x,y, epochs=100, batch_size=32, shuffle = FALSE)
  print("step4")
  print(length(x_test))
  pred = model %>% predict(x_test)
  print("step5")
  return(pred)
    
}

$lstm.r$visible
[1] FALSE


$main.r
NULL

$neural_network.r
$neural_network.r$value
function(train, test, plt = FALSE){
    labels = train$Load
    train.lstm = train[,-c(1,2,22,23)]
    test.lstm = test[,-c(1,20,21,23,24)]

    lstm = function(train_set, train_label, test_set) {
        y = train_label
        x = data.matrix(train_set)
        x_test = data.matrix(test_set)
        model = keras_model_sequential() %>%
            layer_dense(units = 19, activation = 'relu', input_shape = c(19)) %>%
            layer_dense(units = 12, activation = 'relu') %>%
            layer_dense(units = 6, activation = 'relu') %>%
            layer_dense(units=1, activation = "linear")
        
        model %>% compile(loss = 'mse',
                          optimizer = optimizer_adam(0.0005))
        
        model %>% fit(x,y, epochs=15)

        pred.train = model %>% predict(x)
        pred.test = model %>% predict(x_test)

        return(list("train"=pred.train,"test"=pred.test))
    }


    pred.lstm = lstm(train.lstm, labels, test.lstm)

    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(test$time,pred.lstm$test, col='green', lwd=1)
    }

    N = length(test$Load.1)
    RMSE = rmse(pred.lstm$test[-N], test$Load.1[2:N])
    RMSE
    return(pred.lstm)
}

$neural_network.r$visible
[1] FALSE


$random_forest.r
$random_forest.r$value
function(train, test, plt = FALSE){

    rf = randomForest(Load ~ ., data=train, mtry=3,
                      importance=TRUE, na.action=na.omit)
    pred.test.rf = predict(rf,test)

    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(test$time,pred.test.rf, col='green', lwd=1)
    }

    N = length(test$Load.1)
    RMSE = rmse(pred.test.rf[-N], test$Load.1[2:N])
    RMSE
    return(pred.test.rf)
 }

$random_forest.r$visible
[1] FALSE


$scale.r
$scale.r$value
function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

$scale.r$visible
[1] FALSE


$xgboost.r
$xgboost.r$value
function(train_set, train_label, test_set){
  param = list(booster = "gblinear", objective = "reg:squarederror", eval_metric = "rmse", lambda = 0.0003, alpha = 0.0003, nthread = 2, eta = 0.1)
  
  print("Model : XGBOOST")
  
  xgbmodel = xgboost(data = train_set, label = train_label, nrounds = 200, params = param, verbose = 0)
  
  pred = predict(xgbmodel, test_set)
  
  return(pred)
}

$xgboost.r$visible
[1] FALSE


> plt = FALSE #(si on veut plot, mettre à TRUE)
> path_submission = "./submissions/submission.csv"
> model = "lstm"
> data = format_data("./data/train_V2.csv", "./data/test_V2.csv")
[1] "Load and format  ./data/train_V2.csv"
[1] "Load and format  ./data/test_V2.csv"
> train_set = data$train_set
> test_set = data$test_set
> train_label = data$train_label
> test_label = data$test_label
> train <- read_delim(file="data/train_V2.csv",delim=',')

── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character()
)
ℹ Use `spec()` for the full column specifications.

> test <- read_delim(file="data/test_V2.csv",delim=',')

── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character(),
  Usage = col_character()
)
ℹ Use `spec()` for the full column specifications.

> reg = lm(Load~Temp, data=train)
> if (plt){
+     plot(train$Date, train$Load, type='l')
+     par(new=T)
+     plot(train$Date, train$Temp, type='l', col='red')
+     plot(train$Temp,train$Load, col='red')
+     points(train$Temp ,reg$fitted.values)
+ }
> train$WeekDays = days_to_numeric(train)
> test$WeekDays = days_to_numeric(test)
> train$Year = train$Year - 2012
> test$Year = test$Year - 2012
> total.time = c(1:(nrow(train)+nrow(test)))
> length(total.time)
[1] 3303
> train$time = total.time[1:nrow(train)]
> test$time = tail(total.time,nrow(test))
> if (plt){
+     plot(train$Date, train$Load, type='l')
+ }
> MA <- stats::filter(train$Load, filter = rep(1/365,365),
+              method = c("convolution"), sides = 2, circular = FALSE)
> if (plt){
+     plot(train$Date, train$Load, type = "l", xlab = "",
+          ylab = "consumption (kw)", col = "seagreen4", lwd = 1)
+     lines(train$Date, MA, col = "red", lwd = 2)
+ }
> pred.fourier = fourier(train, test, plt = TRUE)
> num.years = 0
> average = 0
> N = 8; a = 0.7 #exponential weight
> while (365*(num.years+1) <= length(train$Date)) {
+     par(mfrow = c(1, 1))
+     dateyear = train$Date[(365*num.years+1):(365*(num.years+1))]
+     loadyear = train$Load[(365*num.years+1):(365*(num.years+1))]
+     MAw <- stats::filter(loadyear, filter = rep(1/52,52),
+                       method = c("convolution"), sides = 2, circular = T)
+     expweight = (((1-a)/(1-a^8))*a^(N-(num.years+1)))
+     average = expweight*(average + (loadyear - MAw))
+     num.years = num.years + 1
+ }
> if (plt){
+     plot(average)
+ }
> train.to.day = train$time %% 365 + 1
> test.to.day = test$time %% 365 + 1
> pred.hebdo.train = average[train.to.day]
> pred.hebdo.test = average[test.to.day]
> pred.total.train = reg$fitted + pred.hebdo.train
> pred.total.test = pred.fourier + pred.hebdo.test
> if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(train$time,pred.total.train, col='red', lwd=1)
+     lines(test$time,pred.total.test, col='green', lwd=1)
+ }
> Load = pred.total.test
> Id = test$Id
> submission = data.frame(Load, Id)
> write.csv(submission, file =path_submission, row.names=F)
> MAw.train.total <- stats::filter(train$Load.1, filter = rep(1/52,52),
+                      method = c("convolution"), sides = 2, circular = T)
> MAw.test.total <- stats::filter(test$Load.1, filter = rep(1/52,52),
+                                  method = c("convolution"), sides = 2, circular = T)
> train$seasonal.tendancy = train$Load.1 - MAw.train.total
> test$seasonal.tendancy = test$Load.1 - MAw.test.total
> train$fourier.fitted = reg$fitted
> test$fourier.fitted = pred.fourier
> reg2 = lm(Load~Load.1+Load.7+Temp
+ +Temp_s95+WeekDays+GovernmentResponseIndex,data=train)
> summary(reg2)

Call:
lm(formula = Load ~ Load.1 + Load.7 + Temp + Temp_s95 + WeekDays + 
    GovernmentResponseIndex, data = train)

Residuals:
     Min       1Q   Median       3Q      Max 
-10526.2  -1304.4    147.1   1369.5   6430.4 

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
(Intercept)              1.957e+04  5.333e+02  36.687  < 2e-16 ***
Load.1                   7.261e-01  9.519e-03  76.273  < 2e-16 ***
Load.7                   8.780e-02  8.258e-03  10.633  < 2e-16 ***
Temp                    -6.524e+02  6.980e+01  -9.346  < 2e-16 ***
Temp_s95                 3.229e+02  7.229e+01   4.467 8.24e-06 ***
WeekDays                -1.309e+03  2.290e+01 -57.167  < 2e-16 ***
GovernmentResponseIndex -3.480e+01  5.039e+00  -6.906 6.05e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2134 on 3021 degrees of freedom
Multiple R-squared:  0.9626,	Adjusted R-squared:  0.9626 
F-statistic: 1.297e+04 on 6 and 3021 DF,  p-value: < 2.2e-16

> gam.test = gam(train, test)
Error in gam(Load ~ s(Load.1) + s(Load.7) + s(Temp) + s(Temp_s95) + s(WeekDays,  : 
  argument inutilisé (data = train)
> tmp = format_data("./data/train_V2.csv", "./data/test_V2.csv")
[1] "Load and format  ./data/train_V2.csv"
[1] "Load and format  ./data/test_V2.csv"
> gam.test = gam(train, test)
Error in gam(Load ~ s(Load.1) + s(Load.7) + s(Temp) + s(Temp_s95) + s(WeekDays,  : 
  argument inutilisé (data = train)
>  Gam <- gam(Load~s(Load.1)+s(Load.7)+s(Temp)
+                +s(Temp_s95)+s(WeekDays,k=7)
+                +s(GovernmentResponseIndex)
+               ,data=train)
Error in gam(Load ~ s(Load.1) + s(Load.7) + s(Temp) + s(Temp_s95) + s(WeekDays,  : 
  argument inutilisé (data = train)
> MAw.train.total <- stats::filter(train$Load.1, filter = rep(1/52,52),
+                      method = c("convolution"), sides = 2, circular = T)
> MAw.test.total <- stats::filter(test$Load.1, filter = rep(1/52,52),
+                                  method = c("convolution"), sides = 2, circular = T)
> train$seasonal.tendancy = train$Load.1 - MAw.train.total
> test$seasonal.tendancy = test$Load.1 - MAw.test.total
> train$fourier.fitted = reg$fitted
> test$fourier.fitted = pred.fourier
> reg2 = lm(Load~Load.1+Load.7+Temp
+ +Temp_s95+WeekDays+GovernmentResponseIndex,data=train)
> summary(reg2)

Call:
lm(formula = Load ~ Load.1 + Load.7 + Temp + Temp_s95 + WeekDays + 
    GovernmentResponseIndex, data = train)

Residuals:
     Min       1Q   Median       3Q      Max 
-10526.2  -1304.4    147.1   1369.5   6430.4 

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
(Intercept)              1.957e+04  5.333e+02  36.687  < 2e-16 ***
Load.1                   7.261e-01  9.519e-03  76.273  < 2e-16 ***
Load.7                   8.780e-02  8.258e-03  10.633  < 2e-16 ***
Temp                    -6.524e+02  6.980e+01  -9.346  < 2e-16 ***
Temp_s95                 3.229e+02  7.229e+01   4.467 8.24e-06 ***
WeekDays                -1.309e+03  2.290e+01 -57.167  < 2e-16 ***
GovernmentResponseIndex -3.480e+01  5.039e+00  -6.906 6.05e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2134 on 3021 degrees of freedom
Multiple R-squared:  0.9626,	Adjusted R-squared:  0.9626 
F-statistic: 1.297e+04 on 6 and 3021 DF,  p-value: < 2.2e-16

>  Gam <- gam(Load~s(Load.1)+s(Load.7)+s(Temp)
+                +s(Temp_s95)+s(WeekDays,k=7)
+                +s(GovernmentResponseIndex)
+               ,data=train)
Error in gam(Load ~ s(Load.1) + s(Load.7) + s(Temp) + s(Temp_s95) + s(WeekDays,  : 
  argument inutilisé (data = train)
> rm(list=objects())
+ graphics.off()
+ 
+ ##pour load tout en 2 lignes, il faut juste rajouter les librairies dans libs.to.load
+ 
+ libs.to.load = c("tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
+ suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
+ 
+ ##setwd("C:/Users/CM/code/M1/R")
+ 
+ ##load tous les fichiers en sources
+ files.sources = list.files(pattern = "*.r$")
+ files.sources = files.sources[files.sources != "main_matthieu.r"]
+ sapply(files.sources, source)
+ 
+ ##
+ plt = FALSE #(si on veut plot, mettre à TRUE)
+ path_submission = "./submissions/submission.csv"
+ 
+ 
+ ##MAIN NICOLAS
+ model = "lstm"
+ 
+ data = format_data("./data/train_V2.csv", "./data/test_V2.csv")
+ train_set = data$train_set
+ test_set = data$test_set
+ train_label = data$train_label
+ test_label = data$test_label
+ 
+ if (model == "xgboost"){
+   pred = xgboost_rte(train_set, train_label, test_set)
+ } else if(model == "lstm"){
+   pred = lstm(train_set, train_label, test_set)
+ }
+ 
+ 
+ print(paste("Score final : ", evaluate(test_label, pred), sep=""))
+ 
+ if (plt) {
+     plot(c(train_label, pred))
+ }
+ 
+ ##FIN MAIN NICOLAS 
+ 
+ 
+ train <- read_delim(file="data/train_V2.csv",delim=',')
+ test <- read_delim(file="data/test_V2.csv",delim=',')
+ 
+ reg = lm(Load~Temp, data=train)
+ 
+ if (plt){
+     plot(train$Date, train$Load, type='l')
+     par(new=T)
+     plot(train$Date, train$Temp, type='l', col='red')
+ 
+     plot(train$Temp,train$Load, col='red')
+     points(train$Temp ,reg$fitted.values)
+ }
+ 
+ train$WeekDays = days_to_numeric(train)
+ test$WeekDays = days_to_numeric(test)
+ 
+ train$Year = train$Year - 2012
+ test$Year = test$Year - 2012
+ 
+ total.time = c(1:(nrow(train)+nrow(test)))
+ length(total.time)
+ train$time = total.time[1:nrow(train)]
+ test$time = tail(total.time,nrow(test))
+ 
+ 
+ #saisonalite : annuelle
+ 
+ if (plt){
+     plot(train$Date, train$Load, type='l')
+ }
+ 
+ MA <- stats::filter(train$Load, filter = rep(1/365,365),
+              method = c("convolution"), sides = 2, circular = FALSE)
+ if (plt){
+     plot(train$Date, train$Load, type = "l", xlab = "",
+          ylab = "consumption (kw)", col = "seagreen4", lwd = 1)
+     lines(train$Date, MA, col = "red", lwd = 2)
+ }
+ 
+ ##estimation avec fourier
+ pred.fourier = fourier(train, test, plt = TRUE)
+ 
+ ##saisonalite : hebdomadaire
+ 
+ num.years = 0
+ average = 0
+ N = 8; a = 0.7 #exponential weight
+ while (365*(num.years+1) <= length(train$Date)) {
+     par(mfrow = c(1, 1))
+     dateyear = train$Date[(365*num.years+1):(365*(num.years+1))]
+     loadyear = train$Load[(365*num.years+1):(365*(num.years+1))]
+ 
+     ##plot(dateyear,loadyear,type='l')
+   
+     MAw <- stats::filter(loadyear, filter = rep(1/52,52),
+                       method = c("convolution"), sides = 2, circular = T)
+     ##plot(dateyear,loadyear, type = "l", xlab = "",
+     ##    ylab = "consumption (kw)", col = "seagreen4", lwd = 1)
+     ##lines(dateyear, MAw, col = "red", lwd = 2)
+   
+     ##plot(dateyear, loadyear - MAw, type="l")
+     expweight = (((1-a)/(1-a^8))*a^(N-(num.years+1)))
+     average = expweight*(average + (loadyear - MAw))
+     num.years = num.years + 1
+ }
+ if (plt){
+     plot(average)
+ }
+ 
+ train.to.day = train$time %% 365 + 1
+ test.to.day = test$time %% 365 + 1
+ 
+ pred.hebdo.train = average[train.to.day]
+ pred.hebdo.test = average[test.to.day]
+ 
+ pred.total.train = reg$fitted + pred.hebdo.train
+ pred.total.test = pred.fourier + pred.hebdo.test
+ 
+ 
+ if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(train$time,pred.total.train, col='red', lwd=1)
+     lines(test$time,pred.total.test, col='green', lwd=1)
+ }
+ 
+ Load = pred.total.test
+ Id = test$Id
+ submission = data.frame(Load, Id)
+ 
+ write.csv(submission, file =path_submission, row.names=F)
+ 
+ 
+ MAw.train.total <- stats::filter(train$Load.1, filter = rep(1/52,52),
+                      method = c("convolution"), sides = 2, circular = T)
+ MAw.test.total <- stats::filter(test$Load.1, filter = rep(1/52,52),
+                                  method = c("convolution"), sides = 2, circular = T)
+ 
+ train$seasonal.tendancy = train$Load.1 - MAw.train.total
+ test$seasonal.tendancy = test$Load.1 - MAw.test.total
+ 
+ train$fourier.fitted = reg$fitted
+ test$fourier.fitted = pred.fourier
+ 
+ reg2 = lm(Load~Load.1+Load.7+Temp
+ +Temp_s95+WeekDays+GovernmentResponseIndex,data=train)
+ summary(reg2)
+ 
   tidyverse    lubridate       ranger       pracma      Metrics         mgcv 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       keras       visreg        caret         mc2d        opera        abind 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
randomForest   tensorflow 
        TRUE         TRUE 
> > > $cross_validation.r
$cross_validation.r$value
function (train, test) {
  set.seed(123)
  to.extract = rbern(n=length(train$Load),p=0.8)==T
  to.train = train[F,]
  for (j in 1:length(to.extract)) {
    if (to.extract[j]) {
      to.train[nrow(to.train)+1,] = train[j,]
    }
  }
  
  cross.train  <- train[to.train$time, ]
  cross.test <- train[-to.train$time, ]
  
  model = gam(Load~s(Load.1)+s(Load.7)+s(Temp)
               +s(Temp_s95)+s(WeekDays,k=7)
               ,data=cross.train)
  
  pred.test = predict(model,test)
  print(length(pred.test))
  N = length(test$Load.1)
  RMSE = rmse(pred.test[-N], test$Load.1[2:N]) #on connait pas le dernier
  
  print(RMSE)
  return(list("model"=model,"pred.test"=pred.test,"RMSE"=RMSE))
}

$cross_validation.r$visible
[1] FALSE


$days_to_numeric.r
$days_to_numeric.r$value
function (data) {
    data$WeekDays[data$WeekDays=='Monday']    = 1
    data$WeekDays[data$WeekDays=='Tuesday']   = 2
    data$WeekDays[data$WeekDays=='Wednesday'] = 3
    data$WeekDays[data$WeekDays=='Thursday']  = 4
    data$WeekDays[data$WeekDays=='Friday']    = 5
    data$WeekDays[data$WeekDays=='Saturday']  = 6
    data$WeekDays[data$WeekDays=='Sunday']    = 7
    data$WeekDays = as.numeric(data$WeekDays)
    return (data$WeekDays)
}

$days_to_numeric.r$visible
[1] FALSE


$evaluate.r
$evaluate.r$value
function(test_label, predicted_set){
    
    return(rmse(test_label, predicted_set))
}

$evaluate.r$visible
[1] FALSE


$format_data.r
$format_data.r$value
function(file_train, file_test) {

    print(paste("Load and format " , file_train, sep = " "))
    train_set = read_csv(file_train, col_types = cols())

    train_set$WeekDays[train_set$WeekDays == "Monday"] <- 1
    train_set$WeekDays[train_set$WeekDays == "Tuesday"] <- 2
    train_set$WeekDays[train_set$WeekDays == "Wednesday"] <- 3
    train_set$WeekDays[train_set$WeekDays == "Thursday"] <- 4
    train_set$WeekDays[train_set$WeekDays == "Friday"] <- 5
    train_set$WeekDays[train_set$WeekDays == "Saturday"] <- 6
    train_set$WeekDays[train_set$WeekDays == "Sunday"] <- 7
    train_set$WeekDays = as.integer(train_set$WeekDays)

    train_set$Year = NULL
    train_set$Date = NULL
    train_label = data.matrix(train_set$Load)


    train_set$Load = NULL
    
    train_set = data.matrix(train_set)


    print(paste("Load and format " , file_test, sep = " "))    
    test_set = read_csv(file_test, col_types = cols())

    test_set$WeekDays[test_set$WeekDays == "Monday"] <- 1
    test_set$WeekDays[test_set$WeekDays == "Tuesday"] <- 2
    test_set$WeekDays[test_set$WeekDays == "Wednesday"] <- 3
    test_set$WeekDays[test_set$WeekDays == "Thursday"] <- 4
    test_set$WeekDays[test_set$WeekDays == "Friday"] <- 5
    test_set$WeekDays[test_set$WeekDays == "Saturday"] <- 6
    test_set$WeekDays[test_set$WeekDays == "Sunday"] <- 7
    test_set$WeekDays = as.integer(test_set$WeekDays)

    test_label = test_set$Load.1
    tmp = test_set$Load.1
    for (i in c(1:(length(tmp)-1))){
        test_label[i] = tmp[i+1]
    }
    
    test_set$Year = NULL
    test_set$Date = NULL
    test_set$Id = NULL
    test_set$Usage = NULL
    test_set = data.matrix(test_set)
    

    test_set = data.matrix(test_set)
    return(list("train_set" = train_set, "train_label" = train_label, "test_set" = test_set, "test_label" = test_label))
}

$format_data.r$visible
[1] FALSE


$fourier.r
$fourier.r$value
function(train, test, plt = FALSE){
    total.time = c(1:(nrow(train)+nrow(test)))
    length(total.time)
    train$time = total.time[1:nrow(train)]
    test$time = tail(total.time,nrow(test))

    fourier.make.matrix = function (t, k, period) {
        w = 2*pi/period
        ret = cbind(cos(w*t), sin(w*t))
        
        for(i in c(2:K))
        {
            ret = cbind(ret, cos(i*w*t), sin(i*w*t))
        }
        return (ret)
    }

    K = 5; period = 365
    fourier.train = fourier.make.matrix(train$time, K, period)
    fourier.test = fourier.make.matrix(test$time, K, period)

    fourier.train.df = data.frame(train$Load,fourier.train)
    fourier.test.df = data.frame(fourier.test)

    reg = lm(train.Load ~., data=fourier.train.df)

    pred.fourier = predict(reg, newdata=fourier.test.df)
    total.fourier = c(reg$fitted,pred.fourier)
    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(reg$fitted,col='red', lwd=2)
        lines(test$time,pred.fourier,col='green', lwd=2)
    }
    return(pred.fourier)
}

$fourier.r$visible
[1] FALSE


$GAM.r
$GAM.r$value
function(train, test, plt = FALSE){
    Gam <- gam(Load~s(Load.1)+s(Load.7)+s(Temp)
               +s(Temp_s95)+s(WeekDays,k=7)
               +s(GovernmentResponseIndex)
              ,data=train)
    summary(Gam)


    gam.train = predict(Gam, newdata=train)
    gam.test = predict(Gam, newdata=test)

    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(train$time,Gam$fit, col='red', lwd=1)
        lines(test$time,gam.test, col='green', lwd=1)
    }

    return(gam.test)
}

$GAM.r$visible
[1] FALSE


$lstm.r
$lstm.r$value
function(train_set, train_label, test_set){
  N = length(train_set[,'Load.1'])
  nb_var = 17
  y = train_label
  x = array(train_set, dim = c(N, nb_var, 1))
  x_test = array(test_set,dim = c(length(test_set[,'Load.1']), nb_var, 1))
  print("step1")
  model = keras_model_sequential() %>%   
    #layer_lstm(units=128, input_shape=c(nb_var, 1), activation="relu", return_sequences = TRUE) %>% 
    layer_dense(units=64, activation = "relu") %>%  
    layer_dense(units=32) %>%  
    layer_dense(units=1, activation = "linear")
  print("step2")
  model %>% compile(loss = 'mse',
                    optimizer = 'adam',
                    metrics = list("mean_absolute_error")
  )
  print("step3")
  model %>% fit(x,y, epochs=100, batch_size=32, shuffle = FALSE)
  print("step4")
  print(length(x_test))
  pred = model %>% predict(x_test)
  print("step5")
  return(pred)
    
}

$lstm.r$visible
[1] FALSE


$main.r
NULL

$neural_network.r
$neural_network.r$value
function(train, test, plt = FALSE){
    labels = train$Load
    train.lstm = train[,-c(1,2,22,23)]
    test.lstm = test[,-c(1,20,21,23,24)]

    lstm = function(train_set, train_label, test_set) {
        y = train_label
        x = data.matrix(train_set)
        x_test = data.matrix(test_set)
        model = keras_model_sequential() %>%
            layer_dense(units = 19, activation = 'relu', input_shape = c(19)) %>%
            layer_dense(units = 12, activation = 'relu') %>%
            layer_dense(units = 6, activation = 'relu') %>%
            layer_dense(units=1, activation = "linear")
        
        model %>% compile(loss = 'mse',
                          optimizer = optimizer_adam(0.0005))
        
        model %>% fit(x,y, epochs=15)

        pred.train = model %>% predict(x)
        pred.test = model %>% predict(x_test)

        return(list("train"=pred.train,"test"=pred.test))
    }


    pred.lstm = lstm(train.lstm, labels, test.lstm)

    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(test$time,pred.lstm$test, col='green', lwd=1)
    }

    N = length(test$Load.1)
    RMSE = rmse(pred.lstm$test[-N], test$Load.1[2:N])
    RMSE
    return(pred.lstm)
}

$neural_network.r$visible
[1] FALSE


$random_forest.r
$random_forest.r$value
function(train, test, plt = FALSE){

    rf = randomForest(Load ~ ., data=train, mtry=3,
                      importance=TRUE, na.action=na.omit)
    pred.test.rf = predict(rf,test)

    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(test$time,pred.test.rf, col='green', lwd=1)
    }

    N = length(test$Load.1)
    RMSE = rmse(pred.test.rf[-N], test$Load.1[2:N])
    RMSE
    return(pred.test.rf)
 }

$random_forest.r$visible
[1] FALSE


$scale.r
$scale.r$value
function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

$scale.r$visible
[1] FALSE


$xgboost.r
$xgboost.r$value
function(train_set, train_label, test_set){
  param = list(booster = "gblinear", objective = "reg:squarederror", eval_metric = "rmse", lambda = 0.0003, alpha = 0.0003, nthread = 2, eta = 0.1)
  
  print("Model : XGBOOST")
  
  xgbmodel = xgboost(data = train_set, label = train_label, nrounds = 200, params = param, verbose = 0)
  
  pred = predict(xgbmodel, test_set)
  
  return(pred)
}

$xgboost.r$visible
[1] FALSE


> > > > > > > > > > [1] "Load and format  ./data/train_V2.csv"
[1] "Load and format  ./data/test_V2.csv"
> > > > > > . + [1] "step1"
[1] "step2"
[1] "step3"
Epoch 1/100
95/95 [==============================] - 0s 1ms/step - loss: 2806989056.0000 - mean_absolute_error: 49930.5742
95/95 [==============================] - 0s 1ms/step - loss: 2806989056.0000 - mean_absolute_error: 49930.5742
Epoch 2/100
95/95 [==============================] - 0s 1ms/step - loss: 2740458496.0000 - mean_absolute_error: 48555.3906
95/95 [==============================] - 0s 1ms/step - loss: 2740458496.0000 - mean_absolute_error: 48555.3906
Epoch 3/100
95/95 [==============================] - 0s 1ms/step - loss: 2739865344.0000 - mean_absolute_error: 48560.5820
95/95 [==============================] - 0s 1ms/step - loss: 2739865344.0000 - mean_absolute_error: 48560.5820
Epoch 4/100
95/95 [==============================] - 0s 991us/step - loss: 2738543104.0000 - mean_absolute_error: 48554.7617
95/95 [==============================] - 0s 1ms/step - loss: 2738543104.0000 - mean_absolute_error: 48554.7617  
Epoch 5/100
95/95 [==============================] - 0s 999us/step - loss: 2736619520.0000 - mean_absolute_error: 48566.5742
95/95 [==============================] - 0s 1ms/step - loss: 2736619520.0000 - mean_absolute_error: 48566.5742  
Epoch 6/100
95/95 [==============================] - 0s 1ms/step - loss: 2733921024.0000 - mean_absolute_error: 48536.4609
95/95 [==============================] - 0s 1ms/step - loss: 2733921024.0000 - mean_absolute_error: 48536.4609
Epoch 7/100
95/95 [==============================] - 0s 968us/step - loss: 2730451968.0000 - mean_absolute_error: 48518.6406
95/95 [==============================] - 0s 986us/step - loss: 2730451968.0000 - mean_absolute_error: 48518.6406
Epoch 8/100
95/95 [==============================] - 0s 999us/step - loss: 2725147136.0000 - mean_absolute_error: 48458.9336
95/95 [==============================] - 0s 1ms/step - loss: 2725147136.0000 - mean_absolute_error: 48458.9336  
Epoch 9/100
95/95 [==============================] - 0s 1ms/step - loss: 2716818432.0000 - mean_absolute_error: 48361.4648
95/95 [==============================] - 0s 1ms/step - loss: 2716818432.0000 - mean_absolute_error: 48361.4648
Epoch 10/100
95/95 [==============================] - 0s 967us/step - loss: 2703452928.0000 - mean_absolute_error: 48234.8047
95/95 [==============================] - 0s 985us/step - loss: 2703452928.0000 - mean_absolute_error: 48234.8047
Epoch 11/100
95/95 [==============================] - 0s 1ms/step - loss: 2682861568.0000 - mean_absolute_error: 48048.4492
95/95 [==============================] - 0s 1ms/step - loss: 2682861568.0000 - mean_absolute_error: 48048.4492
Epoch 12/100
95/95 [==============================] - 0s 1ms/step - loss: 2650150400.0000 - mean_absolute_error: 47777.0312
95/95 [==============================] - 0s 1ms/step - loss: 2650150400.0000 - mean_absolute_error: 47777.0312
Epoch 13/100
95/95 [==============================] - 0s 953us/step - loss: 2601790208.0000 - mean_absolute_error: 47379.7031
95/95 [==============================] - 0s 972us/step - loss: 2601790208.0000 - mean_absolute_error: 47379.7031
Epoch 14/100
95/95 [==============================] - 0s 935us/step - loss: 2533966080.0000 - mean_absolute_error: 46785.0078
95/95 [==============================] - 0s 955us/step - loss: 2533966080.0000 - mean_absolute_error: 46785.0078
Epoch 15/100
95/95 [==============================] - 0s 995us/step - loss: 2441653248.0000 - mean_absolute_error: 45924.7969
95/95 [==============================] - 0s 1ms/step - loss: 2441653248.0000 - mean_absolute_error: 45924.7969  
Epoch 16/100
95/95 [==============================] - 0s 961us/step - loss: 2328386304.0000 - mean_absolute_error: 44870.2734
95/95 [==============================] - 0s 979us/step - loss: 2328386304.0000 - mean_absolute_error: 44870.2734
Epoch 17/100
95/95 [==============================] - 0s 951us/step - loss: 2198151424.0000 - mean_absolute_error: 43708.6914
95/95 [==============================] - 0s 971us/step - loss: 2198151424.0000 - mean_absolute_error: 43708.6914
Epoch 18/100
95/95 [==============================] - 0s 990us/step - loss: 2078262400.0000 - mean_absolute_error: 42596.9570
95/95 [==============================] - 0s 1ms/step - loss: 2078262400.0000 - mean_absolute_error: 42596.9570  
Epoch 19/100
95/95 [==============================] - 0s 949us/step - loss: 1901117824.0000 - mean_absolute_error: 40424.4219
95/95 [==============================] - 0s 967us/step - loss: 1901117824.0000 - mean_absolute_error: 40424.4219
Epoch 20/100
95/95 [==============================] - 0s 937us/step - loss: 1727842304.0000 - mean_absolute_error: 38407.9375
95/95 [==============================] - 0s 955us/step - loss: 1727842304.0000 - mean_absolute_error: 38407.9375
Epoch 21/100
95/95 [==============================] - 0s 977us/step - loss: 1554305408.0000 - mean_absolute_error: 36441.9453
95/95 [==============================] - 0s 999us/step - loss: 1554305408.0000 - mean_absolute_error: 36441.9453
Epoch 22/100
95/95 [==============================] - 0s 1ms/step - loss: 1381245824.0000 - mean_absolute_error: 34361.3359
95/95 [==============================] - 0s 1ms/step - loss: 1381245824.0000 - mean_absolute_error: 34361.3359
Epoch 23/100
95/95 [==============================] - 0s 1ms/step - loss: 1200930816.0000 - mean_absolute_error: 31741.4102
95/95 [==============================] - 0s 1ms/step - loss: 1200930816.0000 - mean_absolute_error: 31741.4102
Epoch 24/100
95/95 [==============================] - 0s 977us/step - loss: 1060201152.0000 - mean_absolute_error: 30020.0586
95/95 [==============================] - 0s 996us/step - loss: 1060201152.0000 - mean_absolute_error: 30020.0586
Epoch 25/100
95/95 [==============================] - 0s 942us/step - loss: 905778944.0000 - mean_absolute_error: 27499.2207
95/95 [==============================] - 0s 960us/step - loss: 905778944.0000 - mean_absolute_error: 27499.2207
Epoch 26/100
95/95 [==============================] - 0s 939us/step - loss: 785026560.0000 - mean_absolute_error: 25225.2871
95/95 [==============================] - 0s 957us/step - loss: 785026560.0000 - mean_absolute_error: 25225.2871
Epoch 27/100
95/95 [==============================] - 0s 963us/step - loss: 626971456.0000 - mean_absolute_error: 22014.7656
95/95 [==============================] - 0s 985us/step - loss: 626971456.0000 - mean_absolute_error: 22014.7656
Epoch 28/100
95/95 [==============================] - 0s 1ms/step - loss: 543345280.0000 - mean_absolute_error: 20333.5000
95/95 [==============================] - 0s 1ms/step - loss: 543345280.0000 - mean_absolute_error: 20333.5000
Epoch 29/100
95/95 [==============================] - 0s 936us/step - loss: 427543648.0000 - mean_absolute_error: 17390.9902
95/95 [==============================] - 0s 954us/step - loss: 427543648.0000 - mean_absolute_error: 17390.9902
Epoch 30/100
95/95 [==============================] - 0s 935us/step - loss: 423319488.0000 - mean_absolute_error: 16594.0332
95/95 [==============================] - 0s 954us/step - loss: 423319488.0000 - mean_absolute_error: 16594.0332
Epoch 31/100
95/95 [==============================] - 0s 960us/step - loss: 285892160.0000 - mean_absolute_error: 13206.8965
95/95 [==============================] - 0s 978us/step - loss: 285892160.0000 - mean_absolute_error: 13206.8965
Epoch 32/100
95/95 [==============================] - 0s 1000us/step - loss: 290182112.0000 - mean_absolute_error: 12751.9131
95/95 [==============================] - 0s 1ms/step - loss: 290182112.0000 - mean_absolute_error: 12751.9131   
Epoch 33/100
95/95 [==============================] - 0s 941us/step - loss: 198720688.0000 - mean_absolute_error: 10482.3623
95/95 [==============================] - 0s 959us/step - loss: 198720688.0000 - mean_absolute_error: 10482.3623
Epoch 34/100
95/95 [==============================] - 0s 1ms/step - loss: 187827216.0000 - mean_absolute_error: 10195.4482
95/95 [==============================] - 0s 1ms/step - loss: 187827216.0000 - mean_absolute_error: 10195.4482
Epoch 35/100
95/95 [==============================] - 0s 946us/step - loss: 155347984.0000 - mean_absolute_error: 9209.9160
95/95 [==============================] - 0s 965us/step - loss: 155347984.0000 - mean_absolute_error: 9209.9160
Epoch 36/100
95/95 [==============================] - 0s 953us/step - loss: 146223712.0000 - mean_absolute_error: 9113.0127
95/95 [==============================] - 0s 971us/step - loss: 146223712.0000 - mean_absolute_error: 9113.0127
Epoch 37/100
95/95 [==============================] - 0s 937us/step - loss: 154123376.0000 - mean_absolute_error: 9403.7441
95/95 [==============================] - 0s 955us/step - loss: 154123376.0000 - mean_absolute_error: 9403.7441
Epoch 38/100
95/95 [==============================] - 0s 934us/step - loss: 129089280.0000 - mean_absolute_error: 8930.9668
95/95 [==============================] - 0s 953us/step - loss: 129089280.0000 - mean_absolute_error: 8930.9668
Epoch 39/100
95/95 [==============================] - 0s 970us/step - loss: 126166240.0000 - mean_absolute_error: 8984.3662
95/95 [==============================] - 0s 991us/step - loss: 126166240.0000 - mean_absolute_error: 8984.3662
Epoch 40/100
95/95 [==============================] - 0s 990us/step - loss: 154446672.0000 - mean_absolute_error: 9635.0381
95/95 [==============================] - 0s 1ms/step - loss: 154446672.0000 - mean_absolute_error: 9635.0381  
Epoch 41/100
95/95 [==============================] - 0s 1ms/step - loss: 119043600.0000 - mean_absolute_error: 8867.8887
95/95 [==============================] - 0s 1ms/step - loss: 119043600.0000 - mean_absolute_error: 8867.8887
Epoch 42/100
95/95 [==============================] - 0s 999us/step - loss: 121132656.0000 - mean_absolute_error: 9012.5205
95/95 [==============================] - 0s 1ms/step - loss: 121132656.0000 - mean_absolute_error: 9012.5205  
Epoch 43/100
95/95 [==============================] - 0s 965us/step - loss: 121195816.0000 - mean_absolute_error: 9054.5811
95/95 [==============================] - 0s 984us/step - loss: 121195816.0000 - mean_absolute_error: 9054.5811
Epoch 44/100
95/95 [==============================] - 0s 998us/step - loss: 121027904.0000 - mean_absolute_error: 9074.7832
95/95 [==============================] - 0s 1ms/step - loss: 121027904.0000 - mean_absolute_error: 9074.7832  
Epoch 45/100
95/95 [==============================] - 0s 977us/step - loss: 120532168.0000 - mean_absolute_error: 9077.0459
95/95 [==============================] - 0s 995us/step - loss: 120532168.0000 - mean_absolute_error: 9077.0459
Epoch 46/100
95/95 [==============================] - 0s 971us/step - loss: 119858808.0000 - mean_absolute_error: 9068.0781
95/95 [==============================] - 0s 990us/step - loss: 119858808.0000 - mean_absolute_error: 9068.0781
Epoch 47/100
95/95 [==============================] - 0s 960us/step - loss: 119029368.0000 - mean_absolute_error: 9049.1045
95/95 [==============================] - 0s 981us/step - loss: 119029368.0000 - mean_absolute_error: 9049.1045
Epoch 48/100
95/95 [==============================] - 0s 940us/step - loss: 118019464.0000 - mean_absolute_error: 9021.1074
95/95 [==============================] - 0s 958us/step - loss: 118019464.0000 - mean_absolute_error: 9021.1074
Epoch 49/100
95/95 [==============================] - 0s 950us/step - loss: 116899800.0000 - mean_absolute_error: 8990.7236
95/95 [==============================] - 0s 968us/step - loss: 116899800.0000 - mean_absolute_error: 8990.7236
Epoch 50/100
95/95 [==============================] - 0s 935us/step - loss: 116038000.0000 - mean_absolute_error: 8977.5967
95/95 [==============================] - 0s 953us/step - loss: 116038000.0000 - mean_absolute_error: 8977.5967
Epoch 51/100
95/95 [==============================] - 0s 953us/step - loss: 115879160.0000 - mean_absolute_error: 8994.3770
95/95 [==============================] - 0s 973us/step - loss: 115879160.0000 - mean_absolute_error: 8994.3770
Epoch 52/100
95/95 [==============================] - 0s 961us/step - loss: 116091552.0000 - mean_absolute_error: 9020.5068
95/95 [==============================] - 0s 980us/step - loss: 116091552.0000 - mean_absolute_error: 9020.5068
Epoch 53/100
95/95 [==============================] - 0s 1ms/step - loss: 116028264.0000 - mean_absolute_error: 9028.4033
95/95 [==============================] - 0s 1ms/step - loss: 116028264.0000 - mean_absolute_error: 9028.4033
Epoch 54/100
95/95 [==============================] - 0s 967us/step - loss: 115593200.0000 - mean_absolute_error: 9015.3213
95/95 [==============================] - 0s 991us/step - loss: 115593200.0000 - mean_absolute_error: 9015.3213
Epoch 55/100
95/95 [==============================] - 0s 952us/step - loss: 114857024.0000 - mean_absolute_error: 8983.4785
95/95 [==============================] - 0s 971us/step - loss: 114857024.0000 - mean_absolute_error: 8983.4785
Epoch 56/100
95/95 [==============================] - 0s 949us/step - loss: 113955728.0000 - mean_absolute_error: 8936.1357
95/95 [==============================] - 0s 968us/step - loss: 113955728.0000 - mean_absolute_error: 8936.1357
Epoch 57/100
95/95 [==============================] - 0s 958us/step - loss: 113512496.0000 - mean_absolute_error: 8902.2705
95/95 [==============================] - 0s 976us/step - loss: 113512496.0000 - mean_absolute_error: 8902.2705
Epoch 58/100
95/95 [==============================] - 0s 939us/step - loss: 114512992.0000 - mean_absolute_error: 8919.9863
95/95 [==============================] - 0s 957us/step - loss: 114512992.0000 - mean_absolute_error: 8919.9863
Epoch 59/100
95/95 [==============================] - 0s 1ms/step - loss: 115354064.0000 - mean_absolute_error: 8934.1367
95/95 [==============================] - 0s 1ms/step - loss: 115354064.0000 - mean_absolute_error: 8934.1367
Epoch 60/100
95/95 [==============================] - 0s 977us/step - loss: 114461760.0000 - mean_absolute_error: 8903.4883
95/95 [==============================] - 0s 995us/step - loss: 114461760.0000 - mean_absolute_error: 8903.4883
Epoch 61/100
95/95 [==============================] - 0s 1ms/step - loss: 114282992.0000 - mean_absolute_error: 8890.1396
95/95 [==============================] - 0s 1ms/step - loss: 114282992.0000 - mean_absolute_error: 8890.1396
Epoch 62/100
95/95 [==============================] - 0s 1ms/step - loss: 113984776.0000 - mean_absolute_error: 8873.9355
95/95 [==============================] - 0s 1ms/step - loss: 113984776.0000 - mean_absolute_error: 8873.9355
Epoch 63/100
95/95 [==============================] - 0s 1ms/step - loss: 113654192.0000 - mean_absolute_error: 8856.6582
95/95 [==============================] - 0s 1ms/step - loss: 113654192.0000 - mean_absolute_error: 8856.6582
Epoch 64/100
95/95 [==============================] - 0s 974us/step - loss: 113351792.0000 - mean_absolute_error: 8839.8721
95/95 [==============================] - 0s 994us/step - loss: 113351792.0000 - mean_absolute_error: 8839.8721
Epoch 65/100
95/95 [==============================] - 0s 986us/step - loss: 113044024.0000 - mean_absolute_error: 8822.7383
95/95 [==============================] - 0s 1ms/step - loss: 113044024.0000 - mean_absolute_error: 8822.7383  
Epoch 66/100
95/95 [==============================] - 0s 939us/step - loss: 112738160.0000 - mean_absolute_error: 8805.4150
95/95 [==============================] - 0s 957us/step - loss: 112738160.0000 - mean_absolute_error: 8805.4150
Epoch 67/100
95/95 [==============================] - 0s 947us/step - loss: 112437472.0000 - mean_absolute_error: 8788.0811
95/95 [==============================] - 0s 966us/step - loss: 112437472.0000 - mean_absolute_error: 8788.0811
Epoch 68/100
95/95 [==============================] - 0s 937us/step - loss: 112141400.0000 - mean_absolute_error: 8770.7109
95/95 [==============================] - 0s 955us/step - loss: 112141400.0000 - mean_absolute_error: 8770.7109
Epoch 69/100
95/95 [==============================] - 0s 938us/step - loss: 111851368.0000 - mean_absolute_error: 8753.3418
95/95 [==============================] - 0s 956us/step - loss: 111851368.0000 - mean_absolute_error: 8753.3418
Epoch 70/100
95/95 [==============================] - 0s 994us/step - loss: 111567328.0000 - mean_absolute_error: 8736.0479
95/95 [==============================] - 0s 1ms/step - loss: 111567328.0000 - mean_absolute_error: 8736.0479  
Epoch 71/100
95/95 [==============================] - 0s 991us/step - loss: 111290872.0000 - mean_absolute_error: 8718.8789
95/95 [==============================] - 0s 1ms/step - loss: 111290872.0000 - mean_absolute_error: 8718.8789  
Epoch 72/100
95/95 [==============================] - 0s 940us/step - loss: 111022352.0000 - mean_absolute_error: 8701.8867
95/95 [==============================] - 0s 959us/step - loss: 111022352.0000 - mean_absolute_error: 8701.8867
Epoch 73/100
95/95 [==============================] - 0s 938us/step - loss: 110762352.0000 - mean_absolute_error: 8685.1445
95/95 [==============================] - 0s 956us/step - loss: 110762352.0000 - mean_absolute_error: 8685.1445
Epoch 74/100
95/95 [==============================] - 0s 1ms/step - loss: 110511616.0000 - mean_absolute_error: 8668.7275
95/95 [==============================] - 0s 1ms/step - loss: 110511616.0000 - mean_absolute_error: 8668.7275
Epoch 75/100
95/95 [==============================] - 0s 1ms/step - loss: 110270888.0000 - mean_absolute_error: 8652.7598
95/95 [==============================] - 0s 1ms/step - loss: 110270888.0000 - mean_absolute_error: 8652.7598
Epoch 76/100
95/95 [==============================] - 0s 993us/step - loss: 110039920.0000 - mean_absolute_error: 8637.2275
95/95 [==============================] - 0s 1ms/step - loss: 110039920.0000 - mean_absolute_error: 8637.2275  
Epoch 77/100
95/95 [==============================] - 0s 1ms/step - loss: 109819736.0000 - mean_absolute_error: 8622.2402
95/95 [==============================] - 0s 1ms/step - loss: 109819736.0000 - mean_absolute_error: 8622.2402
Epoch 78/100
95/95 [==============================] - 0s 939us/step - loss: 109608840.0000 - mean_absolute_error: 8607.7178
95/95 [==============================] - 0s 960us/step - loss: 109608840.0000 - mean_absolute_error: 8607.7178
Epoch 79/100
95/95 [==============================] - 0s 941us/step - loss: 109406368.0000 - mean_absolute_error: 8593.4668
95/95 [==============================] - 0s 959us/step - loss: 109406368.0000 - mean_absolute_error: 8593.4668
Epoch 80/100
95/95 [==============================] - 0s 945us/step - loss: 109215568.0000 - mean_absolute_error: 8579.8574
95/95 [==============================] - 0s 964us/step - loss: 109215568.0000 - mean_absolute_error: 8579.8574
Epoch 81/100
95/95 [==============================] - 0s 939us/step - loss: 109036472.0000 - mean_absolute_error: 8566.9336
95/95 [==============================] - 0s 957us/step - loss: 109036472.0000 - mean_absolute_error: 8566.9336
Epoch 82/100
95/95 [==============================] - 0s 942us/step - loss: 108867080.0000 - mean_absolute_error: 8554.7197
95/95 [==============================] - 0s 960us/step - loss: 108867080.0000 - mean_absolute_error: 8554.7197
Epoch 83/100
95/95 [==============================] - 0s 991us/step - loss: 108707168.0000 - mean_absolute_error: 8543.0459
95/95 [==============================] - 0s 1ms/step - loss: 108707168.0000 - mean_absolute_error: 8543.0459  
Epoch 84/100
95/95 [==============================] - 0s 1ms/step - loss: 108557784.0000 - mean_absolute_error: 8532.0527
95/95 [==============================] - 0s 1ms/step - loss: 108557784.0000 - mean_absolute_error: 8532.0527
Epoch 85/100
95/95 [==============================] - 0s 942us/step - loss: 108417984.0000 - mean_absolute_error: 8521.5693
95/95 [==============================] - 0s 960us/step - loss: 108417984.0000 - mean_absolute_error: 8521.5693
Epoch 86/100
95/95 [==============================] - 0s 950us/step - loss: 108287504.0000 - mean_absolute_error: 8511.8604
95/95 [==============================] - 0s 969us/step - loss: 108287504.0000 - mean_absolute_error: 8511.8604
Epoch 87/100
95/95 [==============================] - 0s 932us/step - loss: 108166552.0000 - mean_absolute_error: 8502.8643
95/95 [==============================] - 0s 950us/step - loss: 108166552.0000 - mean_absolute_error: 8502.8643
Epoch 88/100
95/95 [==============================] - 0s 951us/step - loss: 108054128.0000 - mean_absolute_error: 8494.6445
95/95 [==============================] - 0s 969us/step - loss: 108054128.0000 - mean_absolute_error: 8494.6445
Epoch 89/100
95/95 [==============================] - 0s 1ms/step - loss: 107951360.0000 - mean_absolute_error: 8487.1943
95/95 [==============================] - 0s 1ms/step - loss: 107951360.0000 - mean_absolute_error: 8487.1943
Epoch 90/100
95/95 [==============================] - 0s 935us/step - loss: 107855352.0000 - mean_absolute_error: 8480.3857
95/95 [==============================] - 0s 953us/step - loss: 107855352.0000 - mean_absolute_error: 8480.3857
Epoch 91/100
95/95 [==============================] - 0s 931us/step - loss: 107767304.0000 - mean_absolute_error: 8474.1924
95/95 [==============================] - 0s 949us/step - loss: 107767304.0000 - mean_absolute_error: 8474.1924
Epoch 92/100
95/95 [==============================] - 0s 928us/step - loss: 107685704.0000 - mean_absolute_error: 8468.6035
95/95 [==============================] - 0s 948us/step - loss: 107685704.0000 - mean_absolute_error: 8468.6035
Epoch 93/100
95/95 [==============================] - 0s 1ms/step - loss: 107610288.0000 - mean_absolute_error: 8463.5527
95/95 [==============================] - 0s 1ms/step - loss: 107610288.0000 - mean_absolute_error: 8463.5527
Epoch 94/100
95/95 [==============================] - 0s 969us/step - loss: 107540520.0000 - mean_absolute_error: 8459.0303
95/95 [==============================] - 0s 991us/step - loss: 107540520.0000 - mean_absolute_error: 8459.0303
Epoch 95/100
95/95 [==============================] - 0s 1ms/step - loss: 107476288.0000 - mean_absolute_error: 8454.9707
95/95 [==============================] - 0s 1ms/step - loss: 107476288.0000 - mean_absolute_error: 8454.9707
Epoch 96/100
95/95 [==============================] - 0s 959us/step - loss: 107416488.0000 - mean_absolute_error: 8451.3584
95/95 [==============================] - 0s 977us/step - loss: 107416488.0000 - mean_absolute_error: 8451.3584
Epoch 97/100
95/95 [==============================] - 0s 940us/step - loss: 107360464.0000 - mean_absolute_error: 8448.1270
95/95 [==============================] - 0s 958us/step - loss: 107360464.0000 - mean_absolute_error: 8448.1270
Epoch 98/100
95/95 [==============================] - 0s 935us/step - loss: 107308904.0000 - mean_absolute_error: 8445.2510
95/95 [==============================] - 0s 953us/step - loss: 107308904.0000 - mean_absolute_error: 8445.2510
Epoch 99/100
95/95 [==============================] - 0s 946us/step - loss: 107260096.0000 - mean_absolute_error: 8442.6660
95/95 [==============================] - 0s 966us/step - loss: 107260096.0000 - mean_absolute_error: 8442.6660
Epoch 100/100
95/95 [==============================] - 0s 943us/step - loss: 107214256.0000 - mean_absolute_error: 8440.3213
95/95 [==============================] - 0s 962us/step - loss: 107214256.0000 - mean_absolute_error: 8440.3213
[1] "step4"
[1] 4675
[1] "step5"
[1] "Score final : 11001.1459735947"
> > + + > > > > > 
── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character()
)
ℹ Use `spec()` for the full column specifications.

> 
── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character(),
  Usage = col_character()
)
ℹ Use `spec()` for the full column specifications.

> > > > . + > > > > > > [1] 3303
> > > > > > > + + > > + > . + > > > > + + > Call:
lm(formula = Load ~ Load.1 + Load.7 + Temp + Temp_s95 + WeekDays + 
    GovernmentResponseIndex, data = train)

Residuals:
     Min       1Q   Median       3Q      Max 
-10526.2  -1304.4    147.1   1369.5   6430.4 

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
(Intercept)              1.957e+04  5.333e+02  36.687  < 2e-16 ***
Load.1                   7.261e-01  9.519e-03  76.273  < 2e-16 ***
Load.7                   8.780e-02  8.258e-03  10.633  < 2e-16 ***
Temp                    -6.524e+02  6.980e+01  -9.346  < 2e-16 ***
Temp_s95                 3.229e+02  7.229e+01   4.467 8.24e-06 ***
WeekDays                -1.309e+03  2.290e+01 -57.167  < 2e-16 ***
GovernmentResponseIndex -3.480e+01  5.039e+00  -6.906 6.05e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2134 on 3021 degrees of freedom
Multiple R-squared:  0.9626,	Adjusted R-squared:  0.9626 
F-statistic: 1.297e+04 on 6 and 3021 DF,  p-value: < 2.2e-16

> 
> Gam <- gam(Load~s(Load.1)+s(Load.7)+s(Temp)
+            +s(Temp_s95)+s(WeekDays,k=7)
+            +s(GovernmentResponseIndex)
+            ,data=train)
Error in gam(Load ~ s(Load.1) + s(Load.7) + s(Temp) + s(Temp_s95) + s(WeekDays,  : 
  argument inutilisé (data = train)
> gam.test = gam(train, test)
Error in gam(Load ~ s(Load.1) + s(Load.7) + s(Temp) + s(Temp_s95) + s(WeekDays,  : 
  argument inutilisé (data = train)
> gam_rte = function(train, test, plt = FALSE){
+     Gam <- gam(Load~s(Load.1)+s(Load.7)+s(Temp)
+                +s(Temp_s95)+s(WeekDays,k=7)
+                +s(GovernmentResponseIndex)
+               ,data=train)
+     summary(Gam)
+     gam.train = predict(Gam, newdata=train)
+     gam.test = predict(Gam, newdata=test)
+     if (plt){
+         par(mfrow=c(1,1))
+         plot(train$Load,type='l', xlim=c(0,length(total.time)))
+         lines(train$time,Gam$fit, col='red', lwd=1)
+         lines(test$time,gam.test, col='green', lwd=1)
+     }
+     return(gam.test)
+ }
> 
> rm(list=objects())
+ graphics.off()
+ 
+ ##pour load tout en 2 lignes, il faut juste rajouter les librairies dans libs.to.load
+ 
+ libs.to.load = c("tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
+ suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
+ 
+ ##setwd("C:/Users/CM/code/M1/R")
+ 
+ ##load tous les fichiers en sources
+ files.sources = list.files(pattern = "*.r$")
+ files.sources = files.sources[files.sources != "main_matthieu.r"]
+ sapply(files.sources, source)
+ 
+ ##
+ plt = FALSE #(si on veut plot, mettre à TRUE)
+ path_submission = "./submissions/submission.csv"
+ 
+ 
+ ##MAIN NICOLAS
+ model = "lstm"
+ 
+ data = format_data("./data/train_V2.csv", "./data/test_V2.csv")
+ train_set = data$train_set
+ test_set = data$test_set
+ train_label = data$train_label
+ test_label = data$test_label
+ 
+ if (model == "xgboost"){
+   pred = xgboost_rte(train_set, train_label, test_set)
+ } else if(model == "lstm"){
+   pred = lstm(train_set, train_label, test_set)
+ }
+ 
+ 
+ print(paste("Score final : ", evaluate(test_label, pred), sep=""))
+ 
+ if (plt) {
+     plot(c(train_label, pred))
+ }
+ 
+ ##FIN MAIN NICOLAS 
+ 
+ 
+ train <- read_delim(file="data/train_V2.csv",delim=',')
+ test <- read_delim(file="data/test_V2.csv",delim=',')
+ 
+ reg = lm(Load~Temp, data=train)
+ 
+ if (plt){
+     plot(train$Date, train$Load, type='l')
+     par(new=T)
+     plot(train$Date, train$Temp, type='l', col='red')
+ 
+     plot(train$Temp,train$Load, col='red')
+     points(train$Temp ,reg$fitted.values)
+ }
+ 
+ train$WeekDays = days_to_numeric(train)
+ test$WeekDays = days_to_numeric(test)
+ 
+ train$Year = train$Year - 2012
+ test$Year = test$Year - 2012
+ 
+ total.time = c(1:(nrow(train)+nrow(test)))
+ length(total.time)
+ train$time = total.time[1:nrow(train)]
+ test$time = tail(total.time,nrow(test))
+ 
+ 
+ #saisonalite : annuelle
+ 
+ if (plt){
+     plot(train$Date, train$Load, type='l')
+ }
+ 
+ MA <- stats::filter(train$Load, filter = rep(1/365,365),
+              method = c("convolution"), sides = 2, circular = FALSE)
+ if (plt){
+     plot(train$Date, train$Load, type = "l", xlab = "",
+          ylab = "consumption (kw)", col = "seagreen4", lwd = 1)
+     lines(train$Date, MA, col = "red", lwd = 2)
+ }
+ 
+ ##estimation avec fourier
+ pred.fourier = fourier(train, test, plt = TRUE)
+ 
+ ##saisonalite : hebdomadaire
+ 
+ num.years = 0
+ average = 0
+ N = 8; a = 0.7 #exponential weight
+ while (365*(num.years+1) <= length(train$Date)) {
+     par(mfrow = c(1, 1))
+     dateyear = train$Date[(365*num.years+1):(365*(num.years+1))]
+     loadyear = train$Load[(365*num.years+1):(365*(num.years+1))]
+ 
+     ##plot(dateyear,loadyear,type='l')
+   
+     MAw <- stats::filter(loadyear, filter = rep(1/52,52),
+                       method = c("convolution"), sides = 2, circular = T)
+     ##plot(dateyear,loadyear, type = "l", xlab = "",
+     ##    ylab = "consumption (kw)", col = "seagreen4", lwd = 1)
+     ##lines(dateyear, MAw, col = "red", lwd = 2)
+   
+     ##plot(dateyear, loadyear - MAw, type="l")
+     expweight = (((1-a)/(1-a^8))*a^(N-(num.years+1)))
+     average = expweight*(average + (loadyear - MAw))
+     num.years = num.years + 1
+ }
+ if (plt){
+     plot(average)
+ }
+ 
+ train.to.day = train$time %% 365 + 1
+ test.to.day = test$time %% 365 + 1
+ 
+ pred.hebdo.train = average[train.to.day]
+ pred.hebdo.test = average[test.to.day]
+ 
+ pred.total.train = reg$fitted + pred.hebdo.train
+ pred.total.test = pred.fourier + pred.hebdo.test
+ 
+ 
+ if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(train$time,pred.total.train, col='red', lwd=1)
+     lines(test$time,pred.total.test, col='green', lwd=1)
+ }
+ 
+ Load = pred.total.test
+ Id = test$Id
+ submission = data.frame(Load, Id)
+ 
+ write.csv(submission, file =path_submission, row.names=F)
+ 
+ 
+ MAw.train.total <- stats::filter(train$Load.1, filter = rep(1/52,52),
+                      method = c("convolution"), sides = 2, circular = T)
+ MAw.test.total <- stats::filter(test$Load.1, filter = rep(1/52,52),
+                                  method = c("convolution"), sides = 2, circular = T)
+ 
+ train$seasonal.tendancy = train$Load.1 - MAw.train.total
+ test$seasonal.tendancy = test$Load.1 - MAw.test.total
+ 
+ train$fourier.fitted = reg$fitted
+ test$fourier.fitted = pred.fourier
+ 
+ reg2 = lm(Load~Load.1+Load.7+Temp
+ +Temp_s95+WeekDays+GovernmentResponseIndex,data=train)
+ summary(reg2)
+ 
   tidyverse    lubridate       ranger       pracma      Metrics         mgcv 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       keras       visreg        caret         mc2d        opera        abind 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
randomForest   tensorflow 
        TRUE         TRUE 
> > $cross_validation.r
$cross_validation.r$value
function (train, test) {
  set.seed(123)
  to.extract = rbern(n=length(train$Load),p=0.8)==T
  to.train = train[F,]
  for (j in 1:length(to.extract)) {
    if (to.extract[j]) {
      to.train[nrow(to.train)+1,] = train[j,]
    }
  }
  
  cross.train  <- train[to.train$time, ]
  cross.test <- train[-to.train$time, ]
  
  model = gam(Load~s(Load.1)+s(Load.7)+s(Temp)
               +s(Temp_s95)+s(WeekDays,k=7)
               ,data=cross.train)
  
  pred.test = predict(model,test)
  print(length(pred.test))
  N = length(test$Load.1)
  RMSE = rmse(pred.test[-N], test$Load.1[2:N]) #on connait pas le dernier
  
  print(RMSE)
  return(list("model"=model,"pred.test"=pred.test,"RMSE"=RMSE))
}

$cross_validation.r$visible
[1] FALSE


$days_to_numeric.r
$days_to_numeric.r$value
function (data) {
    data$WeekDays[data$WeekDays=='Monday']    = 1
    data$WeekDays[data$WeekDays=='Tuesday']   = 2
    data$WeekDays[data$WeekDays=='Wednesday'] = 3
    data$WeekDays[data$WeekDays=='Thursday']  = 4
    data$WeekDays[data$WeekDays=='Friday']    = 5
    data$WeekDays[data$WeekDays=='Saturday']  = 6
    data$WeekDays[data$WeekDays=='Sunday']    = 7
    data$WeekDays = as.numeric(data$WeekDays)
    return (data$WeekDays)
}

$days_to_numeric.r$visible
[1] FALSE


$evaluate.r
$evaluate.r$value
function(test_label, predicted_set){
    
    return(rmse(test_label, predicted_set))
}

$evaluate.r$visible
[1] FALSE


$format_data.r
$format_data.r$value
function(file_train, file_test) {

    print(paste("Load and format " , file_train, sep = " "))
    train_set = read_csv(file_train, col_types = cols())

    train_set$WeekDays[train_set$WeekDays == "Monday"] <- 1
    train_set$WeekDays[train_set$WeekDays == "Tuesday"] <- 2
    train_set$WeekDays[train_set$WeekDays == "Wednesday"] <- 3
    train_set$WeekDays[train_set$WeekDays == "Thursday"] <- 4
    train_set$WeekDays[train_set$WeekDays == "Friday"] <- 5
    train_set$WeekDays[train_set$WeekDays == "Saturday"] <- 6
    train_set$WeekDays[train_set$WeekDays == "Sunday"] <- 7
    train_set$WeekDays = as.integer(train_set$WeekDays)

    train_set$Year = NULL
    train_set$Date = NULL
    train_label = data.matrix(train_set$Load)


    train_set$Load = NULL
    
    train_set = data.matrix(train_set)


    print(paste("Load and format " , file_test, sep = " "))    
    test_set = read_csv(file_test, col_types = cols())

    test_set$WeekDays[test_set$WeekDays == "Monday"] <- 1
    test_set$WeekDays[test_set$WeekDays == "Tuesday"] <- 2
    test_set$WeekDays[test_set$WeekDays == "Wednesday"] <- 3
    test_set$WeekDays[test_set$WeekDays == "Thursday"] <- 4
    test_set$WeekDays[test_set$WeekDays == "Friday"] <- 5
    test_set$WeekDays[test_set$WeekDays == "Saturday"] <- 6
    test_set$WeekDays[test_set$WeekDays == "Sunday"] <- 7
    test_set$WeekDays = as.integer(test_set$WeekDays)

    test_label = test_set$Load.1
    tmp = test_set$Load.1
    for (i in c(1:(length(tmp)-1))){
        test_label[i] = tmp[i+1]
    }
    
    test_set$Year = NULL
    test_set$Date = NULL
    test_set$Id = NULL
    test_set$Usage = NULL
    test_set = data.matrix(test_set)
    

    test_set = data.matrix(test_set)
    return(list("train_set" = train_set, "train_label" = train_label, "test_set" = test_set, "test_label" = test_label))
}

$format_data.r$visible
[1] FALSE


$fourier.r
$fourier.r$value
function(train, test, plt = FALSE){
    total.time = c(1:(nrow(train)+nrow(test)))
    length(total.time)
    train$time = total.time[1:nrow(train)]
    test$time = tail(total.time,nrow(test))

    fourier.make.matrix = function (t, k, period) {
        w = 2*pi/period
        ret = cbind(cos(w*t), sin(w*t))
        
        for(i in c(2:K))
        {
            ret = cbind(ret, cos(i*w*t), sin(i*w*t))
        }
        return (ret)
    }

    K = 5; period = 365
    fourier.train = fourier.make.matrix(train$time, K, period)
    fourier.test = fourier.make.matrix(test$time, K, period)

    fourier.train.df = data.frame(train$Load,fourier.train)
    fourier.test.df = data.frame(fourier.test)

    reg = lm(train.Load ~., data=fourier.train.df)

    pred.fourier = predict(reg, newdata=fourier.test.df)
    total.fourier = c(reg$fitted,pred.fourier)
    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(reg$fitted,col='red', lwd=2)
        lines(test$time,pred.fourier,col='green', lwd=2)
    }
    return(pred.fourier)
}

$fourier.r$visible
[1] FALSE


$GAM.r
$GAM.r$value
function(train, test, plt = FALSE){
    Gam <- gam(Load~s(Load.1)+s(Load.7)+s(Temp)
               +s(Temp_s95)+s(WeekDays,k=7)
               +s(GovernmentResponseIndex)
              ,data=train)
    summary(Gam)


    gam.train = predict(Gam, newdata=train)
    gam.test = predict(Gam, newdata=test)

    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(train$time,Gam$fit, col='red', lwd=1)
        lines(test$time,gam.test, col='green', lwd=1)
    }

    return(gam.test)
}

$GAM.r$visible
[1] FALSE


$lstm.r
$lstm.r$value
function(train_set, train_label, test_set){
  N = length(train_set[,'Load.1'])
  nb_var = 17
  y = train_label
  x = array(train_set, dim = c(N, nb_var, 1))
  x_test = array(test_set,dim = c(length(test_set[,'Load.1']), nb_var, 1))
  print("step1")
  model = keras_model_sequential() %>%   
    #layer_lstm(units=128, input_shape=c(nb_var, 1), activation="relu", return_sequences = TRUE) %>% 
    layer_dense(units=64, activation = "relu") %>%  
    layer_dense(units=32) %>%  
    layer_dense(units=1, activation = "linear")
  print("step2")
  model %>% compile(loss = 'mse',
                    optimizer = 'adam',
                    metrics = list("mean_absolute_error")
  )
  print("step3")
  model %>% fit(x,y, epochs=100, batch_size=32, shuffle = FALSE)
  print("step4")
  print(length(x_test))
  pred = model %>% predict(x_test)
  print("step5")
  return(pred)
    
}

$lstm.r$visible
[1] FALSE


$main.r
NULL

$neural_network.r
$neural_network.r$value
function(train, test, plt = FALSE){
    labels = train$Load
    train.lstm = train[,-c(1,2,22,23)]
    test.lstm = test[,-c(1,20,21,23,24)]

    lstm = function(train_set, train_label, test_set) {
        y = train_label
        x = data.matrix(train_set)
        x_test = data.matrix(test_set)
        model = keras_model_sequential() %>%
            layer_dense(units = 19, activation = 'relu', input_shape = c(19)) %>%
            layer_dense(units = 12, activation = 'relu') %>%
            layer_dense(units = 6, activation = 'relu') %>%
            layer_dense(units=1, activation = "linear")
        
        model %>% compile(loss = 'mse',
                          optimizer = optimizer_adam(0.0005))
        
        model %>% fit(x,y, epochs=15)

        pred.train = model %>% predict(x)
        pred.test = model %>% predict(x_test)

        return(list("train"=pred.train,"test"=pred.test))
    }


    pred.lstm = lstm(train.lstm, labels, test.lstm)

    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(test$time,pred.lstm$test, col='green', lwd=1)
    }

    N = length(test$Load.1)
    RMSE = rmse(pred.lstm$test[-N], test$Load.1[2:N])
    RMSE
    return(pred.lstm)
}

$neural_network.r$visible
[1] FALSE


$random_forest.r
$random_forest.r$value
function(train, test, plt = FALSE){

    rf = randomForest(Load ~ ., data=train, mtry=3,
                      importance=TRUE, na.action=na.omit)
    pred.test.rf = predict(rf,test)

    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(test$time,pred.test.rf, col='green', lwd=1)
    }

    N = length(test$Load.1)
    RMSE = rmse(pred.test.rf[-N], test$Load.1[2:N])
    RMSE
    return(pred.test.rf)
 }

$random_forest.r$visible
[1] FALSE


$scale.r
$scale.r$value
function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

$scale.r$visible
[1] FALSE


$xgboost.r
$xgboost.r$value
function(train_set, train_label, test_set){
  param = list(booster = "gblinear", objective = "reg:squarederror", eval_metric = "rmse", lambda = 0.0003, alpha = 0.0003, nthread = 2, eta = 0.1)
  
  print("Model : XGBOOST")
  
  xgbmodel = xgboost(data = train_set, label = train_label, nrounds = 200, params = param, verbose = 0)
  
  pred = predict(xgbmodel, test_set)
  
  return(pred)
}

$xgboost.r$visible
[1] FALSE


> > > > > > > > > > [1] "Load and format  ./data/train_V2.csv"
[1] "Load and format  ./data/test_V2.csv"
> > > > > > . + > [1] "step1"
[1] "step2"
[1] "step3"
Epoch 1/100
95/95 [==============================] - 0s 968us/step - loss: 2801176832.0000 - mean_absolute_error: 49785.4414
95/95 [==============================] - 0s 986us/step - loss: 2801176832.0000 - mean_absolute_error: 49785.4414
Epoch 2/100
95/95 [==============================] - 0s 941us/step - loss: 2740463616.0000 - mean_absolute_error: 48554.9844
95/95 [==============================] - 0s 960us/step - loss: 2740463616.0000 - mean_absolute_error: 48554.9844
Epoch 3/100
95/95 [==============================] - 0s 983us/step - loss: 2739959552.0000 - mean_absolute_error: 48556.6875
95/95 [==============================] - 0s 1ms/step - loss: 2739959552.0000 - mean_absolute_error: 48556.6875  
Epoch 4/100
95/95 [==============================] - 0s 929us/step - loss: 2738844928.0000 - mean_absolute_error: 48554.8906
95/95 [==============================] - 0s 948us/step - loss: 2738844928.0000 - mean_absolute_error: 48554.8906
Epoch 5/100
95/95 [==============================] - 0s 968us/step - loss: 2737188352.0000 - mean_absolute_error: 48561.3203
95/95 [==============================] - 0s 988us/step - loss: 2737188352.0000 - mean_absolute_error: 48561.3203
Epoch 6/100
95/95 [==============================] - 0s 968us/step - loss: 2734904320.0000 - mean_absolute_error: 48539.9727
95/95 [==============================] - 0s 986us/step - loss: 2734904320.0000 - mean_absolute_error: 48539.9727
Epoch 7/100
95/95 [==============================] - 0s 915us/step - loss: 2731446272.0000 - mean_absolute_error: 48501.3789
95/95 [==============================] - 0s 933us/step - loss: 2731446272.0000 - mean_absolute_error: 48501.3789
Epoch 8/100
95/95 [==============================] - 0s 975us/step - loss: 2726755840.0000 - mean_absolute_error: 48476.8516
95/95 [==============================] - 0s 993us/step - loss: 2726755840.0000 - mean_absolute_error: 48476.8516
Epoch 9/100
95/95 [==============================] - 0s 927us/step - loss: 2719941632.0000 - mean_absolute_error: 48407.6875
95/95 [==============================] - 0s 946us/step - loss: 2719941632.0000 - mean_absolute_error: 48407.6875
Epoch 10/100
95/95 [==============================] - 0s 912us/step - loss: 2710204416.0000 - mean_absolute_error: 48325.6289
95/95 [==============================] - 0s 933us/step - loss: 2710204416.0000 - mean_absolute_error: 48325.6289
Epoch 11/100
95/95 [==============================] - 0s 942us/step - loss: 2695094272.0000 - mean_absolute_error: 48169.6133
95/95 [==============================] - 0s 960us/step - loss: 2695094272.0000 - mean_absolute_error: 48169.6133
Epoch 12/100
95/95 [==============================] - 0s 966us/step - loss: 2672550912.0000 - mean_absolute_error: 47959.8047
95/95 [==============================] - 0s 984us/step - loss: 2672550912.0000 - mean_absolute_error: 47959.8047
Epoch 13/100
95/95 [==============================] - 0s 910us/step - loss: 2639622400.0000 - mean_absolute_error: 47667.9727
95/95 [==============================] - 0s 928us/step - loss: 2639622400.0000 - mean_absolute_error: 47667.9727
Epoch 14/100
95/95 [==============================] - 0s 905us/step - loss: 2592794624.0000 - mean_absolute_error: 47267.1211
95/95 [==============================] - 0s 923us/step - loss: 2592794624.0000 - mean_absolute_error: 47267.1211
Epoch 15/100
95/95 [==============================] - 0s 920us/step - loss: 2528531968.0000 - mean_absolute_error: 46720.7500
95/95 [==============================] - 0s 939us/step - loss: 2528531968.0000 - mean_absolute_error: 46720.7500
Epoch 16/100
95/95 [==============================] - 0s 907us/step - loss: 2450635264.0000 - mean_absolute_error: 46078.1836
95/95 [==============================] - 0s 925us/step - loss: 2450635264.0000 - mean_absolute_error: 46078.1836
Epoch 17/100
95/95 [==============================] - 0s 944us/step - loss: 2358808064.0000 - mean_absolute_error: 45371.2773
95/95 [==============================] - 0s 962us/step - loss: 2358808064.0000 - mean_absolute_error: 45371.2773
Epoch 18/100
95/95 [==============================] - 0s 980us/step - loss: 2239469568.0000 - mean_absolute_error: 44003.8359
95/95 [==============================] - 0s 999us/step - loss: 2239469568.0000 - mean_absolute_error: 44003.8359
Epoch 19/100
95/95 [==============================] - 0s 984us/step - loss: 2108918400.0000 - mean_absolute_error: 42630.2539
95/95 [==============================] - 0s 1ms/step - loss: 2108918400.0000 - mean_absolute_error: 42630.2539  
Epoch 20/100
95/95 [==============================] - 0s 914us/step - loss: 1971001728.0000 - mean_absolute_error: 41369.7539
95/95 [==============================] - 0s 932us/step - loss: 1971001728.0000 - mean_absolute_error: 41369.7539
Epoch 21/100
95/95 [==============================] - 0s 908us/step - loss: 1821234688.0000 - mean_absolute_error: 39790.1328
95/95 [==============================] - 0s 926us/step - loss: 1821234688.0000 - mean_absolute_error: 39790.1328
Epoch 22/100
95/95 [==============================] - 0s 906us/step - loss: 1689174272.0000 - mean_absolute_error: 38374.7461
95/95 [==============================] - 0s 926us/step - loss: 1689174272.0000 - mean_absolute_error: 38374.7461
Epoch 23/100
95/95 [==============================] - 0s 911us/step - loss: 1510836992.0000 - mean_absolute_error: 35932.1484
95/95 [==============================] - 0s 929us/step - loss: 1510836992.0000 - mean_absolute_error: 35932.1484
Epoch 24/100
95/95 [==============================] - 0s 908us/step - loss: 1371079808.0000 - mean_absolute_error: 34230.2227
95/95 [==============================] - 0s 926us/step - loss: 1371079808.0000 - mean_absolute_error: 34230.2227
Epoch 25/100
95/95 [==============================] - 0s 925us/step - loss: 1213213568.0000 - mean_absolute_error: 32202.3320
95/95 [==============================] - 0s 944us/step - loss: 1213213568.0000 - mean_absolute_error: 32202.3320
Epoch 26/100
95/95 [==============================] - 0s 941us/step - loss: 1081079936.0000 - mean_absolute_error: 30345.3926
95/95 [==============================] - 0s 959us/step - loss: 1081079936.0000 - mean_absolute_error: 30345.3926
Epoch 27/100
95/95 [==============================] - 0s 929us/step - loss: 928231104.0000 - mean_absolute_error: 27578.0059
95/95 [==============================] - 0s 947us/step - loss: 928231104.0000 - mean_absolute_error: 27578.0059
Epoch 28/100
95/95 [==============================] - 0s 906us/step - loss: 801798272.0000 - mean_absolute_error: 25535.0918
95/95 [==============================] - 0s 924us/step - loss: 801798272.0000 - mean_absolute_error: 25535.0918
Epoch 29/100
95/95 [==============================] - 0s 915us/step - loss: 679635392.0000 - mean_absolute_error: 23204.8613
95/95 [==============================] - 0s 933us/step - loss: 679635392.0000 - mean_absolute_error: 23204.8613
Epoch 30/100
95/95 [==============================] - 0s 949us/step - loss: 614128576.0000 - mean_absolute_error: 21768.3926
95/95 [==============================] - 0s 967us/step - loss: 614128576.0000 - mean_absolute_error: 21768.3926
Epoch 31/100
95/95 [==============================] - 0s 930us/step - loss: 525223328.0000 - mean_absolute_error: 19655.0508
95/95 [==============================] - 0s 948us/step - loss: 525223328.0000 - mean_absolute_error: 19655.0508
Epoch 32/100
95/95 [==============================] - 0s 907us/step - loss: 407362976.0000 - mean_absolute_error: 16859.3730
95/95 [==============================] - 0s 925us/step - loss: 407362976.0000 - mean_absolute_error: 16859.3730
Epoch 33/100
95/95 [==============================] - 0s 920us/step - loss: 365589344.0000 - mean_absolute_error: 15580.0049
95/95 [==============================] - 0s 939us/step - loss: 365589344.0000 - mean_absolute_error: 15580.0049
Epoch 34/100
95/95 [==============================] - 0s 925us/step - loss: 319634560.0000 - mean_absolute_error: 13989.0703
95/95 [==============================] - 0s 943us/step - loss: 319634560.0000 - mean_absolute_error: 13989.0703
Epoch 35/100
95/95 [==============================] - 0s 955us/step - loss: 237310704.0000 - mean_absolute_error: 11665.6064
95/95 [==============================] - 0s 974us/step - loss: 237310704.0000 - mean_absolute_error: 11665.6064
Epoch 36/100
95/95 [==============================] - 0s 934us/step - loss: 208992912.0000 - mean_absolute_error: 10886.9072
95/95 [==============================] - 0s 952us/step - loss: 208992912.0000 - mean_absolute_error: 10886.9072
Epoch 37/100
95/95 [==============================] - 0s 970us/step - loss: 200266176.0000 - mean_absolute_error: 10651.5967
95/95 [==============================] - 0s 991us/step - loss: 200266176.0000 - mean_absolute_error: 10651.5967
Epoch 38/100
95/95 [==============================] - 0s 906us/step - loss: 193213728.0000 - mean_absolute_error: 10108.8232
95/95 [==============================] - 0s 924us/step - loss: 193213728.0000 - mean_absolute_error: 10108.8232
Epoch 39/100
95/95 [==============================] - 0s 907us/step - loss: 142842880.0000 - mean_absolute_error: 8788.0254
95/95 [==============================] - 0s 925us/step - loss: 142842880.0000 - mean_absolute_error: 8788.0254
Epoch 40/100
95/95 [==============================] - 0s 911us/step - loss: 136214352.0000 - mean_absolute_error: 8826.2432
95/95 [==============================] - 0s 929us/step - loss: 136214352.0000 - mean_absolute_error: 8826.2432
Epoch 41/100
95/95 [==============================] - 0s 939us/step - loss: 132750184.0000 - mean_absolute_error: 8920.9785
95/95 [==============================] - 0s 957us/step - loss: 132750184.0000 - mean_absolute_error: 8920.9785
Epoch 42/100
95/95 [==============================] - 0s 909us/step - loss: 128333744.0000 - mean_absolute_error: 8925.3848
95/95 [==============================] - 0s 927us/step - loss: 128333744.0000 - mean_absolute_error: 8925.3848
Epoch 43/100
95/95 [==============================] - 0s 926us/step - loss: 125501232.0000 - mean_absolute_error: 8947.5000
95/95 [==============================] - 0s 945us/step - loss: 125501232.0000 - mean_absolute_error: 8947.5000
Epoch 44/100
95/95 [==============================] - 0s 975us/step - loss: 123515512.0000 - mean_absolute_error: 8973.1768
95/95 [==============================] - 0s 993us/step - loss: 123515512.0000 - mean_absolute_error: 8973.1768
Epoch 45/100
95/95 [==============================] - 0s 914us/step - loss: 122141144.0000 - mean_absolute_error: 8998.8682
95/95 [==============================] - 0s 932us/step - loss: 122141144.0000 - mean_absolute_error: 8998.8682
Epoch 46/100
95/95 [==============================] - 0s 940us/step - loss: 121176808.0000 - mean_absolute_error: 9021.3682
95/95 [==============================] - 0s 958us/step - loss: 121176808.0000 - mean_absolute_error: 9021.3682
Epoch 47/100
95/95 [==============================] - 0s 902us/step - loss: 120454504.0000 - mean_absolute_error: 9038.6035
95/95 [==============================] - 0s 920us/step - loss: 120454504.0000 - mean_absolute_error: 9038.6035
Epoch 48/100
95/95 [==============================] - 0s 905us/step - loss: 119858992.0000 - mean_absolute_error: 9050.0801
95/95 [==============================] - 0s 924us/step - loss: 119858992.0000 - mean_absolute_error: 9050.0801
Epoch 49/100
95/95 [==============================] - 0s 904us/step - loss: 119334040.0000 - mean_absolute_error: 9055.8574
95/95 [==============================] - 0s 922us/step - loss: 119334040.0000 - mean_absolute_error: 9055.8574
Epoch 50/100
95/95 [==============================] - 0s 989us/step - loss: 118861768.0000 - mean_absolute_error: 9057.0996
95/95 [==============================] - 0s 1ms/step - loss: 118861768.0000 - mean_absolute_error: 9057.0996  
Epoch 51/100
95/95 [==============================] - 0s 964us/step - loss: 118436896.0000 - mean_absolute_error: 9054.8486
95/95 [==============================] - 0s 982us/step - loss: 118436896.0000 - mean_absolute_error: 9054.8486
Epoch 52/100
95/95 [==============================] - 0s 952us/step - loss: 118053016.0000 - mean_absolute_error: 9050.0723
95/95 [==============================] - 0s 970us/step - loss: 118053016.0000 - mean_absolute_error: 9050.0723
Epoch 53/100
95/95 [==============================] - 0s 903us/step - loss: 117701488.0000 - mean_absolute_error: 9043.5703
95/95 [==============================] - 0s 921us/step - loss: 117701488.0000 - mean_absolute_error: 9043.5703
Epoch 54/100
95/95 [==============================] - 0s 909us/step - loss: 117374024.0000 - mean_absolute_error: 9035.7354
95/95 [==============================] - 0s 927us/step - loss: 117374024.0000 - mean_absolute_error: 9035.7354
Epoch 55/100
95/95 [==============================] - 0s 923us/step - loss: 117063960.0000 - mean_absolute_error: 9026.8330
95/95 [==============================] - 0s 942us/step - loss: 117063960.0000 - mean_absolute_error: 9026.8330
Epoch 56/100
95/95 [==============================] - 0s 961us/step - loss: 116765608.0000 - mean_absolute_error: 9017.0479
95/95 [==============================] - 0s 979us/step - loss: 116765608.0000 - mean_absolute_error: 9017.0479
Epoch 57/100
95/95 [==============================] - 0s 926us/step - loss: 116474960.0000 - mean_absolute_error: 9006.4990
95/95 [==============================] - 0s 943us/step - loss: 116474960.0000 - mean_absolute_error: 9006.4990
Epoch 58/100
95/95 [==============================] - 0s 904us/step - loss: 116189048.0000 - mean_absolute_error: 8995.3047
95/95 [==============================] - 0s 923us/step - loss: 116189048.0000 - mean_absolute_error: 8995.3047
Epoch 59/100
95/95 [==============================] - 0s 905us/step - loss: 115906040.0000 - mean_absolute_error: 8983.5762
95/95 [==============================] - 0s 923us/step - loss: 115906040.0000 - mean_absolute_error: 8983.5762
Epoch 60/100
95/95 [==============================] - 0s 907us/step - loss: 115624000.0000 - mean_absolute_error: 8971.3496
95/95 [==============================] - 0s 925us/step - loss: 115624000.0000 - mean_absolute_error: 8971.3496
Epoch 61/100
95/95 [==============================] - 0s 909us/step - loss: 115341976.0000 - mean_absolute_error: 8958.6748
95/95 [==============================] - 0s 928us/step - loss: 115341976.0000 - mean_absolute_error: 8958.6748
Epoch 62/100
95/95 [==============================] - 0s 899us/step - loss: 115059192.0000 - mean_absolute_error: 8945.5869
95/95 [==============================] - 0s 917us/step - loss: 115059192.0000 - mean_absolute_error: 8945.5869
Epoch 63/100
95/95 [==============================] - 0s 979us/step - loss: 114775264.0000 - mean_absolute_error: 8932.1318
95/95 [==============================] - 0s 997us/step - loss: 114775264.0000 - mean_absolute_error: 8932.1318
Epoch 64/100
95/95 [==============================] - 0s 905us/step - loss: 114490040.0000 - mean_absolute_error: 8918.3447
95/95 [==============================] - 0s 923us/step - loss: 114490040.0000 - mean_absolute_error: 8918.3447
Epoch 65/100
95/95 [==============================] - 0s 925us/step - loss: 114203696.0000 - mean_absolute_error: 8904.2598
95/95 [==============================] - 0s 943us/step - loss: 114203696.0000 - mean_absolute_error: 8904.2598
Epoch 66/100
95/95 [==============================] - 0s 902us/step - loss: 113915936.0000 - mean_absolute_error: 8889.8418
95/95 [==============================] - 0s 920us/step - loss: 113915936.0000 - mean_absolute_error: 8889.8418
Epoch 67/100
95/95 [==============================] - 0s 908us/step - loss: 113627336.0000 - mean_absolute_error: 8875.1338
95/95 [==============================] - 0s 969us/step - loss: 113627336.0000 - mean_absolute_error: 8875.1338
Epoch 68/100
95/95 [==============================] - 0s 920us/step - loss: 113338048.0000 - mean_absolute_error: 8860.1865
95/95 [==============================] - 0s 937us/step - loss: 113338048.0000 - mean_absolute_error: 8860.1865
Epoch 69/100
95/95 [==============================] - 0s 977us/step - loss: 113048624.0000 - mean_absolute_error: 8844.9717
95/95 [==============================] - 0s 995us/step - loss: 113048624.0000 - mean_absolute_error: 8844.9717
Epoch 70/100
95/95 [==============================] - 0s 922us/step - loss: 112759912.0000 - mean_absolute_error: 8829.5947
95/95 [==============================] - 0s 943us/step - loss: 112759912.0000 - mean_absolute_error: 8829.5947
Epoch 71/100
95/95 [==============================] - 0s 908us/step - loss: 112472072.0000 - mean_absolute_error: 8814.0273
95/95 [==============================] - 0s 926us/step - loss: 112472072.0000 - mean_absolute_error: 8814.0273
Epoch 72/100
95/95 [==============================] - 0s 904us/step - loss: 112185968.0000 - mean_absolute_error: 8798.2783
95/95 [==============================] - 0s 925us/step - loss: 112185968.0000 - mean_absolute_error: 8798.2783
Epoch 73/100
95/95 [==============================] - 0s 900us/step - loss: 111902192.0000 - mean_absolute_error: 8782.4111
95/95 [==============================] - 0s 918us/step - loss: 111902192.0000 - mean_absolute_error: 8782.4111
Epoch 74/100
95/95 [==============================] - 0s 966us/step - loss: 111621128.0000 - mean_absolute_error: 8766.4531
95/95 [==============================] - 0s 984us/step - loss: 111621128.0000 - mean_absolute_error: 8766.4531
Epoch 75/100
95/95 [==============================] - 0s 935us/step - loss: 111343704.0000 - mean_absolute_error: 8750.4258
95/95 [==============================] - 0s 953us/step - loss: 111343704.0000 - mean_absolute_error: 8750.4258
Epoch 76/100
95/95 [==============================] - 0s 950us/step - loss: 111070872.0000 - mean_absolute_error: 8734.3701
95/95 [==============================] - 0s 971us/step - loss: 111070872.0000 - mean_absolute_error: 8734.3701
Epoch 77/100
95/95 [==============================] - 0s 902us/step - loss: 110802624.0000 - mean_absolute_error: 8718.2705
95/95 [==============================] - 0s 921us/step - loss: 110802624.0000 - mean_absolute_error: 8718.2705
Epoch 78/100
95/95 [==============================] - 0s 905us/step - loss: 110540032.0000 - mean_absolute_error: 8702.2354
95/95 [==============================] - 0s 923us/step - loss: 110540032.0000 - mean_absolute_error: 8702.2354
Epoch 79/100
95/95 [==============================] - 0s 912us/step - loss: 110281960.0000 - mean_absolute_error: 8686.1572
95/95 [==============================] - 0s 930us/step - loss: 110281960.0000 - mean_absolute_error: 8686.1572
Epoch 80/100
95/95 [==============================] - 0s 902us/step - loss: 110030856.0000 - mean_absolute_error: 8670.2002
95/95 [==============================] - 0s 922us/step - loss: 110030856.0000 - mean_absolute_error: 8670.2002
Epoch 81/100
95/95 [==============================] - 0s 907us/step - loss: 109787008.0000 - mean_absolute_error: 8654.3994
95/95 [==============================] - 0s 926us/step - loss: 109787008.0000 - mean_absolute_error: 8654.3994
Epoch 82/100
95/95 [==============================] - 0s 931us/step - loss: 109550176.0000 - mean_absolute_error: 8638.8037
95/95 [==============================] - 0s 950us/step - loss: 109550176.0000 - mean_absolute_error: 8638.8037
Epoch 83/100
95/95 [==============================] - 0s 931us/step - loss: 109319232.0000 - mean_absolute_error: 8623.2979
95/95 [==============================] - 0s 951us/step - loss: 109319232.0000 - mean_absolute_error: 8623.2979
Epoch 84/100
95/95 [==============================] - 0s 971us/step - loss: 109094760.0000 - mean_absolute_error: 8607.9551
95/95 [==============================] - 0s 989us/step - loss: 109094760.0000 - mean_absolute_error: 8607.9551
Epoch 85/100
95/95 [==============================] - 0s 963us/step - loss: 108877224.0000 - mean_absolute_error: 8592.8232
95/95 [==============================] - 0s 981us/step - loss: 108877224.0000 - mean_absolute_error: 8592.8232
Epoch 86/100
95/95 [==============================] - 0s 909us/step - loss: 108668416.0000 - mean_absolute_error: 8578.0713
95/95 [==============================] - 0s 929us/step - loss: 108668416.0000 - mean_absolute_error: 8578.0713
Epoch 87/100
95/95 [==============================] - 0s 907us/step - loss: 108469560.0000 - mean_absolute_error: 8563.8516
95/95 [==============================] - 0s 924us/step - loss: 108469560.0000 - mean_absolute_error: 8563.8516
Epoch 88/100
95/95 [==============================] - 0s 905us/step - loss: 108280728.0000 - mean_absolute_error: 8550.2051
95/95 [==============================] - 0s 924us/step - loss: 108280728.0000 - mean_absolute_error: 8550.2051
Epoch 89/100
95/95 [==============================] - 0s 946us/step - loss: 108101560.0000 - mean_absolute_error: 8537.1143
95/95 [==============================] - 0s 964us/step - loss: 108101560.0000 - mean_absolute_error: 8537.1143
Epoch 90/100
95/95 [==============================] - 0s 905us/step - loss: 107932552.0000 - mean_absolute_error: 8524.6299
95/95 [==============================] - 0s 923us/step - loss: 107932552.0000 - mean_absolute_error: 8524.6299
Epoch 91/100
95/95 [==============================] - 0s 897us/step - loss: 107773656.0000 - mean_absolute_error: 8512.7910
95/95 [==============================] - 0s 915us/step - loss: 107773656.0000 - mean_absolute_error: 8512.7910
Epoch 92/100
95/95 [==============================] - 0s 903us/step - loss: 107624312.0000 - mean_absolute_error: 8501.5684
95/95 [==============================] - 0s 921us/step - loss: 107624312.0000 - mean_absolute_error: 8501.5684
Epoch 93/100
95/95 [==============================] - 0s 924us/step - loss: 107484848.0000 - mean_absolute_error: 8490.9502
95/95 [==============================] - 0s 943us/step - loss: 107484848.0000 - mean_absolute_error: 8490.9502
Epoch 94/100
95/95 [==============================] - 0s 921us/step - loss: 107354296.0000 - mean_absolute_error: 8480.9209
95/95 [==============================] - 0s 942us/step - loss: 107354296.0000 - mean_absolute_error: 8480.9209
Epoch 95/100
95/95 [==============================] - 0s 934us/step - loss: 107231552.0000 - mean_absolute_error: 8471.3105
95/95 [==============================] - 0s 953us/step - loss: 107231552.0000 - mean_absolute_error: 8471.3105
Epoch 96/100
95/95 [==============================] - 0s 981us/step - loss: 107118168.0000 - mean_absolute_error: 8462.4668
95/95 [==============================] - 0s 999us/step - loss: 107118168.0000 - mean_absolute_error: 8462.4668
Epoch 97/100
95/95 [==============================] - 0s 906us/step - loss: 107013384.0000 - mean_absolute_error: 8454.3633
95/95 [==============================] - 0s 924us/step - loss: 107013384.0000 - mean_absolute_error: 8454.3633
Epoch 98/100
95/95 [==============================] - 0s 907us/step - loss: 106916624.0000 - mean_absolute_error: 8446.9023
95/95 [==============================] - 0s 925us/step - loss: 106916624.0000 - mean_absolute_error: 8446.9023
Epoch 99/100
95/95 [==============================] - 0s 902us/step - loss: 106827520.0000 - mean_absolute_error: 8440.0771
95/95 [==============================] - 0s 921us/step - loss: 106827520.0000 - mean_absolute_error: 8440.0771
Epoch 100/100
95/95 [==============================] - 0s 957us/step - loss: 106745296.0000 - mean_absolute_error: 8433.7939
95/95 [==============================] - 0s 974us/step - loss: 106745296.0000 - mean_absolute_error: 8433.7939
[1] "step4"
[1] 4675
[1] "step5"
[1] "Score final : 11160.8858358056"
> > + + > > > > > 
── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character()
)
ℹ Use `spec()` for the full column specifications.

> 
── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character(),
  Usage = col_character()
)
ℹ Use `spec()` for the full column specifications.

> > > > . + > > > > > > > > > [1] 3303
> > > > + > Call:
lm(formula = Load ~ Load.1 + Load.7 + Temp + Temp_s95 + WeekDays + 
    GovernmentResponseIndex, data = train)

Residuals:
     Min       1Q   Median       3Q      Max 
-10526.2  -1304.4    147.1   1369.5   6430.4 

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
(Intercept)              1.957e+04  5.333e+02  36.687  < 2e-16 ***
Load.1                   7.261e-01  9.519e-03  76.273  < 2e-16 ***
Load.7                   8.780e-02  8.258e-03  10.633  < 2e-16 ***
Temp                    -6.524e+02  6.980e+01  -9.346  < 2e-16 ***
Temp_s95                 3.229e+02  7.229e+01   4.467 8.24e-06 ***
WeekDays                -1.309e+03  2.290e+01 -57.167  < 2e-16 ***
GovernmentResponseIndex -3.480e+01  5.039e+00  -6.906 6.05e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2134 on 3021 degrees of freedom
Multiple R-squared:  0.9626,	Adjusted R-squared:  0.9626 
F-statistic: 1.297e+04 on 6 and 3021 DF,  p-value: < 2.2e-16

> 
> gam.test = gam(train, test)
Error in gf[[i]][[2]] <- NULL : 
  l'argument de remplacement est de longueur nulle
De plus : Warning message:
In interpret.gam(formula) : single linear predictor indices are ignored
> gam.test = gam(train, test)
Error in gf[[i]][[2]] <- NULL : 
  l'argument de remplacement est de longueur nulle
De plus : Warning message:
In interpret.gam(formula) : single linear predictor indices are ignored
> gam.test = gam_rte(train, test)
tmp = format_data("./data/train_V2.csv", "./data/test_V2.csv")
> [1] "Load and format  ./data/train_V2.csv"
[1] "Load and format  ./data/test_V2.csv"
> test_label = tmp$test_label
> Gam.rmse = evaluate(test_label, gam.test)
> cv = cross_validation(train, test)
[1] 275
[1] 1398.02
> model = cv$model; pred.test = cv$pred.test; RMSE = cv$RMSE
> if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(train$time,predict(model,train), col='red', lwd=1)
+     lines(test$time,pred.test, col='green', lwd=1)
+ }
> Load = pred.test
> Id = 1:length(Load)
> submission = data.frame(Load, Id)
> experts.train  = predict(model,train)
> experts.test = pred.test
> experts.train, experts.test = add_expert(pred.lstm$train, pred.lstm$test, experts.train, experts.test)
Erreur : ',' inattendu(e) in "experts.train,"
> add_expert = function(new_train, new_test, experts.train, experts.test){
+     experts.train = array_reshape(abind(experts.train, new_train, along=2), c(3028,1,length(experts.train)+1))
+     experts.test = cbind(experts.test, new_test)
+     return(experts.train, experts.test)
+ }
> experts.train  = predict(model,train)
> experts.test = pred.test
> experts.train, experts.test = add_expert(pred.lstm$train, pred.lstm$test, experts.train, experts.test)
Erreur : ',' inattendu(e) in "experts.train,"
> (experts.train, experts.test) = add_expert(pred.lstm$train, pred.lstm$test, experts.train, experts.test)
Erreur : ',' inattendu(e) in "(experts.train,"
> res = add_expert(pred.lstm$train, pred.lstm$test, experts.train, experts.test)
Error in abind(experts.train, new_train, along = 2) : 
  objet 'pred.lstm' introuvable
> experts.train = res$experts.train
Erreur : objet 'res' introuvable
> res = add_expert(pred.lstm$train, pred.lstm$test, experts.train, experts.test)
Error in abind(experts.train, new_train, along = 2) : 
  objet 'pred.lstm' introuvable
> pred.lstm = neural_network(train, test)
Epoch 1/15
95/95 [==============================] - 0s 723us/step - loss: 982428800.0000
95/95 [==============================] - 0s 740us/step - loss: 982428800.0000
Epoch 2/15
95/95 [==============================] - 0s 691us/step - loss: 46140488.0000
95/95 [==============================] - 0s 708us/step - loss: 46140488.0000
Epoch 3/15
95/95 [==============================] - 0s 737us/step - loss: 11518197.0000
95/95 [==============================] - 0s 754us/step - loss: 11518197.0000
Epoch 4/15
95/95 [==============================] - 0s 700us/step - loss: 11467888.0000
95/95 [==============================] - 0s 717us/step - loss: 11467888.0000
Epoch 5/15
95/95 [==============================] - 0s 687us/step - loss: 11444440.0000
95/95 [==============================] - 0s 704us/step - loss: 11444440.0000
Epoch 6/15
95/95 [==============================] - 0s 685us/step - loss: 11393203.0000
95/95 [==============================] - 0s 703us/step - loss: 11393203.0000
Epoch 7/15
95/95 [==============================] - 0s 689us/step - loss: 11352122.0000
95/95 [==============================] - 0s 706us/step - loss: 11352122.0000
Epoch 8/15
95/95 [==============================] - 0s 698us/step - loss: 11327307.0000
95/95 [==============================] - 0s 716us/step - loss: 11327307.0000
Epoch 9/15
95/95 [==============================] - 0s 723us/step - loss: 11294969.0000
95/95 [==============================] - 0s 745us/step - loss: 11294969.0000
Epoch 10/15
95/95 [==============================] - 0s 709us/step - loss: 11261490.0000
95/95 [==============================] - 0s 726us/step - loss: 11261490.0000
Epoch 11/15
95/95 [==============================] - 0s 751us/step - loss: 11194477.0000
95/95 [==============================] - 0s 769us/step - loss: 11194477.0000
Epoch 12/15
95/95 [==============================] - 0s 694us/step - loss: 11158209.0000
95/95 [==============================] - 0s 711us/step - loss: 11158209.0000
Epoch 13/15
95/95 [==============================] - 0s 690us/step - loss: 11124139.0000
95/95 [==============================] - 0s 707us/step - loss: 11124139.0000
Epoch 14/15
95/95 [==============================] - 0s 679us/step - loss: 11090318.0000
95/95 [==============================] - 0s 696us/step - loss: 11090318.0000
Epoch 15/15
95/95 [==============================] - 0s 684us/step - loss: 11088033.0000
95/95 [==============================] - 0s 700us/step - loss: 11088033.0000
> add_expert = function(new_train, new_test, experts.train, experts.test){
+     experts.train = array_reshape(abind(experts.train, new_train, along=2), c(3028,1,length(experts.train)+1))
+     experts.test = cbind(experts.test, new_test)
+     return(list("experts.train" = experts.train, "experts.test" = experts.test))
+ }
> experts.train  = predict(model,train)
> experts.test = pred.test
> res = add_expert(pred.lstm$train, pred.lstm$test, experts.train, experts.test)
Error in py_call_impl(callable, dots$args, dots$keywords) : 
  ValueError: cannot reshape array of size 6056 into shape (3028,1,3029)

Detailed traceback: 
  File "<__array_function__ internals>", line 6, in reshape
  File "/home/nicolas/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6/site-packages/numpy/core/fromnumeric.py", line 299, in reshape
    return _wrapfunc(a, 'reshape', newshape, order=order)
  File "/home/nicolas/.local/share/r-miniconda/envs/r-reticulate/lib/python3.6/site-packages/numpy/core/fromnumeric.py", line 58, in _wrapfunc
    return bound(*args, **kwds)
> add_expert = function(new_train, new_test, experts.train, experts.test){
+     experts.train = array_reshape(abind(experts.train, new_train, along=2), c(3028,1,length(experts.train[,3])+1))
+     experts.test = cbind(experts.test, new_test)
+     return(list("experts.train" = experts.train, "experts.test" = experts.test))
+ }
> experts.train  = predict(model,train)
> experts.test = pred.test
> res = add_expert(pred.lstm$train, pred.lstm$test, experts.train, experts.test)
Error in experts.train[, 3] : nombre de dimensions incorrect
> experts.train
       1        2        3        4        5        6        7        8 
48411.71 59681.34 62376.92 66839.19 65904.65 68431.83 62691.44 59030.71 
       9       10       11       12       13       14       15       16 
65195.32 68270.43 69265.90 70673.38 71739.20 69247.39 68473.49 76602.69 
      17       18       19       20       21       22       23       24 
78407.85 76490.51 73105.90 69081.57 61725.91 58018.28 64627.35 67752.40 
      25       26       27       28       29       30       31       32 
68274.14 68321.35 69074.49 66094.16 65670.96 75040.27 80180.89 82793.78 
      33       34       35       36       37       38       39       40 
86782.17 89339.39 85395.05 83449.09 89840.11 91549.60 92820.15 92907.29 
      41       42       43       44       45       46       47       48 
91816.30 87674.00 85572.08 88064.60 83602.34 78812.49 76153.15 73399.09 
      49       50       51       52       53       54       55       56 
67928.97 65429.99 73723.87 75968.03 74934.09 71361.28 67956.27 60626.44 
      57       58       59       60       61       62       63       64 
57400.41 64819.58 66985.20 65174.71 63166.49 61692.66 56162.79 54056.94 
      65       66       67       68       69       70       71       72 
64046.69 69286.45 69095.17 68969.35 66713.62 60262.70 55075.64 59993.87 
      73       74       75       76       77       78       79       80 
60809.29 59814.80 58349.56 57497.94 52247.79 52164.34 60775.69 63510.36 
      81       82       83       84       85       86       87       88 
61796.63 59652.04 56584.75 49507.65 45645.57 51517.01 53034.01 52798.74 
      89       90       91       92       93       94       95       96 
52158.66 51453.33 46967.83 45825.63 52545.80 55572.50 56596.60 56444.45 
      97       98       99      100      101      102      103      104 
56730.04 51584.35 49511.14 55248.76 53263.20 58995.28 59581.36 58836.52 
     105      106      107      108      109      110      111      112 
53855.72 51553.85 59823.16 62914.02 62671.67 61750.97 59847.43 53999.34 
     113      114      115      116      117      118      119      120 
50985.31 57870.83 60821.16 59266.45 58462.32 55863.18 49170.97 46632.69 
     121      122      123      124      125      126      127      128 
53106.87 51885.15 47090.84 51767.75 51654.51 46902.54 44243.70 49887.05 
     129      130      131      132      133      134      135      136 
48819.35 45729.68 49875.82 48574.97 43688.03 41645.11 47732.47 51028.33 
     137      138      139      140      141      142      143      144 
52113.31 51046.59 45712.27 42136.94 41748.02 49355.46 51782.93 51358.29 
     145      146      147      148      149      150      151      152 
49462.96 48343.94 42340.83 39036.20 45160.69 43122.57 47220.27 47855.08 
     153      154      155      156      157      158      159      160 
47427.47 42235.23 38494.75 44818.73 47628.50 48067.69 47887.65 47286.34 
     161      162      163      164      165      166      167      168 
42119.42 38972.77 45926.86 48400.98 48738.69 48446.91 47399.37 42015.60 
     169      170      171      172      173      174      175      176 
39199.55 45099.90 47742.87 48406.92 47803.86 47334.33 41814.40 38614.78 
     177      178      179      180      181      182      183      184 
45411.45 48439.05 49585.51 49256.97 48251.92 42436.54 39155.75 45569.14 
     185      186      187      188      189      190      191      192 
48091.82 48361.31 48233.57 47702.31 42210.88 38965.58 45313.50 47699.90 
     193      194      195      196      197      198      199      200 
48058.95 48014.60 47578.52 42353.58 38130.98 45418.98 47970.57 48616.33 
     201      202      203      204      205      206      207      208 
48190.03 47445.67 42087.86 39017.56 45480.05 48071.71 48953.94 49319.85 
     209      210      211      212      213      214      215      216 
48246.70 41883.46 38498.67 44731.07 46300.17 46469.90 46139.72 45270.43 
     217      218      219      220      221      222      223      224 
39860.49 37310.48 43583.22 43691.90 43737.60 44135.44 43844.06 38902.50 
     225      226      227      228      229      230      231      232 
37205.63 43748.31 44381.20 43853.84 40929.51 43832.42 40189.28 39164.54 
     233      234      235      236      237      238      239      240 
45325.69 46861.23 46155.72 46146.46 45448.53 40524.54 38073.61 44560.25 
     241      242      243      244      245      246      247      248 
46468.78 47197.52 47076.28 46769.22 41569.77 38441.39 45023.64 47658.84 
     249      250      251      252      253      254      255      256 
47976.51 47956.83 47444.76 42431.03 39264.51 45489.12 47819.84 48438.85 
     257      258      259      260      261      262      263      264 
48272.51 47347.77 42084.27 38818.73 45259.70 47659.33 48616.46 48442.41 
     265      266      267      268      269      270      271      272 
47907.98 43050.76 40078.41 45968.36 48758.10 49813.40 50108.30 49251.80 
     273      274      275      276      277      278      279      280 
44198.99 41562.32 47813.85 50153.36 49995.86 50088.07 49346.49 43607.09 
     281      282      283      284      285      286      287      288 
40684.49 47461.92 50251.15 50022.91 50251.12 50552.32 46090.33 44588.32 
     289      290      291      292      293      294      295      296 
52674.25 54389.90 53810.55 53076.98 51285.44 45732.67 42603.84 48878.73 
     297      298      299      300      301      302      303      304 
51076.52 51630.92 52292.98 52909.90 52950.87 54074.44 61969.74 63605.58 
     305      306      307      308      309      310      311      312 
62677.10 61389.63 54487.74 51548.70 50412.96 58399.84 62450.53 63708.34 
     313      314      315      316      317      318      319      320 
62957.23 61656.46 55413.24 52878.99 60870.00 64686.95 64601.80 65093.29 
     321      322      323      324      325      326      327      328 
64022.69 57411.73 53795.58 60164.03 63199.10 63243.22 64421.86 63148.49 
     329      330      331      332      333      334      335      336 
56146.22 52292.05 58544.77 63855.58 66236.52 69531.07 72307.93 68779.62 
     337      338      339      340      341      342      343      344 
65399.94 69460.34 71580.09 72347.70 75796.64 75016.97 71422.11 68085.79 
     345      346      347      348      349      350      351      352 
74425.87 78394.78 81082.95 78247.84 73372.08 64694.17 59054.62 65636.60 
     353      354      355      356      357      358      359      360 
68108.51 68796.27 68031.39 67435.97 59072.28 55234.68 59878.81 57761.41 
     361      362      363      364      365      366      367      368 
54279.28 58404.88 59370.82 54695.58 53944.79 61685.75 60869.63 59170.63 
     369      370      371      372      373      374      375      376 
64767.65 64946.23 60242.25 58594.24 66558.97 69889.05 72219.21 70946.09 
     377      378      379      380      381      382      383      384 
70303.35 66065.15 65172.98 74186.50 77834.67 81057.29 82877.64 83162.44 
     385      386      387      388      389      390      391      392 
76583.72 73587.51 78126.22 78489.71 78516.88 78993.79 80248.49 73940.88 
     393      394      395      396      397      398      399      400 
69129.98 72349.93 70205.05 67572.02 65447.63 65140.31 63958.33 63976.86 
     401      402      403      404      405      406      407      408 
69460.74 70983.40 72779.44 75550.37 76853.88 71439.46 69497.29 76102.53 
     409      410      411      412      413      414      415      416 
76525.73 77761.31 75376.01 74056.49 68177.50 65840.88 71973.27 73125.87 
     417      418      419      420      421      422      423      424 
73779.80 76134.56 79846.57 77145.85 76208.65 81515.40 80816.84 78548.86 
     425      426      427      428      429      430      431      432 
77285.36 75495.23 71748.24 67745.05 69700.70 67283.49 65040.20 62419.82 
     433      434      435      436      437      438      439      440 
60872.88 55936.10 53788.26 62860.32 71004.46 75718.22 76925.50 74739.10 
     441      442      443      444      445      446      447      448 
67503.46 63412.94 69417.44 70447.83 69507.70 67860.72 65104.67 58829.76 
     449      450      451      452      453      454      455      456 
56286.62 64552.00 68205.10 68638.72 69138.58 67696.02 62965.71 61439.23 
     457      458      459      460      461      462      463      464 
67142.89 63462.97 67139.02 67463.79 68713.32 64330.81 60688.23 64479.55 
     465      466      467      468      469      470      471      472 
64331.88 61957.56 60425.25 59052.48 52039.08 47527.53 51054.30 51577.43 
     473      474      475      476      477      478      479      480 
51274.31 51017.22 52468.42 49442.31 47389.70 53166.09 53887.35 52172.84 
     481      482      483      484      485      486      487      488 
50515.63 50528.87 49687.17 48833.88 55440.81 56392.84 55255.82 48210.98 
     489      490      491      492      493      494      495      496 
51744.01 46730.11 43313.68 48622.72 49615.40 48064.01 44475.91 44015.51 
     497      498      499      500      501      502      503      504 
41877.25 41965.02 48613.06 50969.20 51979.29 52612.55 52213.03 47324.40 
     505      506      507      508      509      510      511      512 
44957.71 51435.35 49489.57 53724.14 55200.19 55973.09 51673.90 47002.56 
     513      514      515      516      517      518      519      520 
51078.44 53787.57 53932.73 53777.68 53023.83 46886.27 42625.27 48073.59 
     521      522      523      524      525      526      527      528 
49810.77 49242.86 48794.63 47702.77 41948.43 39479.73 45835.49 47969.14 
     529      530      531      532      533      534      535      536 
48094.62 47513.40 47240.28 41775.35 39038.74 45326.41 48030.10 48159.40 
     537      538      539      540      541      542      543      544 
47990.23 47358.71 41878.04 38891.42 45957.59 48068.59 48030.59 47806.73 
     545      546      547      548      549      550      551      552 
47284.27 42108.98 39241.06 45367.87 47614.53 47724.34 48077.68 47615.87 
     553      554      555      556      557      558      559      560 
42724.90 39917.75 46063.12 48817.33 49042.29 48732.61 48076.71 42874.30 
     561      562      563      564      565      566      567      568 
40057.22 46178.10 48875.44 49191.85 48977.19 48496.46 43593.61 40945.23 
     569      570      571      572      573      574      575      576 
47276.09 49336.20 49613.21 49601.60 49126.98 43438.54 39495.07 45298.07 
     577      578      579      580      581      582      583      584 
46471.20 47065.42 47842.96 47095.40 41013.62 38318.06 44799.48 45079.07 
     585      586      587      588      589      590      591      592 
44339.77 43852.41 42806.50 37759.10 36344.10 43017.68 43209.83 43015.83 
     593      594      595      596      597      598      599      600 
43051.16 39695.90 37187.23 36872.01 43302.80 44264.38 44329.08 44305.13 
     601      602      603      604      605      606      607      608 
44238.20 39261.18 37524.43 43953.33 45579.47 46359.73 46451.97 46191.23 
     609      610      611      612      613      614      615      616 
41209.36 38294.87 44707.57 47279.72 48229.55 48432.37 47241.49 41993.12 
     617      618      619      620      621      622      623      624 
38820.02 45222.84 47508.66 47845.73 48285.05 47843.77 42512.91 39834.07 
     625      626      627      628      629      630      631      632 
46594.78 48791.10 49376.63 49384.73 48434.76 42806.14 39555.39 45816.83 
     633      634      635      636      637      638      639      640 
48040.29 48282.62 48288.70 47664.24 42145.16 39076.11 45787.91 48020.86 
     641      642      643      644      645      646      647      648 
48501.11 48515.06 47712.92 42624.59 39805.58 46485.44 48767.43 49146.09 
     649      650      651      652      653      654      655      656 
51556.85 52940.53 49320.03 45876.64 51787.26 53376.91 52904.79 53107.04 
     657      658      659      660      661      662      663      664 
51754.50 46146.36 42686.03 48572.76 50295.92 49930.26 50028.95 49541.53 
     665      666      667      668      669      670      671      672 
44139.60 40747.81 48025.13 52484.21 54437.63 55578.82 54429.24 44195.25 
     673      674      675      676      677      678      679      680 
45517.49 53700.45 56700.81 57330.77 55513.46 54283.01 50930.79 49631.34 
     681      682      683      684      685      686      687      688 
58104.15 55940.01 61284.23 62671.04 65014.34 61606.70 58842.68 64798.32 
     689      690      691      692      693      694      695      696 
68467.40 70902.01 72968.06 73010.30 67908.79 63646.23 70145.42 72578.65 
     697      698      699      700      701      702      703      704 
74468.82 75259.83 74010.37 67654.81 64839.20 70986.99 74336.79 73891.40 
     705      706      707      708      709      710      711      712 
74212.27 72123.55 67455.72 65423.88 72991.01 76309.09 75643.31 75357.87 
     713      714      715      716      717      718      719      720 
74234.77 67693.05 63623.76 68937.52 69819.25 68019.26 67398.96 68099.75 
     721      722      723      724      725      726      727      728 
63267.36 59479.78 63967.31 61536.02 61150.29 57860.06 60435.60 57842.87 
     729      730      731      732      733      734      735      736 
58190.01 66563.79 66626.13 62620.03 57632.98 59570.67 56641.10 56713.83 
     737      738      739      740      741      742      743      744 
62372.52 63158.29 61982.59 62322.03 64243.49 60305.59 57688.86 65339.40 
     745      746      747      748      749      750      751      752 
68913.69 68662.01 68516.78 67369.13 61626.44 60123.45 67419.71 71606.46 
     753      754      755      756      757      758      759      760 
72616.22 71236.70 70283.76 64150.06 59964.25 68745.42 71956.89 73268.19 
     761      762      763      764      765      766      767      768 
73237.04 71605.89 66581.51 63852.28 70264.53 70658.59 69302.46 68780.99 
     769      770      771      772      773      774      775      776 
67873.75 62307.15 61126.23 68121.18 69765.14 69654.37 69527.25 66944.76 
     777      778      779      780      781      782      783      784 
61815.68 58397.58 65107.65 66204.93 66301.15 64774.59 65552.29 60400.67 
     785      786      787      788      789      790      791      792 
57247.11 61424.60 63925.44 66353.79 66344.41 68233.86 64011.66 60220.05 
     793      794      795      796      797      798      799      800 
66613.28 67960.06 66822.71 65041.96 63609.05 56217.13 50883.31 56546.84 
     801      802      803      804      805      806      807      808 
58906.56 58987.09 58896.39 57685.71 52257.17 48861.99 54977.42 56948.18 
     809      810      811      812      813      814      815      816 
57545.05 55764.40 55412.22 52254.04 52952.43 61338.89 63280.99 63739.99 
     817      818      819      820      821      822      823      824 
63644.56 60532.13 52678.92 48017.96 53488.62 54148.41 53048.52 53211.62 
     825      826      827      828      829      830      831      832 
53514.13 47971.63 44147.82 49515.93 52806.27 53007.70 52737.84 51674.53 
     833      834      835      836      837      838      839      840 
46061.74 43075.30 49725.39 52738.22 53085.00 52603.92 52327.55 47825.77 
     841      842      843      844      845      846      847      848 
45989.24 51430.53 47849.04 51388.65 51405.50 51087.59 46572.46 44696.92 
     849      850      851      852      853      854      855      856 
51038.34 52769.09 52584.33 51667.04 45261.75 44708.44 43774.88 49105.41 
     857      858      859      860      861      862      863      864 
50143.65 49726.73 48832.69 44214.54 41519.30 41530.52 49416.30 52094.91 
     865      866      867      868      869      870      871      872 
51619.11 50687.62 49528.49 43924.76 40233.62 46067.64 48038.38 48221.83 
     873      874      875      876      877      878      879      880 
48627.57 48642.49 43401.51 40320.29 47140.75 49364.26 49597.20 49061.79 
     881      882      883      884      885      886      887      888 
43260.25 39989.56 38730.87 45487.06 47683.69 48304.78 47807.42 47500.14 
     889      890      891      892      893      894      895      896 
42247.12 39281.00 45075.74 42853.66 47557.36 47972.16 47513.31 41942.77 
     897      898      899      900      901      902      903      904 
38391.51 44179.34 46953.60 47378.59 47458.47 47016.54 42030.00 39129.81 
     905      906      907      908      909      910      911      912 
45127.64 47826.94 47849.78 47641.34 47158.15 41800.31 38871.04 45146.03 
     913      914      915      916      917      918      919      920 
47199.05 47601.78 48019.84 47255.77 42226.96 38692.45 45083.60 47167.73 
     921      922      923      924      925      926      927      928 
47549.34 47784.26 47479.96 42217.00 38883.92 45162.77 42477.24 47597.73 
     929      930      931      932      933      934      935      936 
49062.75 48674.51 43329.58 39182.61 44737.95 47285.36 48278.36 48398.48 
     937      938      939      940      941      942      943      944 
47492.04 41913.32 38797.25 44513.06 46236.45 46351.84 46314.70 45459.76 
     945      946      947      948      949      950      951      952 
39688.03 37255.18 43419.27 43914.04 43632.95 43817.11 43173.77 38334.58 
     953      954      955      956      957      958      959      960 
36411.45 42736.91 42758.04 42318.95 42351.26 41573.75 34307.51 35366.37 
     961      962      963      964      965      966      967      968 
42473.46 43195.91 43511.17 43631.20 42961.07 38732.47 37141.88 43815.88 
     969      970      971      972      973      974      975      976 
46006.51 46572.83 46519.38 45841.15 40939.07 38121.38 44710.61 47024.24 
     977      978      979      980      981      982      983      984 
47382.62 47436.66 47070.64 42126.45 39232.38 45480.45 47898.38 47953.86 
     985      986      987      988      989      990      991      992 
47835.70 47212.81 41980.52 38896.49 45426.03 48089.03 48197.90 48024.87 
     993      994      995      996      997      998      999     1000 
47395.00 42291.71 38866.14 45484.41 48109.49 48430.76 48860.40 47879.75 
    1001     1002     1003     1004     1005     1006     1007     1008 
42470.90 39258.02 45509.89 48044.92 48278.68 48149.92 47467.58 42314.34 
    1009     1010     1011     1012     1013     1014     1015     1016 
40362.39 46772.00 49396.96 49627.57 49476.99 48663.48 43518.01 40448.66 
    1017     1018     1019     1020     1021     1022     1023     1024 
47752.43 49955.41 49970.24 49924.76 49159.08 43682.61 40177.14 46192.28 
    1025     1026     1027     1028     1029     1030     1031     1032 
48841.10 52391.78 52511.43 51858.90 46399.38 43308.81 49995.04 52353.17 
    1033     1034     1035     1036     1037     1038     1039     1040 
52862.94 52285.60 50915.94 45446.34 41933.84 50167.43 55354.18 57566.09 
    1041     1042     1043     1044     1045     1046     1047     1048 
59391.53 58510.33 53156.16 50012.22 56738.90 55931.16 53562.10 57616.74 
    1049     1050     1051     1052     1053     1054     1055     1056 
57908.04 53726.59 50864.64 58155.78 60885.24 61948.23 61907.86 60241.47 
    1057     1058     1059     1060     1061     1062     1063     1064 
53328.56 48738.13 55450.89 58298.03 59184.45 59087.19 57231.43 52672.60 
    1065     1066     1067     1068     1069     1070     1071     1072 
51444.68 61886.10 67150.85 69386.95 70631.61 69106.74 64917.45 62138.31 
    1073     1074     1075     1076     1077     1078     1079     1080 
69870.46 72517.23 71466.99 69473.02 67320.49 62592.29 60487.00 67216.06 
    1081     1082     1083     1084     1085     1086     1087     1088 
68275.25 65567.30 64387.20 62191.09 57515.36 57089.38 65279.08 66721.15 
    1089     1090     1091     1092     1093     1094     1095     1096 
65501.22 63354.74 59699.28 59663.09 65706.22 76389.45 77031.24 75928.60 
    1097     1098     1099     1100     1101     1102     1103     1104 
73843.71 67597.20 61878.36 60776.16 69305.38 73654.83 74090.67 70801.66 
    1105     1106     1107     1108     1109     1110     1111     1112 
66554.26 59707.90 56692.28 65065.53 67863.91 67816.26 66811.67 68168.38 
    1113     1114     1115     1116     1117     1118     1119     1120 
65358.13 65101.74 73745.28 77212.18 77167.29 77693.90 77800.48 72749.86 
    1121     1122     1123     1124     1125     1126     1127     1128 
70032.92 74443.89 74798.88 72735.83 72849.19 74316.59 70570.75 68422.65 
    1129     1130     1131     1132     1133     1134     1135     1136 
76726.64 79788.24 79854.69 81326.00 81881.89 77476.55 72469.99 77376.64 
    1137     1138     1139     1140     1141     1142     1143     1144 
77366.01 76108.38 74149.40 71305.60 66007.44 62865.53 69474.89 71789.74 
    1145     1146     1147     1148     1149     1150     1151     1152 
72665.51 72233.45 69890.60 66839.45 64075.11 69468.44 71215.26 70085.78 
    1153     1154     1155     1156     1157     1158     1159     1160 
67860.37 68934.67 63551.50 58688.02 63577.68 65249.62 67400.57 68450.21 
    1161     1162     1163     1164     1165     1166     1167     1168 
67671.11 60787.98 56198.78 61123.78 61519.96 61305.07 61358.19 62190.97 
    1169     1170     1171     1172     1173     1174     1175     1176 
59355.22 57798.67 64337.22 63470.55 61023.94 60526.67 61387.14 57646.83 
    1177     1178     1179     1180     1181     1182     1183     1184 
56419.57 62482.75 63891.94 65702.73 65567.69 64294.96 55830.55 51466.51 
    1185     1186     1187     1188     1189     1190     1191     1192 
57318.42 58683.16 60331.63 60896.52 60369.07 54911.58 53592.76 59758.04 
    1193     1194     1195     1196     1197     1198     1199     1200 
55741.54 58015.84 56206.66 53906.38 48452.57 44797.31 50173.50 50792.89 
    1201     1202     1203     1204     1205     1206     1207     1208 
49669.36 49135.86 49247.70 44759.90 43349.87 49045.60 50042.70 49385.94 
    1209     1210     1211     1212     1213     1214     1215     1216 
49262.57 48429.96 43376.63 40961.88 49338.59 52652.98 52618.99 52170.35 
    1217     1218     1219     1220     1221     1222     1223     1224 
51347.07 40585.67 41715.28 48177.00 49264.84 49517.48 49628.94 48194.87 
    1225     1226     1227     1228     1229     1230     1231     1232 
39056.22 39210.16 45874.24 47350.26 47462.96 47833.50 44014.13 40730.06 
    1233     1234     1235     1236     1237     1238     1239     1240 
39987.70 46373.84 49847.15 50716.76 49863.07 49064.38 43474.87 40005.82 
    1241     1242     1243     1244     1245     1246     1247     1248 
45726.66 44030.07 47993.92 48233.62 47736.78 42538.86 39309.16 45448.19 
    1249     1250     1251     1252     1253     1254     1255     1256 
48192.52 48188.37 48789.00 48079.18 42565.93 39263.53 45280.10 47477.11 
    1257     1258     1259     1260     1261     1262     1263     1264 
47740.21 48252.90 47223.28 42030.47 38561.82 45056.23 47213.94 47399.19 
    1265     1266     1267     1268     1269     1270     1271     1272 
47332.90 47013.12 41462.76 38625.66 44922.00 47526.28 47842.74 48012.31 
    1273     1274     1275     1276     1277     1278     1279     1280 
47752.00 42371.50 39827.93 46487.78 49967.85 50328.84 50135.93 50668.12 
    1281     1282     1283     1284     1285     1286     1287     1288 
45128.87 41588.67 47828.90 49844.61 49529.17 48872.88 48711.48 43269.20 
    1289     1290     1291     1292     1293     1294     1295     1296 
40323.39 46535.70 46796.85 44332.08 49596.30 49514.79 44144.07 40799.72 
    1297     1298     1299     1300     1301     1302     1303     1304 
46852.51 49184.81 49560.51 49190.62 48234.00 42211.31 39016.96 45462.27 
    1305     1306     1307     1308     1309     1310     1311     1312 
46809.03 46771.66 46502.51 45510.66 39940.40 38092.17 44967.41 44855.72 
    1313     1314     1315     1316     1317     1318     1319     1320 
45346.68 45248.32 45082.86 39260.00 37319.18 43695.74 43935.04 44728.06 
    1321     1322     1323     1324     1325     1326     1327     1328 
43735.36 42867.28 37605.39 35458.25 42444.35 43126.67 43510.00 43749.61 
    1329     1330     1331     1332     1333     1334     1335     1336 
43892.87 39230.44 36841.97 43524.63 45359.83 45963.79 45962.99 46313.14 
    1337     1338     1339     1340     1341     1342     1343     1344 
41952.16 39623.08 45561.63 47343.95 47317.79 47058.69 46810.21 41811.65 
    1345     1346     1347     1348     1349     1350     1351     1352 
39011.95 45754.56 47745.80 47751.21 47614.84 47083.26 41814.13 39068.43 
    1353     1354     1355     1356     1357     1358     1359     1360 
45783.46 47858.19 48288.29 48445.17 48016.64 42707.18 39979.31 46446.63 
    1361     1362     1363     1364     1365     1366     1367     1368 
48743.50 49631.98 49298.85 48684.73 43321.04 40348.12 47104.02 49445.11 
    1369     1370     1371     1372     1373     1374     1375     1376 
49660.18 50367.70 49918.63 44855.54 41926.41 48124.86 49809.91 49939.90 
    1377     1378     1379     1380     1381     1382     1383     1384 
50558.00 50475.64 45373.65 42832.48 49919.59 53642.48 56906.80 59491.58 
    1385     1386     1387     1388     1389     1390     1391     1392 
60134.35 54331.22 49399.89 55377.06 57826.43 58042.33 57575.77 56398.55 
    1393     1394     1395     1396     1397     1398     1399     1400 
49811.53 46884.72 52310.38 54131.95 54817.86 54598.40 53827.55 48418.96 
    1401     1402     1403     1404     1405     1406     1407     1408 
45644.14 52696.60 55331.44 55184.47 54231.83 52640.80 46722.01 43144.79 
    1409     1410     1411     1412     1413     1414     1415     1416 
50250.54 52934.08 53930.95 50476.21 54175.33 50396.67 47779.68 54151.39 
    1417     1418     1419     1420     1421     1422     1423     1424 
55440.40 54805.57 54141.61 54828.12 54883.23 57530.97 67892.17 69922.03 
    1425     1426     1427     1428     1429     1430     1431     1432 
68418.61 67083.81 67327.89 62525.44 57398.23 63109.31 63843.74 64458.81 
    1433     1434     1435     1436     1437     1438     1439     1440 
65063.60 64660.92 59952.54 55396.86 60686.56 61781.13 64376.56 67171.73 
    1441     1442     1443     1444     1445     1446     1447     1448 
67092.11 61843.63 58358.74 65149.88 65075.49 62921.16 61398.20 59772.32 
    1449     1450     1451     1452     1453     1454     1455     1456 
53760.27 50911.69 58196.14 59214.02 58994.19 57679.40 54873.68 46300.67 
    1457     1458     1459     1460     1461     1462     1463     1464 
49222.65 57515.98 59050.40 58446.15 59060.91 57720.93 50484.50 54112.97 
    1465     1466     1467     1468     1469     1470     1471     1472 
62677.74 65536.37 66910.59 65682.14 65278.25 59555.30 56784.58 63989.61 
    1473     1474     1475     1476     1477     1478     1479     1480 
67987.76 70586.39 71895.15 73543.71 70406.93 68870.55 76854.62 78507.46 
    1481     1482     1483     1484     1485     1486     1487     1488 
78567.25 78469.76 75176.88 68240.08 61586.74 65927.25 66258.28 64782.48 
    1489     1490     1491     1492     1493     1494     1495     1496 
65623.40 65153.53 60790.83 55778.36 60918.16 62370.80 64814.11 66410.64 
    1497     1498     1499     1500     1501     1502     1503     1504 
66311.85 57931.59 56283.50 61641.98 66017.51 69125.53 71083.90 69063.19 
    1505     1506     1507     1508     1509     1510     1511     1512 
62602.88 60306.21 68764.79 74034.05 75820.19 74934.46 73304.84 63875.05 
    1513     1514     1515     1516     1517     1518     1519     1520 
58085.02 62303.52 65031.19 66338.58 68703.30 69522.98 64782.49 62838.74 
    1521     1522     1523     1524     1525     1526     1527     1528 
70446.84 71005.58 69363.15 68450.80 68787.62 64624.97 63041.54 70585.36 
    1529     1530     1531     1532     1533     1534     1535     1536 
72257.40 71596.01 69851.94 68427.18 62647.63 59148.11 65281.20 66436.24 
    1537     1538     1539     1540     1541     1542     1543     1544 
67289.69 67832.76 66256.46 59583.56 56994.21 62722.63 63691.92 62960.52 
    1545     1546     1547     1548     1549     1550     1551     1552 
63387.36 61580.93 54228.46 51935.27 57451.32 54798.45 58585.06 59466.77 
    1553     1554     1555     1556     1557     1558     1559     1560 
60364.36 54849.76 49974.68 54447.54 56304.86 56201.06 57894.57 58896.28 
    1561     1562     1563     1564     1565     1566     1567     1568 
54496.26 48962.68 54925.85 54771.79 54805.62 53858.89 53095.08 48155.47 
    1569     1570     1571     1572     1573     1574     1575     1576 
47123.08 54881.72 55270.77 53543.70 52268.68 51382.32 47608.21 48268.77 
    1577     1578     1579     1580     1581     1582     1583     1584 
56763.78 59999.72 59824.42 59778.73 57199.63 52863.53 50092.12 54229.75 
    1585     1586     1587     1588     1589     1590     1591     1592 
54928.91 52786.64 51310.85 44841.61 40633.86 39332.83 46045.56 48362.12 
    1593     1594     1595     1596     1597     1598     1599     1600 
49040.18 48604.98 48270.40 44279.68 42071.26 48378.31 45808.02 49194.08 
    1601     1602     1603     1604     1605     1606     1607     1608 
49496.53 48576.14 43264.67 40208.85 47200.63 49560.11 49115.50 48159.24 
    1609     1610     1611     1612     1613     1614     1615     1616 
46715.58 41112.00 38475.22 45506.01 48435.54 48973.68 48438.48 48037.29 
    1617     1618     1619     1620     1621     1622     1623     1624 
42801.32 39554.70 45756.43 47208.17 46925.56 46783.50 45929.26 40776.49 
    1625     1626     1627     1628     1629     1630     1631     1632 
38096.11 44839.97 46958.34 46818.94 47282.52 46781.23 41761.03 38865.72 
    1633     1634     1635     1636     1637     1638     1639     1640 
45382.21 47741.26 48572.00 48413.16 47107.76 41547.37 38582.89 45174.67 
    1641     1642     1643     1644     1645     1646     1647     1648 
47327.43 47282.64 47535.71 47162.56 41739.20 38895.75 45594.79 47700.51 
    1649     1650     1651     1652     1653     1654     1655     1656 
47904.96 48207.79 47864.84 43059.89 40458.70 46347.28 47886.07 47647.46 
    1657     1658     1659     1660     1661     1662     1663     1664 
47216.45 42012.86 39436.78 39010.20 46449.26 49438.05 48777.16 47981.47 
    1665     1666     1667     1668     1669     1670     1671     1672 
47248.69 41961.49 39275.39 45789.86 47731.77 47493.89 47279.04 46757.85 
    1673     1674     1675     1676     1677     1678     1679     1680 
41213.87 38275.32 44659.58 45047.80 45253.87 44640.08 44120.00 38620.38 
    1681     1682     1683     1684     1685     1686     1687     1688 
37188.19 43667.41 43546.70 43337.34 42920.64 42571.99 38478.66 37579.51 
    1689     1690     1691     1692     1693     1694     1695     1696 
44078.84 41425.45 43967.92 44108.20 43803.98 38831.64 37288.68 43765.40 
    1697     1698     1699     1700     1701     1702     1703     1704 
46372.94 47127.72 47862.83 47467.82 43013.66 39678.48 45501.25 47566.12 
    1705     1706     1707     1708     1709     1710     1711     1712 
47926.67 48079.00 47682.21 42904.99 39682.67 46240.40 48500.09 48854.83 
    1713     1714     1715     1716     1717     1718     1719     1720 
48066.06 47563.46 42552.57 39698.21 46839.88 48797.63 47971.02 47557.31 
    1721     1722     1723     1724     1725     1726     1727     1728 
46719.12 41783.95 38884.88 45768.18 47564.50 47537.43 47407.87 46739.49 
    1729     1730     1731     1732     1733     1734     1735     1736 
41660.56 38680.20 45529.09 47578.50 47684.43 47482.61 46798.34 41986.18 
    1737     1738     1739     1740     1741     1742     1743     1744 
40123.85 47038.62 48632.88 49131.43 50818.57 50467.54 46030.09 44180.35 
    1745     1746     1747     1748     1749     1750     1751     1752 
52089.68 55483.87 55802.91 56221.38 55963.53 49633.09 44894.21 51282.28 
    1753     1754     1755     1756     1757     1758     1759     1760 
53591.13 54852.37 56073.16 56357.85 51745.49 47678.22 53842.28 54618.30 
    1761     1762     1763     1764     1765     1766     1767     1768 
54515.37 55441.96 55161.41 50368.96 47329.54 53671.37 53386.82 51907.49 
    1769     1770     1771     1772     1773     1774     1775     1776 
58644.44 59199.38 55221.36 53949.34 62792.66 66879.60 66289.69 64954.15 
    1777     1778     1779     1780     1781     1782     1783     1784 
64201.96 55055.78 56282.75 63695.02 64812.99 63272.85 61346.43 60606.44 
    1785     1786     1787     1788     1789     1790     1791     1792 
57466.19 52586.97 58309.08 60579.92 60388.44 60607.15 61049.56 57604.54 
    1793     1794     1795     1796     1797     1798     1799     1800 
55250.24 64501.45 70648.33 72791.41 73583.00 73043.78 67953.00 63817.48 
    1801     1802     1803     1804     1805     1806     1807     1808 
69965.55 70842.03 70629.19 70301.44 69689.09 64527.76 61936.20 68235.12 
    1809     1810     1811     1812     1813     1814     1815     1816 
69427.03 69512.08 69340.46 68406.13 64080.33 64302.97 72138.22 74060.21 
    1817     1818     1819     1820     1821     1822     1823     1824 
72308.17 70950.42 67629.11 60121.73 55729.44 61400.20 64367.20 67964.17 
    1825     1826     1827     1828     1829     1830     1831     1832 
71560.15 74012.59 70885.89 69735.34 75611.23 77021.85 78206.52 79308.67 
    1833     1834     1835     1836     1837     1838     1839     1840 
81120.62 76711.93 72368.16 75679.31 74925.73 72774.78 71352.04 73343.19 
    1841     1842     1843     1844     1845     1846     1847     1848 
70790.66 69103.34 76584.21 81460.36 84351.96 86199.25 85265.34 78921.02 
    1849     1850     1851     1852     1853     1854     1855     1856 
75910.10 81985.38 83166.64 85081.10 83922.50 78244.47 70900.72 64944.34 
    1857     1858     1859     1860     1861     1862     1863     1864 
69210.33 68483.26 65912.98 64479.57 65738.88 61384.60 60396.01 67820.84 
    1865     1866     1867     1868     1869     1870     1871     1872 
69084.81 70047.80 72393.39 73337.59 69261.06 64680.52 67820.81 66728.47 
    1873     1874     1875     1876     1877     1878     1879     1880 
65028.71 63933.07 63766.25 60103.15 57803.89 63427.97 63261.16 61938.57 
    1881     1882     1883     1884     1885     1886     1887     1888 
61910.59 63937.79 60050.29 56848.42 62080.56 65337.33 65573.22 65483.87 
    1889     1890     1891     1892     1893     1894     1895     1896 
61418.22 58144.04 55628.85 63543.91 66337.91 64147.50 62465.69 59059.97 
    1897     1898     1899     1900     1901     1902     1903     1904 
52409.83 50100.06 56736.95 58863.45 58553.76 56836.64 56047.12 50559.59 
    1905     1906     1907     1908     1909     1910     1911     1912 
48610.41 54971.44 58352.21 59639.22 60715.08 59929.10 54105.81 49567.54 
    1913     1914     1915     1916     1917     1918     1919     1920 
54589.43 54749.04 53310.67 52290.47 51410.85 47600.65 45592.53 51756.29 
    1921     1922     1923     1924     1925     1926     1927     1928 
53184.89 53116.56 53800.42 52828.24 46530.89 42480.64 48659.46 50980.45 
    1929     1930     1931     1932     1933     1934     1935     1936 
51020.13 51216.08 50817.40 45788.93 43880.10 50664.36 49813.23 55704.88 
    1937     1938     1939     1940     1941     1942     1943     1944 
57554.35 55441.26 48592.44 45176.28 50436.97 54744.67 57609.67 60025.14 
    1945     1946     1947     1948     1949     1950     1951     1952 
59068.43 51465.42 47318.65 53574.70 49702.82 54435.49 54556.47 52704.32 
    1953     1954     1955     1956     1957     1958     1959     1960 
47163.42 44262.81 50656.51 48429.76 51171.38 50679.94 49441.15 44068.78 
    1961     1962     1963     1964     1965     1966     1967     1968 
40757.17 46696.10 48671.59 47642.05 47624.46 49014.43 43143.68 39962.32 
    1969     1970     1971     1972     1973     1974     1975     1976 
46204.89 47426.55 47393.12 47437.77 42364.40 39120.73 38678.92 45090.18 
    1977     1978     1979     1980     1981     1982     1983     1984 
47280.81 47448.79 47036.39 46713.42 41409.87 38768.03 44688.55 42756.24 
    1985     1986     1987     1988     1989     1990     1991     1992 
46755.38 47298.44 46464.88 42059.99 39145.94 44859.89 47818.10 48515.35 
    1993     1994     1995     1996     1997     1998     1999     2000 
48295.94 47832.29 42934.81 40477.11 47548.19 50322.67 51025.48 50871.31 
    2001     2002     2003     2004     2005     2006     2007     2008 
49575.06 43626.99 40375.98 46587.92 48627.18 48425.67 47785.84 46762.54 
    2009     2010     2011     2012     2013     2014     2015     2016 
41574.63 38977.99 45781.51 48177.91 48739.72 48918.39 49082.29 43900.04 
    2017     2018     2019     2020     2021     2022     2023     2024 
40353.89 46097.24 48030.09 47994.49 48361.83 47179.85 38017.61 38964.80 
    2025     2026     2027     2028     2029     2030     2031     2032 
46613.05 49541.10 49215.81 48384.98 46803.44 41703.24 39029.16 45453.76 
    2033     2034     2035     2036     2037     2038     2039     2040 
47142.25 47150.12 47161.10 46501.76 41527.07 38580.03 45114.63 45917.70 
    2041     2042     2043     2044     2045     2046     2047     2048 
46407.37 46337.21 45803.55 40301.48 38031.98 44343.72 43733.63 43502.89 
    2049     2050     2051     2052     2053     2054     2055     2056 
43191.75 42568.61 37782.16 36432.20 43683.66 42310.08 40371.38 43282.42 
    2057     2058     2059     2060     2061     2062     2063     2064 
42889.77 38472.94 36971.26 43977.89 45332.79 45492.90 45892.51 45747.71 
    2065     2066     2067     2068     2069     2070     2071     2072 
41371.97 39405.13 46257.71 48673.87 47587.10 47704.85 46521.64 41305.14 
    2073     2074     2075     2076     2077     2078     2079     2080 
38651.44 45723.28 47553.95 47340.41 47323.40 46651.84 42185.94 39301.26 
    2081     2082     2083     2084     2085     2086     2087     2088 
45864.43 48013.21 47646.91 48677.27 49247.99 44334.39 41781.46 48891.97 
    2089     2090     2091     2092     2093     2094     2095     2096 
50790.58 50725.87 50229.74 48954.08 43543.85 40560.07 46882.80 48931.47 
    2097     2098     2099     2100     2101     2102     2103     2104 
48888.69 48683.58 47846.32 42820.73 40302.02 47294.57 49106.63 49517.89 
    2105     2106     2107     2108     2109     2110     2111     2112 
49605.19 50347.10 45572.18 42921.47 49197.44 50651.41 50098.34 49850.02 
    2113     2114     2115     2116     2117     2118     2119     2120 
48923.86 43683.78 40743.08 47007.41 48860.95 48945.98 49068.47 48593.41 
    2121     2122     2123     2124     2125     2126     2127     2128 
44078.70 43175.96 50074.52 51999.67 51226.39 50519.33 50584.84 47279.32 
    2129     2130     2131     2132     2133     2134     2135     2136 
45275.33 54400.90 57715.82 57490.40 52001.81 54370.70 48736.91 49290.27 
    2137     2138     2139     2140     2141     2142     2143     2144 
58838.94 64513.93 65299.24 65269.84 62489.57 55155.52 51607.65 61870.07 
    2145     2146     2147     2148     2149     2150     2151     2152 
67312.48 68163.46 68114.58 66297.66 61174.37 58696.87 64910.98 66011.60 
    2153     2154     2155     2156     2157     2158     2159     2160 
63328.43 60904.95 59760.50 57515.34 58701.49 67622.54 70658.56 71721.31 
    2161     2162     2163     2164     2165     2166     2167     2168 
73124.60 75182.26 71993.29 70543.98 75438.33 74604.22 74521.39 72600.20 
    2169     2170     2171     2172     2173     2174     2175     2176 
73482.78 69854.42 65270.79 71037.46 73321.17 72091.62 71630.25 71156.38 
    2177     2178     2179     2180     2181     2182     2183     2184 
68236.40 66168.14 72652.16 74912.35 73846.02 71477.45 68000.60 61119.09 
    2185     2186     2187     2188     2189     2190     2191     2192 
58992.73 66761.28 62029.52 66423.83 70174.45 68445.99 59411.60 55773.30 
    2193     2194     2195     2196     2197     2198     2199     2200 
61965.22 58622.23 61308.66 61467.00 62333.30 58198.50 56362.86 65098.23 
    2201     2202     2203     2204     2205     2206     2207     2208 
67864.48 67546.90 67871.70 68768.16 65752.46 63835.80 68674.25 68155.59 
    2209     2210     2211     2212     2213     2214     2215     2216 
68401.93 68195.76 68650.28 62141.78 59264.64 63365.02 64013.76 63524.55 
    2217     2218     2219     2220     2221     2222     2223     2224 
63742.30 66000.69 63358.80 60567.94 65663.25 66526.18 66288.51 69349.99 
    2225     2226     2227     2228     2229     2230     2231     2232 
71431.63 67958.04 66428.75 75258.10 78710.06 79763.69 80441.25 78949.76 
    2233     2234     2235     2236     2237     2238     2239     2240 
73032.64 67888.94 75622.40 77289.00 77111.70 72692.99 69457.99 63799.13 
    2241     2242     2243     2244     2245     2246     2247     2248 
62202.97 69577.12 71221.73 73299.81 75789.20 77592.32 72489.43 71098.21 
    2249     2250     2251     2252     2253     2254     2255     2256 
81793.19 87271.19 87686.20 83378.21 78868.82 68385.92 63260.90 68035.15 
    2257     2258     2259     2260     2261     2262     2263     2264 
68653.67 68114.75 67323.60 64306.04 56611.22 51679.13 58402.70 62199.04 
    2265     2266     2267     2268     2269     2270     2271     2272 
62096.59 62166.82 61923.19 59296.71 60464.42 70990.42 74019.23 73985.30 
    2273     2274     2275     2276     2277     2278     2279     2280 
72649.33 69547.43 62514.42 57051.61 62620.84 63157.47 63230.38 63511.06 
    2281     2282     2283     2284     2285     2286     2287     2288 
63496.86 59318.71 55486.98 59363.92 54974.08 57604.18 58579.23 56318.64 
    2289     2290     2291     2292     2293     2294     2295     2296 
50329.51 46064.43 52356.52 55147.56 55654.60 56612.58 54941.77 48628.87 
    2297     2298     2299     2300     2301     2302     2303     2304 
44982.91 51711.51 51703.03 50803.68 48608.19 47118.40 42327.78 39223.63 
    2305     2306     2307     2308     2309     2310     2311     2312 
46007.00 47302.31 47749.49 48908.08 48243.93 44145.97 42244.87 51419.60 
    2313     2314     2315     2316     2317     2318     2319     2320 
53167.70 47908.32 52138.14 50344.85 44230.93 40789.25 46257.42 45511.30 
    2321     2322     2323     2324     2325     2326     2327     2328 
43294.85 46154.28 42676.95 40980.22 42250.73 48870.05 50235.04 49997.31 
    2329     2330     2331     2332     2333     2334     2335     2336 
48792.80 47933.05 42545.11 39692.99 46013.67 43663.06 46988.54 47172.30 
    2337     2338     2339     2340     2341     2342     2343     2344 
46596.02 41946.59 38782.07 44659.35 46971.83 46855.14 47089.69 46548.62 
    2345     2346     2347     2348     2349     2350     2351     2352 
41477.24 38447.54 44865.66 46964.66 47088.82 47223.81 46554.08 41462.67 
    2353     2354     2355     2356     2357     2358     2359     2360 
38704.39 45161.51 47208.05 46917.31 46671.37 46301.71 41368.41 38466.31 
    2361     2362     2363     2364     2365     2366     2367     2368 
45030.46 47193.18 47929.77 47578.70 46652.72 41047.98 38269.00 45237.06 
    2369     2370     2371     2372     2373     2374     2375     2376 
47618.95 48034.48 47950.03 47462.42 43117.21 40880.60 46917.14 48770.69 
    2377     2378     2379     2380     2381     2382     2383     2384 
48933.93 48403.04 47593.26 42963.01 40506.12 46955.93 48571.40 48384.07 
    2385     2386     2387     2388     2389     2390     2391     2392 
48091.95 48122.20 43264.25 39791.55 46298.37 48022.86 48377.65 48702.61 
    2393     2394     2395     2396     2397     2398     2399     2400 
47565.80 42181.01 39405.27 46395.03 48664.05 49309.25 49922.46 48976.15 
    2401     2402     2403     2404     2405     2406     2407     2408 
42658.47 39799.02 46613.80 47887.31 48125.59 48699.47 48617.25 43373.27 
    2409     2410     2411     2412     2413     2414     2415     2416 
41021.07 47729.06 47379.34 46368.90 44714.05 43812.77 38465.36 37470.80 
    2417     2418     2419     2420     2421     2422     2423     2424 
43543.32 43598.04 43318.30 41007.55 42328.50 38099.42 37421.49 44438.29 
    2425     2426     2427     2428     2429     2430     2431     2432 
45650.73 45903.95 45931.06 44768.65 39625.21 37628.05 44445.95 46561.13 
    2433     2434     2435     2436     2437     2438     2439     2440 
46472.96 46492.38 45816.85 40859.67 38302.42 45136.47 46893.30 47191.83 
    2441     2442     2443     2444     2445     2446     2447     2448 
46987.55 46425.57 41430.92 38750.72 45476.90 47549.33 47697.70 47384.80 
    2449     2450     2451     2452     2453     2454     2455     2456 
46778.31 41535.35 38870.81 45933.82 47607.10 47659.03 47786.85 46359.50 
    2457     2458     2459     2460     2461     2462     2463     2464 
41761.97 38750.45 46771.49 48342.35 48115.33 48240.06 47342.78 42146.44 
    2465     2466     2467     2468     2469     2470     2471     2472 
40057.59 48264.16 49523.01 49967.70 49242.06 48117.76 42750.41 40703.86 
    2473     2474     2475     2476     2477     2478     2479     2480 
47975.63 49430.76 48991.31 48495.72 47646.42 42309.41 39181.17 46185.44 
    2481     2482     2483     2484     2485     2486     2487     2488 
48146.06 48372.67 48354.98 47949.22 43352.66 41327.85 48866.52 51757.35 
    2489     2490     2491     2492     2493     2494     2495     2496 
51693.57 52087.30 51945.05 50638.72 51316.79 61777.56 64538.26 62300.89 
    2497     2498     2499     2500     2501     2502     2503     2504 
60487.69 55017.55 53215.49 52397.09 57205.38 58103.52 57359.42 58499.00 
    2505     2506     2507     2508     2509     2510     2511     2512 
57545.26 51776.99 48016.85 54128.64 56280.32 56659.46 57457.57 57909.66 
    2513     2514     2515     2516     2517     2518     2519     2520 
55542.65 55553.26 66495.00 71500.39 71736.85 70080.63 68044.14 61963.80 
    2521     2522     2523     2524     2525     2526     2527     2528 
58520.56 66501.93 68313.50 66687.27 64354.46 63343.50 57489.78 53317.57 
    2529     2530     2531     2532     2533     2534     2535     2536 
58572.04 60072.05 59833.50 59911.30 59655.37 55414.11 53905.86 62283.41 
    2537     2538     2539     2540     2541     2542     2543     2544 
67831.05 70851.30 73419.08 75596.03 69939.35 65718.97 69780.15 69761.92 
    2545     2546     2547     2548     2549     2550     2551     2552 
68904.65 67401.91 64185.79 57368.34 52539.43 60074.63 61520.17 62560.24 
    2553     2554     2555     2556     2557     2558     2559     2560 
68982.42 69588.89 64545.14 62504.23 67301.61 64812.17 63054.05 70739.22 
    2561     2562     2563     2564     2565     2566     2567     2568 
74423.15 70347.19 68154.74 73190.98 72179.62 73276.66 76077.29 76425.28 
    2569     2570     2571     2572     2573     2574     2575     2576 
69043.53 62836.96 67562.71 70299.93 71310.75 72455.44 73871.42 70306.13 
    2577     2578     2579     2580     2581     2582     2583     2584 
67799.19 76193.30 78266.70 78890.92 78868.39 77122.74 68800.84 64710.25 
    2585     2586     2587     2588     2589     2590     2591     2592 
72472.05 75905.99 77225.63 76275.24 74822.33 69547.90 68330.50 76027.75 
    2593     2594     2595     2596     2597     2598     2599     2600 
76378.38 72504.48 70780.43 68439.59 62645.23 59831.21 68183.25 72074.58 
    2601     2602     2603     2604     2605     2606     2607     2608 
72325.90 70499.03 67882.99 60978.12 56622.77 63156.47 65548.46 65659.52 
    2609     2610     2611     2612     2613     2614     2615     2616 
65070.62 62205.97 56392.92 53242.91 59917.95 60597.34 59085.76 58754.64 
    2617     2618     2619     2620     2621     2622     2623     2624 
58760.95 53910.21 50474.88 58919.33 60887.28 60100.52 61368.41 60570.83 
    2625     2626     2627     2628     2629     2630     2631     2632 
54363.17 51216.75 59697.64 62865.87 64241.49 63142.97 61594.06 54555.78 
    2633     2634     2635     2636     2637     2638     2639     2640 
52066.55 60866.27 64427.96 63232.10 61803.39 58639.70 51739.52 49064.75 
    2641     2642     2643     2644     2645     2646     2647     2648 
57091.04 59999.28 59950.13 59042.70 56601.16 50023.87 46204.27 52682.76 
    2649     2650     2651     2652     2653     2654     2655     2656 
55116.53 58701.41 61622.54 60451.51 54823.94 50979.72 56561.97 57462.43 
    2657     2658     2659     2660     2661     2662     2663     2664 
57257.84 57725.00 57922.67 53886.24 52564.57 57484.25 57678.51 54968.87 
    2665     2666     2667     2668     2669     2670     2671     2672 
53092.45 50352.38 43978.97 41366.26 47554.90 44744.40 49503.84 50726.36 
    2673     2674     2675     2676     2677     2678     2679     2680 
51160.30 46743.66 45249.17 52056.35 52627.64 51009.18 46055.68 50581.32 
    2681     2682     2683     2684     2685     2686     2687     2688 
48485.48 48802.05 55894.84 54736.87 52927.85 48501.32 50724.35 46547.93 
    2689     2690     2691     2692     2693     2694     2695     2696 
43949.16 50298.88 50951.57 49873.60 49741.91 49637.40 44660.04 42472.76 
    2697     2698     2699     2700     2701     2702     2703     2704 
49053.68 49985.18 48903.09 48092.44 47060.84 41874.64 39195.72 46001.01 
    2705     2706     2707     2708     2709     2710     2711     2712 
48213.79 47995.86 47778.42 42586.06 39324.75 38395.40 44698.12 46699.78 
    2713     2714     2715     2716     2717     2718     2719     2720 
46807.52 46652.58 46071.61 41295.63 38656.13 45479.20 44561.23 47602.94 
    2721     2722     2723     2724     2725     2726     2727     2728 
47704.20 46639.46 41476.89 38712.36 45232.33 47434.55 46999.91 46704.90 
    2729     2730     2731     2732     2733     2734     2735     2736 
46043.59 41138.07 39041.29 45823.56 48810.50 50302.62 50826.33 50021.27 
    2737     2738     2739     2740     2741     2742     2743     2744 
44930.17 41632.42 47556.28 49007.98 48810.37 49150.85 48994.87 44162.65 
    2745     2746     2747     2748     2749     2750     2751     2752 
40941.77 47144.24 48223.19 47867.86 47985.46 47710.60 42638.81 39728.78 
    2753     2754     2755     2756     2757     2758     2759     2760 
45764.82 47455.69 47905.95 47802.95 47640.04 42585.45 40181.27 47603.73 
    2761     2762     2763     2764     2765     2766     2767     2768 
50786.23 51277.52 51840.03 49115.75 42209.43 38458.08 44847.67 45567.02 
    2769     2770     2771     2772     2773     2774     2775     2776 
45735.85 45654.70 45200.07 39980.15 38163.40 44857.42 44846.73 44354.70 
    2777     2778     2779     2780     2781     2782     2783     2784 
44671.94 44168.32 39125.95 36600.35 43292.50 42713.55 42454.30 42303.41 
    2785     2786     2787     2788     2789     2790     2791     2792 
39443.45 36447.10 36349.12 43231.16 43441.94 43712.56 43641.68 43816.00 
    2793     2794     2795     2796     2797     2798     2799     2800 
39717.34 38154.14 45364.16 46850.09 46997.78 47056.10 46798.59 41752.91 
    2801     2802     2803     2804     2805     2806     2807     2808 
38672.85 45378.51 46577.01 46775.73 46633.91 46141.45 40976.40 38539.95 
    2809     2810     2811     2812     2813     2814     2815     2816 
45776.89 47280.82 47108.70 47019.62 46465.48 41279.96 38664.88 45501.21 
    2817     2818     2819     2820     2821     2822     2823     2824 
47003.35 46690.56 46566.63 46047.96 41152.76 38007.48 45012.92 46383.46 
    2825     2826     2827     2828     2829     2830     2831     2832 
46456.44 46843.73 46363.86 41110.08 38706.53 45629.16 47179.50 47780.90 
    2833     2834     2835     2836     2837     2838     2839     2840 
48860.64 48328.57 43238.06 40585.09 47846.66 49102.13 49523.29 49601.89 
    2841     2842     2843     2844     2845     2846     2847     2848 
48734.02 43227.84 40091.62 46515.56 48820.37 48934.86 49565.36 49054.39 
    2849     2850     2851     2852     2853     2854     2855     2856 
43886.26 41815.24 50074.90 51927.37 52126.14 51675.06 50311.24 44906.69 
    2857     2858     2859     2860     2861     2862     2863     2864 
42795.99 50771.95 53354.01 53779.08 53693.13 52085.23 43032.00 44171.68 
    2865     2866     2867     2868     2869     2870     2871     2872 
52824.86 56129.62 57221.62 59245.44 60373.63 57158.82 55665.63 62520.54 
    2873     2874     2875     2876     2877     2878     2879     2880 
60762.02 65629.76 68123.56 69448.93 64720.16 62201.13 68112.51 70885.04 
    2881     2882     2883     2884     2885     2886     2887     2888 
71287.06 70172.46 66671.93 59363.35 55804.88 61506.29 61405.87 60375.50 
    2889     2890     2891     2892     2893     2894     2895     2896 
60354.78 60673.95 57789.30 57927.87 67418.81 71947.69 73404.65 73428.94 
    2897     2898     2899     2900     2901     2902     2903     2904 
70221.96 63229.30 56891.58 64192.73 69043.46 70323.50 69900.93 67379.81 
    2905     2906     2907     2908     2909     2910     2911     2912 
59482.63 55836.26 61246.47 62012.49 60873.00 59773.41 59815.95 54374.11 
    2913     2914     2915     2916     2917     2918     2919     2920 
53412.98 61085.49 59189.48 58316.63 54580.30 58115.29 55449.33 58894.82 
    2921     2922     2923     2924     2925     2926     2927     2928 
68787.82 69213.94 67617.81 62512.15 63424.31 59625.37 60544.98 68520.77 
    2929     2930     2931     2932     2933     2934     2935     2936 
68907.79 65460.77 63582.52 63632.98 61509.18 60616.29 67667.54 66442.85 
    2937     2938     2939     2940     2941     2942     2943     2944 
65504.04 64352.25 64191.75 62205.05 62761.12 71966.67 74423.27 73985.45 
    2945     2946     2947     2948     2949     2950     2951     2952 
73887.60 73197.40 66570.49 62609.22 67448.75 68451.73 68303.18 66106.53 
    2953     2954     2955     2956     2957     2958     2959     2960 
63444.49 55530.57 51078.65 57532.33 61593.01 65512.62 68278.20 67097.62 
    2961     2962     2963     2964     2965     2966     2967     2968 
60901.93 54689.82 60536.02 63113.64 65815.74 65306.48 64722.39 56809.47 
    2969     2970     2971     2972     2973     2974     2975     2976 
52388.37 59721.17 63125.03 64334.05 64200.13 63900.10 58155.28 53496.02 
    2977     2978     2979     2980     2981     2982     2983     2984 
58863.29 61177.48 64296.74 66452.59 66130.56 59542.85 55592.06 63939.63 
    2985     2986     2987     2988     2989     2990     2991     2992 
67569.38 67249.40 65875.68 65969.24 60586.64 56680.54 63116.01 62286.47 
    2993     2994     2995     2996     2997     2998     2999     3000 
60789.84 58845.20 59383.80 55334.94 51756.84 57588.81 57286.91 54198.45 
    3001     3002     3003     3004     3005     3006     3007     3008 
50783.62 49269.89 45252.58 45940.46 54592.47 54425.91 55123.68 56070.85 
    3009     3010     3011     3012     3013     3014     3015     3016 
54350.94 47613.58 48377.70 59025.72 58534.09 56480.03 55262.63 53423.82 
    3017     3018     3019     3020     3021     3022     3023     3024 
46028.81 43055.76 49299.48 47039.27 46044.39 44731.23 43222.21 37723.85 
    3025     3026     3027     3028 
36882.21 44841.07 43218.83 44476.85 
> dim(experts.train)
[1] 3028
> experts.train  = array_reshape(predict(model,train), c(3028,1,1))
> experts.test = pred.test
> res = add_expert(pred.lstm$train, pred.lstm$test, experts.train, experts.test)
Error in experts.train[, 3] : nombre de dimensions incorrect
> dim(experts.train)
[1] 3028    1    1
> dim(experts.train)[3]
[1] 1
> add_expert = function(new_train, new_test, experts.train, experts.test){
+     experts.train = array_reshape(abind(experts.train, new_train, along=2), c(3028,1,dim(experts.train)[3]+1))
+     experts.test = cbind(experts.test, new_test)
+     return(list("experts.train" = experts.train, "experts.test" = experts.test))
+ }
> experts.train  = array_reshape(predict(model,train), c(3028,1,1))
> experts.test = pred.test
> res = add_expert(pred.lstm$train, pred.lstm$test, experts.train, experts.test)
> experts.train = res$experts.train
> experts.test = res$experts.test
> expert2.train  = pred.lstm$train
> add_expert = function(new_train, new_test, experts.train, experts.test){
+     experts.train = array_reshape(abind(experts.train, new_train, along=2), c(3028,1,dim(experts.train)[3]+1))
+     experts.test = cbind(experts.test, new_test)
+     return(list("experts.train" = experts.train, "experts.test" = experts.test))
+ }
> experts.train  = array_reshape(predict(model,train), c(3028,1,1))
> experts.test = pred.test
> res = add_expert(pred.lstm$train, pred.lstm$test, experts.train, experts.test)
> experts.train = res$experts.train
> experts.test = res$experts.test
> res = add_expert(rf$predicted, pred.test.rf, experts.train, experts.test)
Error in rf$predicted : objet de type 'closure' non indiçable
> experts.train = res$experts.train
> rm(list=objects())
+ graphics.off()
+ 
+ ##pour load tout en 2 lignes, il faut juste rajouter les librairies dans libs.to.load
+ 
+ libs.to.load = c("tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
+ suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
+ 
+ ##setwd("C:/Users/CM/code/M1/R")
+ 
+ ##load tous les fichiers en sources
+ files.sources = list.files(pattern = "*.r$")
+ files.sources = files.sources[files.sources != "main_matthieu.r"]
+ sapply(files.sources, source)
+ 
+ ##
+ plt = FALSE #(si on veut plot, mettre à TRUE)
+ path_submission = "./submissions/submission.csv"
+ 
+ 
+ ##MAIN NICOLAS
+ model = "lstm"
+ 
+ data = format_data("./data/train_V2.csv", "./data/test_V2.csv")
+ train_set = data$train_set
+ test_set = data$test_set
+ train_label = data$train_label
+ test_label = data$test_label
+ 
+ if (model == "xgboost"){
+   pred = xgboost_rte(train_set, train_label, test_set)
+ } else if(model == "lstm"){
+   pred = lstm(train_set, train_label, test_set)
+ }
+ 
+ 
+ print(paste("Score final : ", evaluate(test_label, pred), sep=""))
+ 
+ if (plt) {
+     plot(c(train_label, pred))
+ }
+ 
+ ##FIN MAIN NICOLAS 
+ 
+ 
+ train <- read_delim(file="data/train_V2.csv",delim=',')
+ test <- read_delim(file="data/test_V2.csv",delim=',')
+ 
+ reg = lm(Load~Temp, data=train)
+ 
+ if (plt){
+     plot(train$Date, train$Load, type='l')
+     par(new=T)
+     plot(train$Date, train$Temp, type='l', col='red')
+ 
+     plot(train$Temp,train$Load, col='red')
+     points(train$Temp ,reg$fitted.values)
+ }
+ 
+ train$WeekDays = days_to_numeric(train)
+ test$WeekDays = days_to_numeric(test)
+ 
+ train$Year = train$Year - 2012
+ test$Year = test$Year - 2012
+ 
+ total.time = c(1:(nrow(train)+nrow(test)))
+ length(total.time)
+ train$time = total.time[1:nrow(train)]
+ test$time = tail(total.time,nrow(test))
+ 
+ 
+ #saisonalite : annuelle
+ 
+ if (plt){
+     plot(train$Date, train$Load, type='l')
+ }
+ 
+ MA <- stats::filter(train$Load, filter = rep(1/365,365),
+              method = c("convolution"), sides = 2, circular = FALSE)
+ if (plt){
+     plot(train$Date, train$Load, type = "l", xlab = "",
+          ylab = "consumption (kw)", col = "seagreen4", lwd = 1)
+     lines(train$Date, MA, col = "red", lwd = 2)
+ }
+ 
+ ##estimation avec fourier
+ pred.fourier = fourier(train, test, plt = TRUE)
+ 
+ ##saisonalite : hebdomadaire
+ 
+ num.years = 0
+ average = 0
+ N = 8; a = 0.7 #exponential weight
+ while (365*(num.years+1) <= length(train$Date)) {
+     par(mfrow = c(1, 1))
+     dateyear = train$Date[(365*num.years+1):(365*(num.years+1))]
+     loadyear = train$Load[(365*num.years+1):(365*(num.years+1))]
+ 
+     ##plot(dateyear,loadyear,type='l')
+   
+     MAw <- stats::filter(loadyear, filter = rep(1/52,52),
+                       method = c("convolution"), sides = 2, circular = T)
+     ##plot(dateyear,loadyear, type = "l", xlab = "",
+     ##    ylab = "consumption (kw)", col = "seagreen4", lwd = 1)
+     ##lines(dateyear, MAw, col = "red", lwd = 2)
+   
+     ##plot(dateyear, loadyear - MAw, type="l")
+     expweight = (((1-a)/(1-a^8))*a^(N-(num.years+1)))
+     average = expweight*(average + (loadyear - MAw))
+     num.years = num.years + 1
+ }
+ if (plt){
+     plot(average)
+ }
+ 
+ train.to.day = train$time %% 365 + 1
+ test.to.day = test$time %% 365 + 1
+ 
+ pred.hebdo.train = average[train.to.day]
+ pred.hebdo.test = average[test.to.day]
+ 
+ pred.total.train = reg$fitted + pred.hebdo.train
+ pred.total.test = pred.fourier + pred.hebdo.test
+ 
+ 
+ if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(train$time,pred.total.train, col='red', lwd=1)
+     lines(test$time,pred.total.test, col='green', lwd=1)
+ }
+ 
+ Load = pred.total.test
+ Id = test$Id
+ submission = data.frame(Load, Id)
+ 
+ write.csv(submission, file =path_submission, row.names=F)
+ 
+ 
+ MAw.train.total <- stats::filter(train$Load.1, filter = rep(1/52,52),
+                      method = c("convolution"), sides = 2, circular = T)
+ MAw.test.total <- stats::filter(test$Load.1, filter = rep(1/52,52),
+                                  method = c("convolution"), sides = 2, circular = T)
+ 
+ train$seasonal.tendancy = train$Load.1 - MAw.train.total
+ test$seasonal.tendancy = test$Load.1 - MAw.test.total
+ 
+ train$fourier.fitted = reg$fitted
+ test$fourier.fitted = pred.fourier
+ 
+ reg2 = lm(Load~Load.1+Load.7+Temp
+ +Temp_s95+WeekDays+GovernmentResponseIndex,data=train)
+ summary(reg2)
+ 
+ ##visreg(Gam,"Temp_s95")
+ gam.test = gam_rte(train, test)
+ 
+ tmp = format_data("./data/train_V2.csv", "./data/test_V2.csv")
+ test_label = tmp$test_label
+ 
+ Gam.rmse = evaluate(test_label, gam.test)
+ 
+ ##cross-validation
+ cv = cross_validation(train, test)
+ model = cv$model; pred.test = cv$pred.test; RMSE = cv$RMSE
+ 
+ if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(train$time,predict(model,train), col='red', lwd=1)
+     lines(test$time,pred.test, col='green', lwd=1)
+ }
+ 
+ Load = pred.test
+ Id = 1:length(Load)
+ submission = data.frame(Load, Id)
+ 
+ write.csv(submission, file =path_submission, row.names=F)
+ 
+ ##lstm
+ 
+ pred.lstm = neural_network(train, test)
+ 
+ ##random forest
+ 
+ res = random_forest(train, test)
+ pred.test.rf = res$pred.test.rf
+ rf = res$rf
+ 
+ 
   tidyverse    lubridate       ranger       pracma      Metrics         mgcv 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       keras       visreg        caret         mc2d        opera        abind 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
randomForest   tensorflow 
        TRUE         TRUE 
> > > > > > > Error in FUN(X[[i]], ...) : random_forest.r:17:2: '}' inattendu(e)
16:     return(list("pred.test.rf" = pred.test.rf, "rf" = rf)
17:  }
     ^
> > > > > > [1] "Load and format  ./data/train_V2.csv"
[1] "Load and format  ./data/test_V2.csv"
> > > > > > . + > [1] "step1"
[1] "step2"
[1] "step3"
Epoch 1/100
95/95 [==============================] - 0s 955us/step - loss: 2755577344.0000 - mean_absolute_error: 49037.9766
95/95 [==============================] - 0s 974us/step - loss: 2755577344.0000 - mean_absolute_error: 49037.9766
Epoch 2/100
95/95 [==============================] - 0s 952us/step - loss: 2740423680.0000 - mean_absolute_error: 48555.7031
95/95 [==============================] - 0s 972us/step - loss: 2740423680.0000 - mean_absolute_error: 48555.7031
Epoch 3/100
95/95 [==============================] - 0s 960us/step - loss: 2739674880.0000 - mean_absolute_error: 48562.5898
95/95 [==============================] - 0s 978us/step - loss: 2739674880.0000 - mean_absolute_error: 48562.5898
Epoch 4/100
95/95 [==============================] - 0s 903us/step - loss: 2738417920.0000 - mean_absolute_error: 48582.3516
95/95 [==============================] - 0s 921us/step - loss: 2738417920.0000 - mean_absolute_error: 48582.3516
Epoch 5/100
95/95 [==============================] - 0s 925us/step - loss: 2736305152.0000 - mean_absolute_error: 48571.8750
95/95 [==============================] - 0s 944us/step - loss: 2736305152.0000 - mean_absolute_error: 48571.8750
Epoch 6/100
95/95 [==============================] - 0s 1ms/step - loss: 2733650688.0000 - mean_absolute_error: 48584.3164
95/95 [==============================] - 0s 1ms/step - loss: 2733650688.0000 - mean_absolute_error: 48584.3164
Epoch 7/100
95/95 [==============================] - 0s 935us/step - loss: 2729272320.0000 - mean_absolute_error: 48532.3633
95/95 [==============================] - 0s 955us/step - loss: 2729272320.0000 - mean_absolute_error: 48532.3633
Epoch 8/100
95/95 [==============================] - 0s 970us/step - loss: 2722296064.0000 - mean_absolute_error: 48433.3281
95/95 [==============================] - 0s 992us/step - loss: 2722296064.0000 - mean_absolute_error: 48433.3281
Epoch 9/100
95/95 [==============================] - 0s 940us/step - loss: 2710958848.0000 - mean_absolute_error: 48318.0117
95/95 [==============================] - 0s 960us/step - loss: 2710958848.0000 - mean_absolute_error: 48318.0117
Epoch 10/100
95/95 [==============================] - 0s 939us/step - loss: 2692416000.0000 - mean_absolute_error: 48150.6211
95/95 [==============================] - 0s 959us/step - loss: 2692416000.0000 - mean_absolute_error: 48150.6211
Epoch 11/100
95/95 [==============================] - 0s 919us/step - loss: 2664218624.0000 - mean_absolute_error: 47959.3984
95/95 [==============================] - 0s 938us/step - loss: 2664218624.0000 - mean_absolute_error: 47959.3984
Epoch 12/100
95/95 [==============================] - 0s 957us/step - loss: 2621976832.0000 - mean_absolute_error: 47598.2344
95/95 [==============================] - 0s 976us/step - loss: 2621976832.0000 - mean_absolute_error: 47598.2344
Epoch 13/100
95/95 [==============================] - 0s 935us/step - loss: 2561312000.0000 - mean_absolute_error: 47001.6211
95/95 [==============================] - 0s 953us/step - loss: 2561312000.0000 - mean_absolute_error: 47001.6211
Epoch 14/100
95/95 [==============================] - 0s 904us/step - loss: 2482472704.0000 - mean_absolute_error: 46394.1328
95/95 [==============================] - 0s 922us/step - loss: 2482472704.0000 - mean_absolute_error: 46394.1328
Epoch 15/100
95/95 [==============================] - 0s 901us/step - loss: 2382533120.0000 - mean_absolute_error: 45380.6172
95/95 [==============================] - 0s 919us/step - loss: 2382533120.0000 - mean_absolute_error: 45380.6172
Epoch 16/100
95/95 [==============================] - 0s 948us/step - loss: 2281399808.0000 - mean_absolute_error: 44722.7070
95/95 [==============================] - 0s 970us/step - loss: 2281399808.0000 - mean_absolute_error: 44722.7070
Epoch 17/100
95/95 [==============================] - 0s 948us/step - loss: 2141156736.0000 - mean_absolute_error: 42952.7891
95/95 [==============================] - 0s 968us/step - loss: 2141156736.0000 - mean_absolute_error: 42952.7891
Epoch 18/100
95/95 [==============================] - 0s 972us/step - loss: 1994609664.0000 - mean_absolute_error: 41407.4023
95/95 [==============================] - 0s 991us/step - loss: 1994609664.0000 - mean_absolute_error: 41407.4023
Epoch 19/100
95/95 [==============================] - 0s 922us/step - loss: 1846904576.0000 - mean_absolute_error: 40017.1797
95/95 [==============================] - 0s 943us/step - loss: 1846904576.0000 - mean_absolute_error: 40017.1797
Epoch 20/100
95/95 [==============================] - 0s 919us/step - loss: 1672118400.0000 - mean_absolute_error: 37783.2773
95/95 [==============================] - 0s 937us/step - loss: 1672118400.0000 - mean_absolute_error: 37783.2773
Epoch 21/100
95/95 [==============================] - 0s 912us/step - loss: 1532434432.0000 - mean_absolute_error: 36593.1484
95/95 [==============================] - 0s 930us/step - loss: 1532434432.0000 - mean_absolute_error: 36593.1484
Epoch 22/100
95/95 [==============================] - 0s 979us/step - loss: 1338516224.0000 - mean_absolute_error: 33642.4258
95/95 [==============================] - 0s 999us/step - loss: 1338516224.0000 - mean_absolute_error: 33642.4258
Epoch 23/100
95/95 [==============================] - 0s 921us/step - loss: 1171110016.0000 - mean_absolute_error: 31368.7090
95/95 [==============================] - 0s 939us/step - loss: 1171110016.0000 - mean_absolute_error: 31368.7090
Epoch 24/100
95/95 [==============================] - 0s 920us/step - loss: 1020056000.0000 - mean_absolute_error: 29233.9102
95/95 [==============================] - 0s 939us/step - loss: 1020056000.0000 - mean_absolute_error: 29233.9102
Epoch 25/100
95/95 [==============================] - 0s 955us/step - loss: 910897600.0000 - mean_absolute_error: 27568.3066
95/95 [==============================] - 0s 976us/step - loss: 910897600.0000 - mean_absolute_error: 27568.3066
Epoch 26/100
95/95 [==============================] - 0s 897us/step - loss: 745062016.0000 - mean_absolute_error: 24505.1855
95/95 [==============================] - 0s 916us/step - loss: 745062016.0000 - mean_absolute_error: 24505.1855
Epoch 27/100
95/95 [==============================] - 0s 922us/step - loss: 604988928.0000 - mean_absolute_error: 21417.8516
95/95 [==============================] - 0s 940us/step - loss: 604988928.0000 - mean_absolute_error: 21417.8516
Epoch 28/100
95/95 [==============================] - 0s 911us/step - loss: 532464512.0000 - mean_absolute_error: 20112.2266
95/95 [==============================] - 0s 929us/step - loss: 532464512.0000 - mean_absolute_error: 20112.2266
Epoch 29/100
95/95 [==============================] - 0s 924us/step - loss: 412008544.0000 - mean_absolute_error: 16795.5742
95/95 [==============================] - 0s 944us/step - loss: 412008544.0000 - mean_absolute_error: 16795.5742
Epoch 30/100
95/95 [==============================] - 0s 907us/step - loss: 344386016.0000 - mean_absolute_error: 15030.1982
95/95 [==============================] - 0s 925us/step - loss: 344386016.0000 - mean_absolute_error: 15030.1982
Epoch 31/100
95/95 [==============================] - 0s 926us/step - loss: 309393792.0000 - mean_absolute_error: 13869.0527
95/95 [==============================] - 0s 944us/step - loss: 309393792.0000 - mean_absolute_error: 13869.0527
Epoch 32/100
95/95 [==============================] - 0s 908us/step - loss: 248028128.0000 - mean_absolute_error: 11968.4199
95/95 [==============================] - 0s 926us/step - loss: 248028128.0000 - mean_absolute_error: 11968.4199
Epoch 33/100
95/95 [==============================] - 0s 912us/step - loss: 217194816.0000 - mean_absolute_error: 10833.6680
95/95 [==============================] - 0s 932us/step - loss: 217194816.0000 - mean_absolute_error: 10833.6680
Epoch 34/100
95/95 [==============================] - 0s 945us/step - loss: 185676544.0000 - mean_absolute_error: 10038.3984
95/95 [==============================] - 0s 963us/step - loss: 185676544.0000 - mean_absolute_error: 10038.3984
Epoch 35/100
95/95 [==============================] - 0s 908us/step - loss: 154944880.0000 - mean_absolute_error: 9161.5645
95/95 [==============================] - 0s 927us/step - loss: 154944880.0000 - mean_absolute_error: 9161.5645
Epoch 36/100
95/95 [==============================] - 0s 903us/step - loss: 143938656.0000 - mean_absolute_error: 8987.5439
95/95 [==============================] - 0s 921us/step - loss: 143938656.0000 - mean_absolute_error: 8987.5439
Epoch 37/100
95/95 [==============================] - 0s 951us/step - loss: 134079960.0000 - mean_absolute_error: 8867.8516
95/95 [==============================] - 0s 970us/step - loss: 134079960.0000 - mean_absolute_error: 8867.8516
Epoch 38/100
95/95 [==============================] - 0s 1ms/step - loss: 127698392.0000 - mean_absolute_error: 8836.5742
95/95 [==============================] - 0s 1ms/step - loss: 127698392.0000 - mean_absolute_error: 8836.5742
Epoch 39/100
95/95 [==============================] - 0s 908us/step - loss: 123414872.0000 - mean_absolute_error: 8834.8809
95/95 [==============================] - 0s 926us/step - loss: 123414872.0000 - mean_absolute_error: 8834.8809
Epoch 40/100
95/95 [==============================] - 0s 912us/step - loss: 120573168.0000 - mean_absolute_error: 8848.0518
95/95 [==============================] - 0s 930us/step - loss: 120573168.0000 - mean_absolute_error: 8848.0518
Epoch 41/100
95/95 [==============================] - 0s 909us/step - loss: 118673216.0000 - mean_absolute_error: 8866.4316
95/95 [==============================] - 0s 927us/step - loss: 118673216.0000 - mean_absolute_error: 8866.4316
Epoch 42/100
95/95 [==============================] - 0s 897us/step - loss: 117380008.0000 - mean_absolute_error: 8883.6162
95/95 [==============================] - 0s 915us/step - loss: 117380008.0000 - mean_absolute_error: 8883.6162
Epoch 43/100
95/95 [==============================] - 0s 900us/step - loss: 116472968.0000 - mean_absolute_error: 8897.3281
95/95 [==============================] - 0s 918us/step - loss: 116472968.0000 - mean_absolute_error: 8897.3281
Epoch 44/100
95/95 [==============================] - 0s 977us/step - loss: 115809752.0000 - mean_absolute_error: 8907.2676
95/95 [==============================] - 0s 995us/step - loss: 115809752.0000 - mean_absolute_error: 8907.2676
Epoch 45/100
95/95 [==============================] - 0s 921us/step - loss: 115299888.0000 - mean_absolute_error: 8913.3584
95/95 [==============================] - 0s 940us/step - loss: 115299888.0000 - mean_absolute_error: 8913.3584
Epoch 46/100
95/95 [==============================] - 0s 907us/step - loss: 114886112.0000 - mean_absolute_error: 8915.8770
95/95 [==============================] - 0s 927us/step - loss: 114886112.0000 - mean_absolute_error: 8915.8770
Epoch 47/100
95/95 [==============================] - 0s 918us/step - loss: 114532480.0000 - mean_absolute_error: 8915.2930
95/95 [==============================] - 0s 936us/step - loss: 114532480.0000 - mean_absolute_error: 8915.2930
Epoch 48/100
95/95 [==============================] - 0s 901us/step - loss: 114216304.0000 - mean_absolute_error: 8912.1777
95/95 [==============================] - 0s 919us/step - loss: 114216304.0000 - mean_absolute_error: 8912.1777
Epoch 49/100
95/95 [==============================] - 0s 921us/step - loss: 113923256.0000 - mean_absolute_error: 8907.0459
95/95 [==============================] - 0s 940us/step - loss: 113923256.0000 - mean_absolute_error: 8907.0459
Epoch 50/100
95/95 [==============================] - 0s 934us/step - loss: 113643880.0000 - mean_absolute_error: 8900.3027
95/95 [==============================] - 0s 953us/step - loss: 113643880.0000 - mean_absolute_error: 8900.3027
Epoch 51/100
95/95 [==============================] - 0s 966us/step - loss: 113372280.0000 - mean_absolute_error: 8892.2842
95/95 [==============================] - 0s 984us/step - loss: 113372280.0000 - mean_absolute_error: 8892.2842
Epoch 52/100
95/95 [==============================] - 0s 921us/step - loss: 113104176.0000 - mean_absolute_error: 8883.1885
95/95 [==============================] - 0s 940us/step - loss: 113104176.0000 - mean_absolute_error: 8883.1885
Epoch 53/100
95/95 [==============================] - 0s 900us/step - loss: 112837000.0000 - mean_absolute_error: 8873.1738
95/95 [==============================] - 0s 918us/step - loss: 112837000.0000 - mean_absolute_error: 8873.1738
Epoch 54/100
95/95 [==============================] - 0s 973us/step - loss: 112568968.0000 - mean_absolute_error: 8862.3984
95/95 [==============================] - 0s 993us/step - loss: 112568968.0000 - mean_absolute_error: 8862.3984
Epoch 55/100
95/95 [==============================] - 0s 921us/step - loss: 112298984.0000 - mean_absolute_error: 8850.9609
95/95 [==============================] - 0s 940us/step - loss: 112298984.0000 - mean_absolute_error: 8850.9609
Epoch 56/100
95/95 [==============================] - 0s 938us/step - loss: 112026344.0000 - mean_absolute_error: 8838.9414
95/95 [==============================] - 0s 957us/step - loss: 112026344.0000 - mean_absolute_error: 8838.9414
Epoch 57/100
95/95 [==============================] - 0s 991us/step - loss: 111750480.0000 - mean_absolute_error: 8826.3818
95/95 [==============================] - 0s 1ms/step - loss: 111750480.0000 - mean_absolute_error: 8826.3818  
Epoch 58/100
95/95 [==============================] - 0s 908us/step - loss: 111471312.0000 - mean_absolute_error: 8813.3564
95/95 [==============================] - 0s 926us/step - loss: 111471312.0000 - mean_absolute_error: 8813.3564
Epoch 59/100
95/95 [==============================] - 0s 898us/step - loss: 111188976.0000 - mean_absolute_error: 8799.8916
95/95 [==============================] - 0s 916us/step - loss: 111188976.0000 - mean_absolute_error: 8799.8916
Epoch 60/100
95/95 [==============================] - 0s 926us/step - loss: 110903816.0000 - mean_absolute_error: 8786.0605
95/95 [==============================] - 0s 944us/step - loss: 110903816.0000 - mean_absolute_error: 8786.0605
Epoch 61/100
95/95 [==============================] - 0s 897us/step - loss: 110615720.0000 - mean_absolute_error: 8771.8770
95/95 [==============================] - 0s 915us/step - loss: 110615720.0000 - mean_absolute_error: 8771.8770
Epoch 62/100
95/95 [==============================] - 0s 906us/step - loss: 110325400.0000 - mean_absolute_error: 8757.3340
95/95 [==============================] - 0s 924us/step - loss: 110325400.0000 - mean_absolute_error: 8757.3340
Epoch 63/100
95/95 [==============================] - 0s 942us/step - loss: 110033224.0000 - mean_absolute_error: 8742.4502
95/95 [==============================] - 0s 961us/step - loss: 110033224.0000 - mean_absolute_error: 8742.4502
Epoch 64/100
95/95 [==============================] - 0s 914us/step - loss: 109739512.0000 - mean_absolute_error: 8727.2764
95/95 [==============================] - 0s 932us/step - loss: 109739512.0000 - mean_absolute_error: 8727.2764
Epoch 65/100
95/95 [==============================] - 0s 910us/step - loss: 109444440.0000 - mean_absolute_error: 8711.7988
95/95 [==============================] - 0s 931us/step - loss: 109444440.0000 - mean_absolute_error: 8711.7988
Epoch 66/100
95/95 [==============================] - 0s 910us/step - loss: 109149256.0000 - mean_absolute_error: 8696.0869
95/95 [==============================] - 0s 928us/step - loss: 109149256.0000 - mean_absolute_error: 8696.0869
Epoch 67/100
95/95 [==============================] - 0s 924us/step - loss: 108854440.0000 - mean_absolute_error: 8680.1582
95/95 [==============================] - 0s 942us/step - loss: 108854440.0000 - mean_absolute_error: 8680.1582
Epoch 68/100
95/95 [==============================] - 0s 907us/step - loss: 108559928.0000 - mean_absolute_error: 8663.9395
95/95 [==============================] - 0s 928us/step - loss: 108559928.0000 - mean_absolute_error: 8663.9395
Epoch 69/100
95/95 [==============================] - 0s 936us/step - loss: 108267152.0000 - mean_absolute_error: 8647.5449
95/95 [==============================] - 0s 955us/step - loss: 108267152.0000 - mean_absolute_error: 8647.5449
Epoch 70/100
95/95 [==============================] - 0s 990us/step - loss: 107976752.0000 - mean_absolute_error: 8631.0420
95/95 [==============================] - 0s 1ms/step - loss: 107976752.0000 - mean_absolute_error: 8631.0420  
Epoch 71/100
95/95 [==============================] - 0s 933us/step - loss: 107689536.0000 - mean_absolute_error: 8614.4199
95/95 [==============================] - 0s 953us/step - loss: 107689536.0000 - mean_absolute_error: 8614.4199
Epoch 72/100
95/95 [==============================] - 0s 901us/step - loss: 107406112.0000 - mean_absolute_error: 8597.7324
95/95 [==============================] - 0s 919us/step - loss: 107406112.0000 - mean_absolute_error: 8597.7324
Epoch 73/100
95/95 [==============================] - 0s 924us/step - loss: 107127264.0000 - mean_absolute_error: 8581.0137
95/95 [==============================] - 0s 942us/step - loss: 107127264.0000 - mean_absolute_error: 8581.0137
Epoch 74/100
95/95 [==============================] - 0s 910us/step - loss: 106853816.0000 - mean_absolute_error: 8564.2764
95/95 [==============================] - 0s 928us/step - loss: 106853816.0000 - mean_absolute_error: 8564.2764
Epoch 75/100
95/95 [==============================] - 0s 899us/step - loss: 106586408.0000 - mean_absolute_error: 8547.5518
95/95 [==============================] - 0s 917us/step - loss: 106586408.0000 - mean_absolute_error: 8547.5518
Epoch 76/100
95/95 [==============================] - 0s 946us/step - loss: 106324584.0000 - mean_absolute_error: 8530.8359
95/95 [==============================] - 0s 964us/step - loss: 106324584.0000 - mean_absolute_error: 8530.8359
Epoch 77/100
95/95 [==============================] - 0s 930us/step - loss: 106067992.0000 - mean_absolute_error: 8514.0400
95/95 [==============================] - 0s 948us/step - loss: 106067992.0000 - mean_absolute_error: 8514.0400
Epoch 78/100
95/95 [==============================] - 0s 912us/step - loss: 105819680.0000 - mean_absolute_error: 8497.4697
95/95 [==============================] - 0s 932us/step - loss: 105819680.0000 - mean_absolute_error: 8497.4697
Epoch 79/100
95/95 [==============================] - 0s 902us/step - loss: 105579720.0000 - mean_absolute_error: 8481.1826
95/95 [==============================] - 0s 920us/step - loss: 105579720.0000 - mean_absolute_error: 8481.1826
Epoch 80/100
95/95 [==============================] - 0s 903us/step - loss: 105348728.0000 - mean_absolute_error: 8465.2773
95/95 [==============================] - 0s 921us/step - loss: 105348728.0000 - mean_absolute_error: 8465.2773
Epoch 81/100
95/95 [==============================] - 0s 898us/step - loss: 105126624.0000 - mean_absolute_error: 8449.6641
95/95 [==============================] - 0s 916us/step - loss: 105126624.0000 - mean_absolute_error: 8449.6641
Epoch 82/100
95/95 [==============================] - 0s 939us/step - loss: 104914632.0000 - mean_absolute_error: 8434.5244
95/95 [==============================] - 0s 957us/step - loss: 104914632.0000 - mean_absolute_error: 8434.5244
Epoch 83/100
95/95 [==============================] - 0s 912us/step - loss: 104713000.0000 - mean_absolute_error: 8419.9434
95/95 [==============================] - 0s 930us/step - loss: 104713000.0000 - mean_absolute_error: 8419.9434
Epoch 84/100
95/95 [==============================] - 0s 903us/step - loss: 104521816.0000 - mean_absolute_error: 8405.9082
95/95 [==============================] - 0s 921us/step - loss: 104521816.0000 - mean_absolute_error: 8405.9082
Epoch 85/100
95/95 [==============================] - 0s 911us/step - loss: 104341688.0000 - mean_absolute_error: 8392.4727
95/95 [==============================] - 0s 929us/step - loss: 104341688.0000 - mean_absolute_error: 8392.4727
Epoch 86/100
95/95 [==============================] - 0s 990us/step - loss: 104171720.0000 - mean_absolute_error: 8379.5645
95/95 [==============================] - 0s 1ms/step - loss: 104171720.0000 - mean_absolute_error: 8379.5645  
Epoch 87/100
95/95 [==============================] - 0s 924us/step - loss: 104012760.0000 - mean_absolute_error: 8367.3330
95/95 [==============================] - 0s 943us/step - loss: 104012760.0000 - mean_absolute_error: 8367.3330
Epoch 88/100
95/95 [==============================] - 0s 939us/step - loss: 103864864.0000 - mean_absolute_error: 8355.7871
95/95 [==============================] - 0s 957us/step - loss: 103864864.0000 - mean_absolute_error: 8355.7871
Epoch 89/100
95/95 [==============================] - 0s 950us/step - loss: 103726984.0000 - mean_absolute_error: 8344.8398
95/95 [==============================] - 0s 968us/step - loss: 103726984.0000 - mean_absolute_error: 8344.8398
Epoch 90/100
95/95 [==============================] - 0s 907us/step - loss: 103598768.0000 - mean_absolute_error: 8334.3926
95/95 [==============================] - 0s 925us/step - loss: 103598768.0000 - mean_absolute_error: 8334.3926
Epoch 91/100
95/95 [==============================] - 0s 910us/step - loss: 103480888.0000 - mean_absolute_error: 8324.6816
95/95 [==============================] - 0s 928us/step - loss: 103480888.0000 - mean_absolute_error: 8324.6816
Epoch 92/100
95/95 [==============================] - 0s 898us/step - loss: 103372816.0000 - mean_absolute_error: 8315.7568
95/95 [==============================] - 0s 917us/step - loss: 103372816.0000 - mean_absolute_error: 8315.7568
Epoch 93/100
95/95 [==============================] - 0s 956us/step - loss: 103273912.0000 - mean_absolute_error: 8307.5537
95/95 [==============================] - 0s 976us/step - loss: 103273912.0000 - mean_absolute_error: 8307.5537
Epoch 94/100
95/95 [==============================] - 0s 901us/step - loss: 103183696.0000 - mean_absolute_error: 8300.0029
95/95 [==============================] - 0s 920us/step - loss: 103183696.0000 - mean_absolute_error: 8300.0029
Epoch 95/100
95/95 [==============================] - 0s 973us/step - loss: 103101664.0000 - mean_absolute_error: 8293.1074
95/95 [==============================] - 0s 991us/step - loss: 103101664.0000 - mean_absolute_error: 8293.1074
Epoch 96/100
95/95 [==============================] - 0s 922us/step - loss: 103027136.0000 - mean_absolute_error: 8286.7715
95/95 [==============================] - 0s 940us/step - loss: 103027136.0000 - mean_absolute_error: 8286.7715
Epoch 97/100
95/95 [==============================] - 0s 900us/step - loss: 102959600.0000 - mean_absolute_error: 8280.9951
95/95 [==============================] - 0s 918us/step - loss: 102959600.0000 - mean_absolute_error: 8280.9951
Epoch 98/100
95/95 [==============================] - 0s 896us/step - loss: 102898520.0000 - mean_absolute_error: 8275.7412
95/95 [==============================] - 0s 914us/step - loss: 102898520.0000 - mean_absolute_error: 8275.7412
Epoch 99/100
95/95 [==============================] - 0s 907us/step - loss: 102843344.0000 - mean_absolute_error: 8271.0107
95/95 [==============================] - 0s 925us/step - loss: 102843344.0000 - mean_absolute_error: 8271.0107
Epoch 100/100
95/95 [==============================] - 0s 934us/step - loss: 102793392.0000 - mean_absolute_error: 8266.7695
95/95 [==============================] - 0s 953us/step - loss: 102793392.0000 - mean_absolute_error: 8266.7695
[1] "step4"
[1] 4675
[1] "step5"
[1] "Score final : 11083.0297923039"
> > + + > > > > > 
── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character()
)
ℹ Use `spec()` for the full column specifications.

> 
── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character(),
  Usage = col_character()
)
ℹ Use `spec()` for the full column specifications.

> > [1] 3303
> > > > > > > + + > > + > . + > > > > > > + > 
Call:
lm(formula = Load ~ Load.1 + Load.7 + Temp + Temp_s95 + WeekDays + 
    GovernmentResponseIndex, data = train)

Residuals:
     Min       1Q   Median       3Q      Max 
-10526.2  -1304.4    147.1   1369.5   6430.4 

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
(Intercept)              1.957e+04  5.333e+02  36.687  < 2e-16 ***
Load.1                   7.261e-01  9.519e-03  76.273  < 2e-16 ***
Load.7                   8.780e-02  8.258e-03  10.633  < 2e-16 ***
Temp                    -6.524e+02  6.980e+01  -9.346  < 2e-16 ***
Temp_s95                 3.229e+02  7.229e+01   4.467 8.24e-06 ***
WeekDays                -1.309e+03  2.290e+01 -57.167  < 2e-16 ***
GovernmentResponseIndex -3.480e+01  5.039e+00  -6.906 6.05e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2134 on 3021 degrees of freedom
Multiple R-squared:  0.9626,	Adjusted R-squared:  0.9626 
F-statistic: 1.297e+04 on 6 and 3021 DF,  p-value: < 2.2e-16

> > > > [1] "Load and format  ./data/train_V2.csv"
[1] "Load and format  ./data/test_V2.csv"
> > > > > > [1] 275
[1] 1398.02
> > > . + > > > > > > > > > > Epoch 1/15
95/95 [==============================] - 0s 696us/step - loss: 3206686720.0000
95/95 [==============================] - 0s 713us/step - loss: 3206686720.0000
Epoch 2/15
95/95 [==============================] - 0s 690us/step - loss: 1537782400.0000
95/95 [==============================] - 0s 707us/step - loss: 1537782400.0000
Epoch 3/15
95/95 [==============================] - 0s 674us/step - loss: 130111384.0000
95/95 [==============================] - 0s 691us/step - loss: 130111384.0000
Epoch 4/15
95/95 [==============================] - 0s 681us/step - loss: 11360167.0000
95/95 [==============================] - 0s 697us/step - loss: 11360167.0000
Epoch 5/15
95/95 [==============================] - 0s 684us/step - loss: 11329098.0000
95/95 [==============================] - 0s 701us/step - loss: 11329098.0000
Epoch 6/15
95/95 [==============================] - 0s 683us/step - loss: 11325921.0000
95/95 [==============================] - 0s 700us/step - loss: 11325921.0000
Epoch 7/15
95/95 [==============================] - 0s 769us/step - loss: 11325512.0000
95/95 [==============================] - 0s 786us/step - loss: 11325512.0000
Epoch 8/15
95/95 [==============================] - 0s 674us/step - loss: 11272313.0000
95/95 [==============================] - 0s 691us/step - loss: 11272313.0000
Epoch 9/15
95/95 [==============================] - 0s 681us/step - loss: 11251475.0000
95/95 [==============================] - 0s 698us/step - loss: 11251475.0000
Epoch 10/15
95/95 [==============================] - 0s 673us/step - loss: 11227763.0000
95/95 [==============================] - 0s 690us/step - loss: 11227763.0000
Epoch 11/15
95/95 [==============================] - 0s 675us/step - loss: 11211960.0000
95/95 [==============================] - 0s 692us/step - loss: 11211960.0000
Epoch 12/15
95/95 [==============================] - 0s 669us/step - loss: 11184295.0000
95/95 [==============================] - 0s 686us/step - loss: 11184295.0000
Epoch 13/15
95/95 [==============================] - 0s 677us/step - loss: 11196080.0000
95/95 [==============================] - 0s 694us/step - loss: 11196080.0000
Epoch 14/15
95/95 [==============================] - 0s 704us/step - loss: 11142939.0000
95/95 [==============================] - 0s 722us/step - loss: 11142939.0000
Epoch 15/15
95/95 [==============================] - 0s 688us/step - loss: 11140456.0000
95/95 [==============================] - 0s 705us/step - loss: 11140456.0000
> > > > Error in random_forest(train, test) : 
  impossible de trouver la fonction "random_forest"
> Erreur : objet 'res' introuvable
> Erreur : objet 'res' introuvable
> > res = random_forest(train, test)
Error in random_forest(train, test) : 
  impossible de trouver la fonction "random_forest"
> res = random_forest(train, test)
Error in random_forest(train, test) : 
  impossible de trouver la fonction "random_forest"
> source("random_forest.r")
Error in source("random_forest.r") : 
  random_forest.r:17:2: '}' inattendu(e)
16:     return(list("pred.test.rf" = pred.test.rf, "rf" = rf)
17:  }
     ^
> rm(list=objects())
+ graphics.off()
+ 
+ ##pour load tout en 2 lignes, il faut juste rajouter les librairies dans libs.to.load
+ 
+ libs.to.load = c("tidyverse", "lubridate", "ranger", "pracma", "Metrics", "mgcv", "keras", "visreg", "caret", "mc2d", "opera", "abind", "randomForest", "tensorflow")
+ suppressPackageStartupMessages(sapply(libs.to.load, require, character.only = TRUE))
+ 
+ ##setwd("C:/Users/CM/code/M1/R")
+ 
+ ##load tous les fichiers en sources
+ files.sources = list.files(pattern = "*.r$")
+ files.sources = files.sources[files.sources != "main_matthieu.r"]
+ sapply(files.sources, source)
+ 
+ ##
+ plt = FALSE #(si on veut plot, mettre à TRUE)
+ path_submission = "./submissions/submission.csv"
+ 
+ 
+ ##MAIN NICOLAS
+ model = "lstm"
+ 
+ data = format_data("./data/train_V2.csv", "./data/test_V2.csv")
+ train_set = data$train_set
+ test_set = data$test_set
+ train_label = data$train_label
+ test_label = data$test_label
+ 
+ if (model == "xgboost"){
+   pred = xgboost_rte(train_set, train_label, test_set)
+ } else if(model == "lstm"){
+   pred = lstm(train_set, train_label, test_set)
+ }
+ 
+ 
+ print(paste("Score final : ", evaluate(test_label, pred), sep=""))
+ 
+ if (plt) {
+     plot(c(train_label, pred))
+ }
+ 
+ ##FIN MAIN NICOLAS 
+ 
+ 
+ train <- read_delim(file="data/train_V2.csv",delim=',')
+ test <- read_delim(file="data/test_V2.csv",delim=',')
+ 
+ reg = lm(Load~Temp, data=train)
+ 
+ if (plt){
+     plot(train$Date, train$Load, type='l')
+     par(new=T)
+     plot(train$Date, train$Temp, type='l', col='red')
+ 
+     plot(train$Temp,train$Load, col='red')
+     points(train$Temp ,reg$fitted.values)
+ }
+ 
+ train$WeekDays = days_to_numeric(train)
+ test$WeekDays = days_to_numeric(test)
+ 
+ train$Year = train$Year - 2012
+ test$Year = test$Year - 2012
+ 
+ total.time = c(1:(nrow(train)+nrow(test)))
+ length(total.time)
+ train$time = total.time[1:nrow(train)]
+ test$time = tail(total.time,nrow(test))
+ 
+ 
+ #saisonalite : annuelle
+ 
+ if (plt){
+     plot(train$Date, train$Load, type='l')
+ }
+ 
+ MA <- stats::filter(train$Load, filter = rep(1/365,365),
+              method = c("convolution"), sides = 2, circular = FALSE)
+ if (plt){
+     plot(train$Date, train$Load, type = "l", xlab = "",
+          ylab = "consumption (kw)", col = "seagreen4", lwd = 1)
+     lines(train$Date, MA, col = "red", lwd = 2)
+ }
+ 
+ ##estimation avec fourier
+ pred.fourier = fourier(train, test, plt = TRUE)
+ 
+ ##saisonalite : hebdomadaire
+ 
+ num.years = 0
+ average = 0
+ N = 8; a = 0.7 #exponential weight
+ while (365*(num.years+1) <= length(train$Date)) {
+     par(mfrow = c(1, 1))
+     dateyear = train$Date[(365*num.years+1):(365*(num.years+1))]
+     loadyear = train$Load[(365*num.years+1):(365*(num.years+1))]
+ 
+     ##plot(dateyear,loadyear,type='l')
+   
+     MAw <- stats::filter(loadyear, filter = rep(1/52,52),
+                       method = c("convolution"), sides = 2, circular = T)
+     ##plot(dateyear,loadyear, type = "l", xlab = "",
+     ##    ylab = "consumption (kw)", col = "seagreen4", lwd = 1)
+     ##lines(dateyear, MAw, col = "red", lwd = 2)
+   
+     ##plot(dateyear, loadyear - MAw, type="l")
+     expweight = (((1-a)/(1-a^8))*a^(N-(num.years+1)))
+     average = expweight*(average + (loadyear - MAw))
+     num.years = num.years + 1
+ }
+ if (plt){
+     plot(average)
+ }
+ 
+ train.to.day = train$time %% 365 + 1
+ test.to.day = test$time %% 365 + 1
+ 
+ pred.hebdo.train = average[train.to.day]
+ pred.hebdo.test = average[test.to.day]
+ 
+ pred.total.train = reg$fitted + pred.hebdo.train
+ pred.total.test = pred.fourier + pred.hebdo.test
+ 
+ 
+ if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(train$time,pred.total.train, col='red', lwd=1)
+     lines(test$time,pred.total.test, col='green', lwd=1)
+ }
+ 
+ Load = pred.total.test
+ Id = test$Id
+ submission = data.frame(Load, Id)
+ 
+ write.csv(submission, file =path_submission, row.names=F)
+ 
+ 
+ MAw.train.total <- stats::filter(train$Load.1, filter = rep(1/52,52),
+                      method = c("convolution"), sides = 2, circular = T)
+ MAw.test.total <- stats::filter(test$Load.1, filter = rep(1/52,52),
+                                  method = c("convolution"), sides = 2, circular = T)
+ 
+ train$seasonal.tendancy = train$Load.1 - MAw.train.total
+ test$seasonal.tendancy = test$Load.1 - MAw.test.total
+ 
+ train$fourier.fitted = reg$fitted
+ test$fourier.fitted = pred.fourier
+ 
+ reg2 = lm(Load~Load.1+Load.7+Temp
+ +Temp_s95+WeekDays+GovernmentResponseIndex,data=train)
+ summary(reg2)
+ 
+ ##visreg(Gam,"Temp_s95")
+ gam.test = gam_rte(train, test)
+ 
+ tmp = format_data("./data/train_V2.csv", "./data/test_V2.csv")
+ test_label = tmp$test_label
+ 
+ Gam.rmse = evaluate(test_label, gam.test)
+ 
+ ##cross-validation
+ cv = cross_validation(train, test)
+ model = cv$model; pred.test = cv$pred.test; RMSE = cv$RMSE
+ 
+ if (plt){
+     par(mfrow=c(1,1))
+     plot(train$Load,type='l', xlim=c(0,length(total.time)))
+     lines(train$time,predict(model,train), col='red', lwd=1)
+     lines(test$time,pred.test, col='green', lwd=1)
+ }
+ 
+ Load = pred.test
+ Id = 1:length(Load)
+ submission = data.frame(Load, Id)
+ 
+ write.csv(submission, file =path_submission, row.names=F)
+ 
+ ##lstm
+ 
+ pred.lstm = neural_network(train, test)
+ 
+ ##random forest
+ 
+ res = random_forest(train, test)
+ pred.test.rf = res$pred.test.rf
+ rf = res$rf
+ 
+ 
+ ##aggregation
+ 
+ add_expert = function(new_train, new_test, experts.train, experts.test){
+     experts.train = array_reshape(abind(experts.train, new_train, along=2), c(3028,1,dim(experts.train)[3]+1))
+     experts.test = cbind(experts.test, new_test)
+     return(list("experts.train" = experts.train, "experts.test" = experts.test))
+ }
+ 
   tidyverse    lubridate       ranger       pracma      Metrics         mgcv 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
       keras       visreg        caret         mc2d        opera        abind 
        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE 
randomForest   tensorflow 
        TRUE         TRUE 
> > > > > $cross_validation.r
$cross_validation.r$value
function (train, test) {
  set.seed(123)
  to.extract = rbern(n=length(train$Load),p=0.8)==T
  to.train = train[F,]
  for (j in 1:length(to.extract)) {
    if (to.extract[j]) {
      to.train[nrow(to.train)+1,] = train[j,]
    }
  }
  
  cross.train  <- train[to.train$time, ]
  cross.test <- train[-to.train$time, ]
  
  model = gam(Load~s(Load.1)+s(Load.7)+s(Temp)
               +s(Temp_s95)+s(WeekDays,k=7)
               ,data=cross.train)
  
  pred.test = predict(model,test)
  print(length(pred.test))
  N = length(test$Load.1)
  RMSE = rmse(pred.test[-N], test$Load.1[2:N]) #on connait pas le dernier
  
  print(RMSE)
  return(list("model"=model,"pred.test"=pred.test,"RMSE"=RMSE))
}

$cross_validation.r$visible
[1] FALSE


$days_to_numeric.r
$days_to_numeric.r$value
function (data) {
    data$WeekDays[data$WeekDays=='Monday']    = 1
    data$WeekDays[data$WeekDays=='Tuesday']   = 2
    data$WeekDays[data$WeekDays=='Wednesday'] = 3
    data$WeekDays[data$WeekDays=='Thursday']  = 4
    data$WeekDays[data$WeekDays=='Friday']    = 5
    data$WeekDays[data$WeekDays=='Saturday']  = 6
    data$WeekDays[data$WeekDays=='Sunday']    = 7
    data$WeekDays = as.numeric(data$WeekDays)
    return (data$WeekDays)
}

$days_to_numeric.r$visible
[1] FALSE


$evaluate.r
$evaluate.r$value
function(test_label, predicted_set){
    
    return(rmse(test_label, predicted_set))
}

$evaluate.r$visible
[1] FALSE


$format_data.r
$format_data.r$value
function(file_train, file_test) {

    print(paste("Load and format " , file_train, sep = " "))
    train_set = read_csv(file_train, col_types = cols())

    train_set$WeekDays[train_set$WeekDays == "Monday"] <- 1
    train_set$WeekDays[train_set$WeekDays == "Tuesday"] <- 2
    train_set$WeekDays[train_set$WeekDays == "Wednesday"] <- 3
    train_set$WeekDays[train_set$WeekDays == "Thursday"] <- 4
    train_set$WeekDays[train_set$WeekDays == "Friday"] <- 5
    train_set$WeekDays[train_set$WeekDays == "Saturday"] <- 6
    train_set$WeekDays[train_set$WeekDays == "Sunday"] <- 7
    train_set$WeekDays = as.integer(train_set$WeekDays)

    train_set$Year = NULL
    train_set$Date = NULL
    train_label = data.matrix(train_set$Load)


    train_set$Load = NULL
    
    train_set = data.matrix(train_set)


    print(paste("Load and format " , file_test, sep = " "))    
    test_set = read_csv(file_test, col_types = cols())

    test_set$WeekDays[test_set$WeekDays == "Monday"] <- 1
    test_set$WeekDays[test_set$WeekDays == "Tuesday"] <- 2
    test_set$WeekDays[test_set$WeekDays == "Wednesday"] <- 3
    test_set$WeekDays[test_set$WeekDays == "Thursday"] <- 4
    test_set$WeekDays[test_set$WeekDays == "Friday"] <- 5
    test_set$WeekDays[test_set$WeekDays == "Saturday"] <- 6
    test_set$WeekDays[test_set$WeekDays == "Sunday"] <- 7
    test_set$WeekDays = as.integer(test_set$WeekDays)

    test_label = test_set$Load.1
    tmp = test_set$Load.1
    for (i in c(1:(length(tmp)-1))){
        test_label[i] = tmp[i+1]
    }
    
    test_set$Year = NULL
    test_set$Date = NULL
    test_set$Id = NULL
    test_set$Usage = NULL
    test_set = data.matrix(test_set)
    

    test_set = data.matrix(test_set)
    return(list("train_set" = train_set, "train_label" = train_label, "test_set" = test_set, "test_label" = test_label))
}

$format_data.r$visible
[1] FALSE


$fourier.r
$fourier.r$value
function(train, test, plt = FALSE){
    total.time = c(1:(nrow(train)+nrow(test)))
    length(total.time)
    train$time = total.time[1:nrow(train)]
    test$time = tail(total.time,nrow(test))

    fourier.make.matrix = function (t, k, period) {
        w = 2*pi/period
        ret = cbind(cos(w*t), sin(w*t))
        
        for(i in c(2:K))
        {
            ret = cbind(ret, cos(i*w*t), sin(i*w*t))
        }
        return (ret)
    }

    K = 5; period = 365
    fourier.train = fourier.make.matrix(train$time, K, period)
    fourier.test = fourier.make.matrix(test$time, K, period)

    fourier.train.df = data.frame(train$Load,fourier.train)
    fourier.test.df = data.frame(fourier.test)

    reg = lm(train.Load ~., data=fourier.train.df)

    pred.fourier = predict(reg, newdata=fourier.test.df)
    total.fourier = c(reg$fitted,pred.fourier)
    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(reg$fitted,col='red', lwd=2)
        lines(test$time,pred.fourier,col='green', lwd=2)
    }
    return(pred.fourier)
}

$fourier.r$visible
[1] FALSE


$GAM.r
$GAM.r$value
function(train, test, plt = FALSE){
    Gam <- gam(Load~s(Load.1)+s(Load.7)+s(Temp)
               +s(Temp_s95)+s(WeekDays,k=7)
               +s(GovernmentResponseIndex)
              ,data=train)
    summary(Gam)


    gam.train = predict(Gam, newdata=train)
    gam.test = predict(Gam, newdata=test)

    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(train$time,Gam$fit, col='red', lwd=1)
        lines(test$time,gam.test, col='green', lwd=1)
    }

    return(gam.test)
}

$GAM.r$visible
[1] FALSE


$lstm.r
$lstm.r$value
function(train_set, train_label, test_set){
  N = length(train_set[,'Load.1'])
  nb_var = 17
  y = train_label
  x = array(train_set, dim = c(N, nb_var, 1))
  x_test = array(test_set,dim = c(length(test_set[,'Load.1']), nb_var, 1))
  print("step1")
  model = keras_model_sequential() %>%   
    #layer_lstm(units=128, input_shape=c(nb_var, 1), activation="relu", return_sequences = TRUE) %>% 
    layer_dense(units=64, activation = "relu") %>%  
    layer_dense(units=32) %>%  
    layer_dense(units=1, activation = "linear")
  print("step2")
  model %>% compile(loss = 'mse',
                    optimizer = 'adam',
                    metrics = list("mean_absolute_error")
  )
  print("step3")
  model %>% fit(x,y, epochs=100, batch_size=32, shuffle = FALSE)
  print("step4")
  print(length(x_test))
  pred = model %>% predict(x_test)
  print("step5")
  return(pred)
    
}

$lstm.r$visible
[1] FALSE


$main.r
NULL

$neural_network.r
$neural_network.r$value
function(train, test, plt = FALSE){
    labels = train$Load
    train.lstm = train[,-c(1,2,22,23)]
    test.lstm = test[,-c(1,20,21,23,24)]

    lstm = function(train_set, train_label, test_set) {
        y = train_label
        x = data.matrix(train_set)
        x_test = data.matrix(test_set)
        model = keras_model_sequential() %>%
            layer_dense(units = 19, activation = 'relu', input_shape = c(19)) %>%
            layer_dense(units = 12, activation = 'relu') %>%
            layer_dense(units = 6, activation = 'relu') %>%
            layer_dense(units=1, activation = "linear")
        
        model %>% compile(loss = 'mse',
                          optimizer = optimizer_adam(0.0005))
        
        model %>% fit(x,y, epochs=15)

        pred.train = model %>% predict(x)
        pred.test = model %>% predict(x_test)

        return(list("train"=pred.train,"test"=pred.test))
    }


    pred.lstm = lstm(train.lstm, labels, test.lstm)

    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(test$time,pred.lstm$test, col='green', lwd=1)
    }

    N = length(test$Load.1)
    RMSE = rmse(pred.lstm$test[-N], test$Load.1[2:N])
    RMSE
    return(pred.lstm)
}

$neural_network.r$visible
[1] FALSE


$random_forest.r
$random_forest.r$value
function(train, test, plt = FALSE){

    rf = randomForest(Load ~ ., data=train, mtry=3,
                      importance=TRUE, na.action=na.omit)
    pred.test.rf = predict(rf,test)

    if (plt){
        par(mfrow=c(1,1))
        plot(train$Load,type='l', xlim=c(0,length(total.time)))
        lines(test$time,pred.test.rf, col='green', lwd=1)
    }

    N = length(test$Load.1)
    RMSE = rmse(pred.test.rf[-N], test$Load.1[2:N])
    RMSE
    return(list("pred.test.rf" = pred.test.rf, "rf" = rf))
 }

$random_forest.r$visible
[1] FALSE


$scale.r
$scale.r$value
function(scaled, scaler, feature_range = c(0, 1)){
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for( i in 1:t){
    X = (scaled[i]- mins)/(maxs - mins)
    rawValues = X *(max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

$scale.r$visible
[1] FALSE


$xgboost.r
$xgboost.r$value
function(train_set, train_label, test_set){
  param = list(booster = "gblinear", objective = "reg:squarederror", eval_metric = "rmse", lambda = 0.0003, alpha = 0.0003, nthread = 2, eta = 0.1)
  
  print("Model : XGBOOST")
  
  xgbmodel = xgboost(data = train_set, label = train_label, nrounds = 200, params = param, verbose = 0)
  
  pred = predict(xgbmodel, test_set)
  
  return(pred)
}

$xgboost.r$visible
[1] FALSE


> > > > > > > > > > [1] "Load and format  ./data/train_V2.csv"
[1] "Load and format  ./data/test_V2.csv"
> > > > > > . + > [1] "step1"
[1] "step2"
[1] "step3"
Epoch 1/100
95/95 [==============================] - 0s 969us/step - loss: 2879972096.0000 - mean_absolute_error: 50798.9609
95/95 [==============================] - 0s 988us/step - loss: 2879972096.0000 - mean_absolute_error: 50798.9609
Epoch 2/100
95/95 [==============================] - 0s 931us/step - loss: 2740345856.0000 - mean_absolute_error: 48557.0352
95/95 [==============================] - 0s 949us/step - loss: 2740345856.0000 - mean_absolute_error: 48557.0352
Epoch 3/100
95/95 [==============================] - 0s 905us/step - loss: 2739357440.0000 - mean_absolute_error: 48554.9102
95/95 [==============================] - 0s 923us/step - loss: 2739357440.0000 - mean_absolute_error: 48554.9102
Epoch 4/100
95/95 [==============================] - 0s 924us/step - loss: 2737440512.0000 - mean_absolute_error: 48554.2500
95/95 [==============================] - 0s 943us/step - loss: 2737440512.0000 - mean_absolute_error: 48554.2500
Epoch 5/100
95/95 [==============================] - 0s 912us/step - loss: 2734628352.0000 - mean_absolute_error: 48550.8047
95/95 [==============================] - 0s 930us/step - loss: 2734628352.0000 - mean_absolute_error: 48550.8047
Epoch 6/100
95/95 [==============================] - 0s 930us/step - loss: 2731017472.0000 - mean_absolute_error: 48528.6602
95/95 [==============================] - 0s 954us/step - loss: 2731017472.0000 - mean_absolute_error: 48528.6602
Epoch 7/100
95/95 [==============================] - 0s 912us/step - loss: 2726283776.0000 - mean_absolute_error: 48492.0117
95/95 [==============================] - 0s 930us/step - loss: 2726283776.0000 - mean_absolute_error: 48492.0117
Epoch 8/100
95/95 [==============================] - 0s 905us/step - loss: 2719321600.0000 - mean_absolute_error: 48390.9961
95/95 [==============================] - 0s 923us/step - loss: 2719321600.0000 - mean_absolute_error: 48390.9961
Epoch 9/100
95/95 [==============================] - 0s 913us/step - loss: 2709944576.0000 - mean_absolute_error: 48312.3008
95/95 [==============================] - 0s 931us/step - loss: 2709944576.0000 - mean_absolute_error: 48312.3008
Epoch 10/100
95/95 [==============================] - 0s 913us/step - loss: 2695269120.0000 - mean_absolute_error: 48166.1719
95/95 [==============================] - 0s 932us/step - loss: 2695269120.0000 - mean_absolute_error: 48166.1719
Epoch 11/100
95/95 [==============================] - 0s 913us/step - loss: 2673327104.0000 - mean_absolute_error: 47953.5742
95/95 [==============================] - 0s 931us/step - loss: 2673327104.0000 - mean_absolute_error: 47953.5742
Epoch 12/100
95/95 [==============================] - 0s 932us/step - loss: 2641661696.0000 - mean_absolute_error: 47673.4570
95/95 [==============================] - 0s 951us/step - loss: 2641661696.0000 - mean_absolute_error: 47673.4570
Epoch 13/100
95/95 [==============================] - 0s 950us/step - loss: 2594999296.0000 - mean_absolute_error: 47270.2812
95/95 [==============================] - 0s 968us/step - loss: 2594999296.0000 - mean_absolute_error: 47270.2812
Epoch 14/100
95/95 [==============================] - 0s 902us/step - loss: 2530115840.0000 - mean_absolute_error: 46681.9609
95/95 [==============================] - 0s 919us/step - loss: 2530115840.0000 - mean_absolute_error: 46681.9609
Epoch 15/100
95/95 [==============================] - 0s 926us/step - loss: 2441387264.0000 - mean_absolute_error: 45809.3320
95/95 [==============================] - 0s 944us/step - loss: 2441387264.0000 - mean_absolute_error: 45809.3320
Epoch 16/100
95/95 [==============================] - 0s 977us/step - loss: 2337203712.0000 - mean_absolute_error: 45079.1875
95/95 [==============================] - 0s 997us/step - loss: 2337203712.0000 - mean_absolute_error: 45079.1875
Epoch 17/100
95/95 [==============================] - 0s 928us/step - loss: 2201764352.0000 - mean_absolute_error: 43590.6250
95/95 [==============================] - 0s 946us/step - loss: 2201764352.0000 - mean_absolute_error: 43590.6250
Epoch 18/100
95/95 [==============================] - 0s 906us/step - loss: 2048607360.0000 - mean_absolute_error: 42110.3828
95/95 [==============================] - 0s 925us/step - loss: 2048607360.0000 - mean_absolute_error: 42110.3828
Epoch 19/100
95/95 [==============================] - 0s 972us/step - loss: 1894235264.0000 - mean_absolute_error: 40677.9414
95/95 [==============================] - 0s 991us/step - loss: 1894235264.0000 - mean_absolute_error: 40677.9414
Epoch 20/100
95/95 [==============================] - 0s 924us/step - loss: 1699837312.0000 - mean_absolute_error: 38054.8359
95/95 [==============================] - 0s 942us/step - loss: 1699837312.0000 - mean_absolute_error: 38054.8359
Epoch 21/100
95/95 [==============================] - 0s 907us/step - loss: 1531004288.0000 - mean_absolute_error: 36236.7734
95/95 [==============================] - 0s 927us/step - loss: 1531004288.0000 - mean_absolute_error: 36236.7734
Epoch 22/100
95/95 [==============================] - 0s 904us/step - loss: 1346939648.0000 - mean_absolute_error: 33893.0234
95/95 [==============================] - 0s 922us/step - loss: 1346939648.0000 - mean_absolute_error: 33893.0234
Epoch 23/100
95/95 [==============================] - 0s 900us/step - loss: 1167016832.0000 - mean_absolute_error: 31402.6270
95/95 [==============================] - 0s 920us/step - loss: 1167016832.0000 - mean_absolute_error: 31402.6270
Epoch 24/100
95/95 [==============================] - 0s 904us/step - loss: 991708608.0000 - mean_absolute_error: 28730.2520
95/95 [==============================] - 0s 922us/step - loss: 991708608.0000 - mean_absolute_error: 28730.2520
Epoch 25/100
95/95 [==============================] - 0s 908us/step - loss: 823477248.0000 - mean_absolute_error: 25856.8965
95/95 [==============================] - 0s 927us/step - loss: 823477248.0000 - mean_absolute_error: 25856.8965
Epoch 26/100
95/95 [==============================] - 0s 1ms/step - loss: 697971136.0000 - mean_absolute_error: 23648.1484
95/95 [==============================] - 0s 1ms/step - loss: 697971136.0000 - mean_absolute_error: 23648.1484
Epoch 27/100
95/95 [==============================] - 0s 915us/step - loss: 577284288.0000 - mean_absolute_error: 21190.9902
95/95 [==============================] - 0s 945us/step - loss: 577284288.0000 - mean_absolute_error: 21190.9902
Epoch 28/100
95/95 [==============================] - 0s 947us/step - loss: 450568256.0000 - mean_absolute_error: 17814.6621
95/95 [==============================] - 0s 966us/step - loss: 450568256.0000 - mean_absolute_error: 17814.6621
Epoch 29/100
95/95 [==============================] - 0s 908us/step - loss: 399809632.0000 - mean_absolute_error: 16293.6631
95/95 [==============================] - 0s 926us/step - loss: 399809632.0000 - mean_absolute_error: 16293.6631
Epoch 30/100
95/95 [==============================] - 0s 899us/step - loss: 287360640.0000 - mean_absolute_error: 13191.7520
95/95 [==============================] - 0s 917us/step - loss: 287360640.0000 - mean_absolute_error: 13191.7520
Epoch 31/100
95/95 [==============================] - 0s 909us/step - loss: 232144720.0000 - mean_absolute_error: 11464.3457
95/95 [==============================] - 0s 927us/step - loss: 232144720.0000 - mean_absolute_error: 11464.3457
Epoch 32/100
95/95 [==============================] - 0s 1ms/step - loss: 208540896.0000 - mean_absolute_error: 10843.9639
95/95 [==============================] - 0s 1ms/step - loss: 208540896.0000 - mean_absolute_error: 10843.9639
Epoch 33/100
95/95 [==============================] - 0s 917us/step - loss: 163269920.0000 - mean_absolute_error: 9248.9951
95/95 [==============================] - 0s 935us/step - loss: 163269920.0000 - mean_absolute_error: 9248.9951
Epoch 34/100
95/95 [==============================] - 0s 906us/step - loss: 146895424.0000 - mean_absolute_error: 8933.7559
95/95 [==============================] - 0s 926us/step - loss: 146895424.0000 - mean_absolute_error: 8933.7559
Epoch 35/100
95/95 [==============================] - 0s 901us/step - loss: 133009616.0000 - mean_absolute_error: 8662.1953
95/95 [==============================] - 0s 919us/step - loss: 133009616.0000 - mean_absolute_error: 8662.1953
Epoch 36/100
95/95 [==============================] - 0s 901us/step - loss: 124341792.0000 - mean_absolute_error: 8604.9277
95/95 [==============================] - 0s 920us/step - loss: 124341792.0000 - mean_absolute_error: 8604.9277
Epoch 37/100
95/95 [==============================] - 0s 949us/step - loss: 120727104.0000 - mean_absolute_error: 8686.7168
95/95 [==============================] - 0s 967us/step - loss: 120727104.0000 - mean_absolute_error: 8686.7168
Epoch 38/100
95/95 [==============================] - 0s 928us/step - loss: 115693800.0000 - mean_absolute_error: 8616.0430
95/95 [==============================] - 0s 947us/step - loss: 115693800.0000 - mean_absolute_error: 8616.0430
Epoch 39/100
95/95 [==============================] - 0s 913us/step - loss: 114532480.0000 - mean_absolute_error: 8687.4160
95/95 [==============================] - 0s 931us/step - loss: 114532480.0000 - mean_absolute_error: 8687.4160
Epoch 40/100
95/95 [==============================] - 0s 908us/step - loss: 113265456.0000 - mean_absolute_error: 8711.1787
95/95 [==============================] - 0s 926us/step - loss: 113265456.0000 - mean_absolute_error: 8711.1787
Epoch 41/100
95/95 [==============================] - 0s 903us/step - loss: 112483552.0000 - mean_absolute_error: 8732.3418
95/95 [==============================] - 0s 922us/step - loss: 112483552.0000 - mean_absolute_error: 8732.3418
Epoch 42/100
95/95 [==============================] - 0s 904us/step - loss: 111936520.0000 - mean_absolute_error: 8746.8564
95/95 [==============================] - 0s 922us/step - loss: 111936520.0000 - mean_absolute_error: 8746.8564
Epoch 43/100
95/95 [==============================] - 0s 909us/step - loss: 111522848.0000 - mean_absolute_error: 8755.4238
95/95 [==============================] - 0s 927us/step - loss: 111522848.0000 - mean_absolute_error: 8755.4238
Epoch 44/100
95/95 [==============================] - 0s 945us/step - loss: 111186608.0000 - mean_absolute_error: 8759.0645
95/95 [==============================] - 0s 964us/step - loss: 111186608.0000 - mean_absolute_error: 8759.0645
Epoch 45/100
95/95 [==============================] - 0s 993us/step - loss: 110893840.0000 - mean_absolute_error: 8758.9463
95/95 [==============================] - 0s 1ms/step - loss: 110893840.0000 - mean_absolute_error: 8758.9463  
Epoch 46/100
95/95 [==============================] - 0s 933us/step - loss: 110625176.0000 - mean_absolute_error: 8755.9463
95/95 [==============================] - 0s 951us/step - loss: 110625176.0000 - mean_absolute_error: 8755.9463
Epoch 47/100
95/95 [==============================] - 0s 959us/step - loss: 110369064.0000 - mean_absolute_error: 8750.6553
95/95 [==============================] - 0s 979us/step - loss: 110369064.0000 - mean_absolute_error: 8750.6553
Epoch 48/100
95/95 [==============================] - 0s 947us/step - loss: 110118736.0000 - mean_absolute_error: 8743.5918
95/95 [==============================] - 0s 966us/step - loss: 110118736.0000 - mean_absolute_error: 8743.5918
Epoch 49/100
95/95 [==============================] - 0s 936us/step - loss: 109870480.0000 - mean_absolute_error: 8735.1709
95/95 [==============================] - 0s 954us/step - loss: 109870480.0000 - mean_absolute_error: 8735.1709
Epoch 50/100
95/95 [==============================] - 0s 906us/step - loss: 109621544.0000 - mean_absolute_error: 8725.6592
95/95 [==============================] - 0s 925us/step - loss: 109621544.0000 - mean_absolute_error: 8725.6592
Epoch 51/100
95/95 [==============================] - 0s 949us/step - loss: 109370440.0000 - mean_absolute_error: 8715.2764
95/95 [==============================] - 0s 968us/step - loss: 109370440.0000 - mean_absolute_error: 8715.2764
Epoch 52/100
95/95 [==============================] - 0s 907us/step - loss: 109116608.0000 - mean_absolute_error: 8704.1885
95/95 [==============================] - 0s 926us/step - loss: 109116608.0000 - mean_absolute_error: 8704.1885
Epoch 53/100
95/95 [==============================] - 0s 902us/step - loss: 108859368.0000 - mean_absolute_error: 8692.4932
95/95 [==============================] - 0s 922us/step - loss: 108859368.0000 - mean_absolute_error: 8692.4932
Epoch 54/100
95/95 [==============================] - 0s 908us/step - loss: 108598704.0000 - mean_absolute_error: 8680.2930
95/95 [==============================] - 0s 926us/step - loss: 108598704.0000 - mean_absolute_error: 8680.2930
Epoch 55/100
95/95 [==============================] - 0s 915us/step - loss: 108334592.0000 - mean_absolute_error: 8667.6396
95/95 [==============================] - 0s 933us/step - loss: 108334592.0000 - mean_absolute_error: 8667.6396
Epoch 56/100
95/95 [==============================] - 0s 906us/step - loss: 108067176.0000 - mean_absolute_error: 8654.5684
95/95 [==============================] - 0s 925us/step - loss: 108067176.0000 - mean_absolute_error: 8654.5684
Epoch 57/100
95/95 [==============================] - 0s 916us/step - loss: 107796864.0000 - mean_absolute_error: 8641.0928
95/95 [==============================] - 0s 935us/step - loss: 107796864.0000 - mean_absolute_error: 8641.0928
Epoch 58/100
95/95 [==============================] - 0s 940us/step - loss: 107523840.0000 - mean_absolute_error: 8627.2539
95/95 [==============================] - 0s 958us/step - loss: 107523840.0000 - mean_absolute_error: 8627.2539
Epoch 59/100
95/95 [==============================] - 0s 935us/step - loss: 107248768.0000 - mean_absolute_error: 8613.0957
95/95 [==============================] - 0s 953us/step - loss: 107248768.0000 - mean_absolute_error: 8613.0957
Epoch 60/100
95/95 [==============================] - 0s 936us/step - loss: 106971984.0000 - mean_absolute_error: 8598.6875
95/95 [==============================] - 0s 955us/step - loss: 106971984.0000 - mean_absolute_error: 8598.6875
Epoch 61/100
95/95 [==============================] - 0s 907us/step - loss: 106694168.0000 - mean_absolute_error: 8583.9980
95/95 [==============================] - 0s 926us/step - loss: 106694168.0000 - mean_absolute_error: 8583.9980
Epoch 62/100
95/95 [==============================] - 0s 912us/step - loss: 106415856.0000 - mean_absolute_error: 8569.0674
95/95 [==============================] - 0s 931us/step - loss: 106415856.0000 - mean_absolute_error: 8569.0674
Epoch 63/100
95/95 [==============================] - 0s 986us/step - loss: 106137848.0000 - mean_absolute_error: 8553.9053
95/95 [==============================] - 0s 1ms/step - loss: 106137848.0000 - mean_absolute_error: 8553.9053  
Epoch 64/100
95/95 [==============================] - 0s 949us/step - loss: 105860392.0000 - mean_absolute_error: 8538.5254
95/95 [==============================] - 0s 967us/step - loss: 105860392.0000 - mean_absolute_error: 8538.5254
Epoch 65/100
95/95 [==============================] - 0s 907us/step - loss: 105584624.0000 - mean_absolute_error: 8522.9863
95/95 [==============================] - 0s 925us/step - loss: 105584624.0000 - mean_absolute_error: 8522.9863
Epoch 66/100
95/95 [==============================] - 0s 906us/step - loss: 105310696.0000 - mean_absolute_error: 8507.2783
95/95 [==============================] - 0s 925us/step - loss: 105310696.0000 - mean_absolute_error: 8507.2783
Epoch 67/100
95/95 [==============================] - 0s 916us/step - loss: 105038688.0000 - mean_absolute_error: 8491.3789
95/95 [==============================] - 0s 933us/step - loss: 105038688.0000 - mean_absolute_error: 8491.3789
Epoch 68/100
95/95 [==============================] - 0s 903us/step - loss: 104768880.0000 - mean_absolute_error: 8475.3086
95/95 [==============================] - 0s 921us/step - loss: 104768880.0000 - mean_absolute_error: 8475.3086
Epoch 69/100
95/95 [==============================] - 0s 893us/step - loss: 104503216.0000 - mean_absolute_error: 8459.1582
95/95 [==============================] - 0s 911us/step - loss: 104503216.0000 - mean_absolute_error: 8459.1582
Epoch 70/100
95/95 [==============================] - 0s 963us/step - loss: 104241904.0000 - mean_absolute_error: 8442.9150
95/95 [==============================] - 0s 982us/step - loss: 104241904.0000 - mean_absolute_error: 8442.9150
Epoch 71/100
95/95 [==============================] - 0s 960us/step - loss: 103984880.0000 - mean_absolute_error: 8426.5869
95/95 [==============================] - 0s 979us/step - loss: 103984880.0000 - mean_absolute_error: 8426.5869
Epoch 72/100
95/95 [==============================] - 0s 922us/step - loss: 103733936.0000 - mean_absolute_error: 8410.3154
95/95 [==============================] - 0s 940us/step - loss: 103733936.0000 - mean_absolute_error: 8410.3154
Epoch 73/100
95/95 [==============================] - 0s 910us/step - loss: 103489328.0000 - mean_absolute_error: 8394.1494
95/95 [==============================] - 0s 929us/step - loss: 103489328.0000 - mean_absolute_error: 8394.1494
Epoch 74/100
95/95 [==============================] - 0s 904us/step - loss: 103250712.0000 - mean_absolute_error: 8378.0293
95/95 [==============================] - 0s 922us/step - loss: 103250712.0000 - mean_absolute_error: 8378.0293
Epoch 75/100
95/95 [==============================] - 0s 904us/step - loss: 103020312.0000 - mean_absolute_error: 8362.1621
95/95 [==============================] - 0s 922us/step - loss: 103020312.0000 - mean_absolute_error: 8362.1621
Epoch 76/100
95/95 [==============================] - 0s 910us/step - loss: 102798544.0000 - mean_absolute_error: 8346.5879
95/95 [==============================] - 0s 928us/step - loss: 102798544.0000 - mean_absolute_error: 8346.5879
Epoch 77/100
95/95 [==============================] - 0s 968us/step - loss: 102584544.0000 - mean_absolute_error: 8331.2168
95/95 [==============================] - 0s 987us/step - loss: 102584544.0000 - mean_absolute_error: 8331.2168
Epoch 78/100
95/95 [==============================] - 0s 958us/step - loss: 102380200.0000 - mean_absolute_error: 8316.3145
95/95 [==============================] - 0s 979us/step - loss: 102380200.0000 - mean_absolute_error: 8316.3145
Epoch 79/100
95/95 [==============================] - 0s 921us/step - loss: 102185600.0000 - mean_absolute_error: 8301.9092
95/95 [==============================] - 0s 940us/step - loss: 102185600.0000 - mean_absolute_error: 8301.9092
Epoch 80/100
95/95 [==============================] - 0s 896us/step - loss: 102001392.0000 - mean_absolute_error: 8288.0361
95/95 [==============================] - 0s 914us/step - loss: 102001392.0000 - mean_absolute_error: 8288.0361
Epoch 81/100
95/95 [==============================] - 0s 966us/step - loss: 101827824.0000 - mean_absolute_error: 8274.7109
95/95 [==============================] - 0s 988us/step - loss: 101827824.0000 - mean_absolute_error: 8274.7109
Epoch 82/100
95/95 [==============================] - 0s 930us/step - loss: 101664688.0000 - mean_absolute_error: 8261.9336
95/95 [==============================] - 0s 948us/step - loss: 101664688.0000 - mean_absolute_error: 8261.9336
Epoch 83/100
95/95 [==============================] - 0s 919us/step - loss: 101511432.0000 - mean_absolute_error: 8249.7188
95/95 [==============================] - 0s 937us/step - loss: 101511432.0000 - mean_absolute_error: 8249.7188
Epoch 84/100
95/95 [==============================] - 0s 907us/step - loss: 101369280.0000 - mean_absolute_error: 8238.1680
95/95 [==============================] - 0s 925us/step - loss: 101369280.0000 - mean_absolute_error: 8238.1680
Epoch 85/100
95/95 [==============================] - 0s 901us/step - loss: 101237688.0000 - mean_absolute_error: 8227.2217
95/95 [==============================] - 0s 919us/step - loss: 101237688.0000 - mean_absolute_error: 8227.2217
Epoch 86/100
95/95 [==============================] - 0s 900us/step - loss: 101116272.0000 - mean_absolute_error: 8216.9033
95/95 [==============================] - 0s 919us/step - loss: 101116272.0000 - mean_absolute_error: 8216.9033
Epoch 87/100
95/95 [==============================] - 0s 904us/step - loss: 101004240.0000 - mean_absolute_error: 8207.1924
95/95 [==============================] - 0s 922us/step - loss: 101004240.0000 - mean_absolute_error: 8207.1924
Epoch 88/100
95/95 [==============================] - 0s 915us/step - loss: 100902336.0000 - mean_absolute_error: 8198.2764
95/95 [==============================] - 0s 935us/step - loss: 100902336.0000 - mean_absolute_error: 8198.2764
Epoch 89/100
95/95 [==============================] - 0s 940us/step - loss: 100809432.0000 - mean_absolute_error: 8190.0942
95/95 [==============================] - 0s 959us/step - loss: 100809432.0000 - mean_absolute_error: 8190.0942
Epoch 90/100
95/95 [==============================] - 0s 950us/step - loss: 100724608.0000 - mean_absolute_error: 8182.4893
95/95 [==============================] - 0s 969us/step - loss: 100724608.0000 - mean_absolute_error: 8182.4893
Epoch 91/100
95/95 [==============================] - 0s 900us/step - loss: 100648600.0000 - mean_absolute_error: 8175.5601
95/95 [==============================] - 0s 918us/step - loss: 100648600.0000 - mean_absolute_error: 8175.5601
Epoch 92/100
95/95 [==============================] - 0s 923us/step - loss: 100580240.0000 - mean_absolute_error: 8169.2622
95/95 [==============================] - 0s 941us/step - loss: 100580240.0000 - mean_absolute_error: 8169.2622
Epoch 93/100
95/95 [==============================] - 0s 920us/step - loss: 100519152.0000 - mean_absolute_error: 8163.5615
95/95 [==============================] - 0s 938us/step - loss: 100519152.0000 - mean_absolute_error: 8163.5615
Epoch 94/100
95/95 [==============================] - 0s 3ms/step - loss: 100464640.0000 - mean_absolute_error: 8158.4302
95/95 [==============================] - 0s 3ms/step - loss: 100464640.0000 - mean_absolute_error: 8158.4302
Epoch 95/100
95/95 [==============================] - 0s 948us/step - loss: 100416120.0000 - mean_absolute_error: 8153.8481
95/95 [==============================] - 0s 968us/step - loss: 100416120.0000 - mean_absolute_error: 8153.8481
Epoch 96/100
95/95 [==============================] - 0s 905us/step - loss: 100373008.0000 - mean_absolute_error: 8149.7627
95/95 [==============================] - 0s 923us/step - loss: 100373008.0000 - mean_absolute_error: 8149.7627
Epoch 97/100
95/95 [==============================] - 0s 922us/step - loss: 100334624.0000 - mean_absolute_error: 8146.1475
95/95 [==============================] - 0s 940us/step - loss: 100334624.0000 - mean_absolute_error: 8146.1475
Epoch 98/100
95/95 [==============================] - 0s 922us/step - loss: 100300248.0000 - mean_absolute_error: 8142.8901
95/95 [==============================] - 0s 943us/step - loss: 100300248.0000 - mean_absolute_error: 8142.8901
Epoch 99/100
95/95 [==============================] - 0s 914us/step - loss: 100269744.0000 - mean_absolute_error: 8140.0127
95/95 [==============================] - 0s 932us/step - loss: 100269744.0000 - mean_absolute_error: 8140.0127
Epoch 100/100
95/95 [==============================] - 0s 897us/step - loss: 100242520.0000 - mean_absolute_error: 8137.4707
95/95 [==============================] - 0s 915us/step - loss: 100242520.0000 - mean_absolute_error: 8137.4707
[1] "step4"
[1] 4675
[1] "step5"
> > > [1] "Score final : 11148.9079732126"
> > + + > > > > > 
── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character()
)
ℹ Use `spec()` for the full column specifications.

> 
── Column specification ─────────────────────────────────────────────────────────────────
cols(
  .default = col_double(),
  Date = col_date(format = ""),
  WeekDays = col_character(),
  Usage = col_character()
)
ℹ Use `spec()` for the full column specifications.

> > > > . + > > > > > > > > > [1] 3303
> > > > > > > + + > > + > . + > > > Call:
lm(formula = Load ~ Load.1 + Load.7 + Temp + Temp_s95 + WeekDays + 
    GovernmentResponseIndex, data = train)

Residuals:
     Min       1Q   Median       3Q      Max 
-10526.2  -1304.4    147.1   1369.5   6430.4 

Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
(Intercept)              1.957e+04  5.333e+02  36.687  < 2e-16 ***
Load.1                   7.261e-01  9.519e-03  76.273  < 2e-16 ***
Load.7                   8.780e-02  8.258e-03  10.633  < 2e-16 ***
Temp                    -6.524e+02  6.980e+01  -9.346  < 2e-16 ***
Temp_s95                 3.229e+02  7.229e+01   4.467 8.24e-06 ***
WeekDays                -1.309e+03  2.290e+01 -57.167  < 2e-16 ***
GovernmentResponseIndex -3.480e+01  5.039e+00  -6.906 6.05e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2134 on 3021 degrees of freedom
Multiple R-squared:  0.9626,	Adjusted R-squared:  0.9626 
F-statistic: 1.297e+04 on 6 and 3021 DF,  p-value: < 2.2e-16

> > > > > [1] "Load and format  ./data/train_V2.csv"
[1] "Load and format  ./data/test_V2.csv"
> > > > > > [1] 275
[1] 1398.02
> > > . + > > > > > > > > > > Epoch 1/15
95/95 [==============================] - 0s 702us/step - loss: 2211858944.0000
95/95 [==============================] - 0s 719us/step - loss: 2211858944.0000
Epoch 2/15
95/95 [==============================] - 0s 682us/step - loss: 277506720.0000
95/95 [==============================] - 0s 700us/step - loss: 277506720.0000
Epoch 3/15
95/95 [==============================] - 0s 670us/step - loss: 12131720.0000
95/95 [==============================] - 0s 688us/step - loss: 12131720.0000
Epoch 4/15
95/95 [==============================] - 0s 704us/step - loss: 12073290.0000
95/95 [==============================] - 0s 721us/step - loss: 12073290.0000
Epoch 5/15
95/95 [==============================] - 0s 685us/step - loss: 12039374.0000
95/95 [==============================] - 0s 703us/step - loss: 12039374.0000
Epoch 6/15
95/95 [==============================] - 0s 705us/step - loss: 11999642.0000
95/95 [==============================] - 0s 722us/step - loss: 11999642.0000
Epoch 7/15
95/95 [==============================] - 0s 678us/step - loss: 11966045.0000
95/95 [==============================] - 0s 695us/step - loss: 11966045.0000
Epoch 8/15
95/95 [==============================] - 0s 742us/step - loss: 11911267.0000
95/95 [==============================] - 0s 759us/step - loss: 11911267.0000
Epoch 9/15
95/95 [==============================] - 0s 676us/step - loss: 11876869.0000
95/95 [==============================] - 0s 693us/step - loss: 11876869.0000
Epoch 10/15
95/95 [==============================] - 0s 675us/step - loss: 11840704.0000
95/95 [==============================] - 0s 692us/step - loss: 11840704.0000
Epoch 11/15
95/95 [==============================] - 0s 677us/step - loss: 11809770.0000
95/95 [==============================] - 0s 694us/step - loss: 11809770.0000
Epoch 12/15
95/95 [==============================] - 0s 679us/step - loss: 11748905.0000
95/95 [==============================] - 0s 696us/step - loss: 11748905.0000
Epoch 13/15
95/95 [==============================] - 0s 676us/step - loss: 11703066.0000
95/95 [==============================] - 0s 693us/step - loss: 11703066.0000
Epoch 14/15
95/95 [==============================] - 0s 690us/step - loss: 11638650.0000
95/95 [==============================] - 0s 707us/step - loss: 11638650.0000
Epoch 15/15
95/95 [==============================] - 0s 678us/step - loss: 11620338.0000
95/95 [==============================] - 0s 694us/step - loss: 11620338.0000
> > > 
> > > > > > + + + + > > > experts.train  = array_reshape(predict(model,train), c(3028,1,1))
> experts.test = pred.test
> res = add_expert(pred.lstm$train, pred.lstm$test, experts.train, experts.test)
> experts.train = res$experts.train
> experts.test = res$experts.test
> res = add_expert(rf$predicted, pred.test.rf, experts.train, experts.test)
Error in abind(experts.train, new_train, along = 2) : 
  'X2' does not fit: should have `length(dim())'=3 or 2
> add_expert = function(new_train, new_test, experts.train, experts.test){
+     experts.train = array_reshape(abind(experts.train, array_reshape(new_train, c(3028, 1, 1)), along=2), c(3028,1,dim(experts.train)[3]+1))
+     experts.test = cbind(experts.test, new_test)
+     return(list("experts.train" = experts.train, "experts.test" = experts.test))
+ }
> experts.train  = array_reshape(predict(model,train), c(3028,1,1))
> experts.test = pred.test
> res = add_expert(pred.lstm$train, pred.lstm$test, experts.train, experts.test)
> experts.train = res$experts.train
> experts.test = res$experts.test
> res = add_expert(rf$predicted, pred.test.rf, experts.train, experts.test)
Error in abind(experts.train, array_reshape(new_train, c(3028, 1, 1)),  : 
  arg 'X2' has dims=3028, 1, 1; but need dims=3028, X, 2
> list_experts_train = c(predict(model, train), pred.lstm$train, rf$predicted) 
> list_experts_test = c(pred.test, pred.lstm$test, pred.test.rf)
> list_experts_train
       1        2        3        4        5        6        7        8 
48411.71 59681.34 62376.92 66839.19 65904.65 68431.83 62691.44 59030.71 
       9       10       11       12       13       14       15       16 
65195.32 68270.43 69265.90 70673.38 71739.20 69247.39 68473.49 76602.69 
      17       18       19       20       21       22       23       24 
78407.85 76490.51 73105.90 69081.57 61725.91 58018.28 64627.35 67752.40 
      25       26       27       28       29       30       31       32 
68274.14 68321.35 69074.49 66094.16 65670.96 75040.27 80180.89 82793.78 
      33       34       35       36       37       38       39       40 
86782.17 89339.39 85395.05 83449.09 89840.11 91549.60 92820.15 92907.29 
      41       42       43       44       45       46       47       48 
91816.30 87674.00 85572.08 88064.60 83602.34 78812.49 76153.15 73399.09 
      49       50       51       52       53       54       55       56 
67928.97 65429.99 73723.87 75968.03 74934.09 71361.28 67956.27 60626.44 
      57       58       59       60       61       62       63       64 
57400.41 64819.58 66985.20 65174.71 63166.49 61692.66 56162.79 54056.94 
      65       66       67       68       69       70       71       72 
64046.69 69286.45 69095.17 68969.35 66713.62 60262.70 55075.64 59993.87 
      73       74       75       76       77       78       79       80 
60809.29 59814.80 58349.56 57497.94 52247.79 52164.34 60775.69 63510.36 
      81       82       83       84       85       86       87       88 
61796.63 59652.04 56584.75 49507.65 45645.57 51517.01 53034.01 52798.74 
      89       90       91       92       93       94       95       96 
52158.66 51453.33 46967.83 45825.63 52545.80 55572.50 56596.60 56444.45 
      97       98       99      100      101      102      103      104 
56730.04 51584.35 49511.14 55248.76 53263.20 58995.28 59581.36 58836.52 
     105      106      107      108      109      110      111      112 
53855.72 51553.85 59823.16 62914.02 62671.67 61750.97 59847.43 53999.34 
     113      114      115      116      117      118      119      120 
50985.31 57870.83 60821.16 59266.45 58462.32 55863.18 49170.97 46632.69 
     121      122      123      124      125      126      127      128 
53106.87 51885.15 47090.84 51767.75 51654.51 46902.54 44243.70 49887.05 
     129      130      131      132      133      134      135      136 
48819.35 45729.68 49875.82 48574.97 43688.03 41645.11 47732.47 51028.33 
     137      138      139      140      141      142      143      144 
52113.31 51046.59 45712.27 42136.94 41748.02 49355.46 51782.93 51358.29 
     145      146      147      148      149      150      151      152 
49462.96 48343.94 42340.83 39036.20 45160.69 43122.57 47220.27 47855.08 
     153      154      155      156      157      158      159      160 
47427.47 42235.23 38494.75 44818.73 47628.50 48067.69 47887.65 47286.34 
     161      162      163      164      165      166      167      168 
42119.42 38972.77 45926.86 48400.98 48738.69 48446.91 47399.37 42015.60 
     169      170      171      172      173      174      175      176 
39199.55 45099.90 47742.87 48406.92 47803.86 47334.33 41814.40 38614.78 
     177      178      179      180      181      182      183      184 
45411.45 48439.05 49585.51 49256.97 48251.92 42436.54 39155.75 45569.14 
     185      186      187      188      189      190      191      192 
48091.82 48361.31 48233.57 47702.31 42210.88 38965.58 45313.50 47699.90 
     193      194      195      196      197      198      199      200 
48058.95 48014.60 47578.52 42353.58 38130.98 45418.98 47970.57 48616.33 
     201      202      203      204      205      206      207      208 
48190.03 47445.67 42087.86 39017.56 45480.05 48071.71 48953.94 49319.85 
     209      210      211      212      213      214      215      216 
48246.70 41883.46 38498.67 44731.07 46300.17 46469.90 46139.72 45270.43 
     217      218      219      220      221      222      223      224 
39860.49 37310.48 43583.22 43691.90 43737.60 44135.44 43844.06 38902.50 
     225      226      227      228      229      230      231      232 
37205.63 43748.31 44381.20 43853.84 40929.51 43832.42 40189.28 39164.54 
     233      234      235      236      237      238      239      240 
45325.69 46861.23 46155.72 46146.46 45448.53 40524.54 38073.61 44560.25 
     241      242      243      244      245      246      247      248 
46468.78 47197.52 47076.28 46769.22 41569.77 38441.39 45023.64 47658.84 
     249      250      251      252      253      254      255      256 
47976.51 47956.83 47444.76 42431.03 39264.51 45489.12 47819.84 48438.85 
     257      258      259      260      261      262      263      264 
48272.51 47347.77 42084.27 38818.73 45259.70 47659.33 48616.46 48442.41 
     265      266      267      268      269      270      271      272 
47907.98 43050.76 40078.41 45968.36 48758.10 49813.40 50108.30 49251.80 
     273      274      275      276      277      278      279      280 
44198.99 41562.32 47813.85 50153.36 49995.86 50088.07 49346.49 43607.09 
     281      282      283      284      285      286      287      288 
40684.49 47461.92 50251.15 50022.91 50251.12 50552.32 46090.33 44588.32 
     289      290      291      292      293      294      295      296 
52674.25 54389.90 53810.55 53076.98 51285.44 45732.67 42603.84 48878.73 
     297      298      299      300      301      302      303      304 
51076.52 51630.92 52292.98 52909.90 52950.87 54074.44 61969.74 63605.58 
     305      306      307      308      309      310      311      312 
62677.10 61389.63 54487.74 51548.70 50412.96 58399.84 62450.53 63708.34 
     313      314      315      316      317      318      319      320 
62957.23 61656.46 55413.24 52878.99 60870.00 64686.95 64601.80 65093.29 
     321      322      323      324      325      326      327      328 
64022.69 57411.73 53795.58 60164.03 63199.10 63243.22 64421.86 63148.49 
     329      330      331      332      333      334      335      336 
56146.22 52292.05 58544.77 63855.58 66236.52 69531.07 72307.93 68779.62 
     337      338      339      340      341      342      343      344 
65399.94 69460.34 71580.09 72347.70 75796.64 75016.97 71422.11 68085.79 
     345      346      347      348      349      350      351      352 
74425.87 78394.78 81082.95 78247.84 73372.08 64694.17 59054.62 65636.60 
     353      354      355      356      357      358      359      360 
68108.51 68796.27 68031.39 67435.97 59072.28 55234.68 59878.81 57761.41 
     361      362      363      364      365      366      367      368 
54279.28 58404.88 59370.82 54695.58 53944.79 61685.75 60869.63 59170.63 
     369      370      371      372      373      374      375      376 
64767.65 64946.23 60242.25 58594.24 66558.97 69889.05 72219.21 70946.09 
     377      378      379      380      381      382      383      384 
70303.35 66065.15 65172.98 74186.50 77834.67 81057.29 82877.64 83162.44 
     385      386      387      388      389      390      391      392 
76583.72 73587.51 78126.22 78489.71 78516.88 78993.79 80248.49 73940.88 
     393      394      395      396      397      398      399      400 
69129.98 72349.93 70205.05 67572.02 65447.63 65140.31 63958.33 63976.86 
     401      402      403      404      405      406      407      408 
69460.74 70983.40 72779.44 75550.37 76853.88 71439.46 69497.29 76102.53 
     409      410      411      412      413      414      415      416 
76525.73 77761.31 75376.01 74056.49 68177.50 65840.88 71973.27 73125.87 
     417      418      419      420      421      422      423      424 
73779.80 76134.56 79846.57 77145.85 76208.65 81515.40 80816.84 78548.86 
     425      426      427      428      429      430      431      432 
77285.36 75495.23 71748.24 67745.05 69700.70 67283.49 65040.20 62419.82 
     433      434      435      436      437      438      439      440 
60872.88 55936.10 53788.26 62860.32 71004.46 75718.22 76925.50 74739.10 
     441      442      443      444      445      446      447      448 
67503.46 63412.94 69417.44 70447.83 69507.70 67860.72 65104.67 58829.76 
     449      450      451      452      453      454      455      456 
56286.62 64552.00 68205.10 68638.72 69138.58 67696.02 62965.71 61439.23 
     457      458      459      460      461      462      463      464 
67142.89 63462.97 67139.02 67463.79 68713.32 64330.81 60688.23 64479.55 
     465      466      467      468      469      470      471      472 
64331.88 61957.56 60425.25 59052.48 52039.08 47527.53 51054.30 51577.43 
     473      474      475      476      477      478      479      480 
51274.31 51017.22 52468.42 49442.31 47389.70 53166.09 53887.35 52172.84 
     481      482      483      484      485      486      487      488 
50515.63 50528.87 49687.17 48833.88 55440.81 56392.84 55255.82 48210.98 
     489      490      491      492      493      494      495      496 
51744.01 46730.11 43313.68 48622.72 49615.40 48064.01 44475.91 44015.51 
     497      498      499      500      501      502      503      504 
41877.25 41965.02 48613.06 50969.20 51979.29 52612.55 52213.03 47324.40 
     505      506      507      508      509      510      511      512 
44957.71 51435.35 49489.57 53724.14 55200.19 55973.09 51673.90 47002.56 
     513      514      515      516      517      518      519      520 
51078.44 53787.57 53932.73 53777.68 53023.83 46886.27 42625.27 48073.59 
     521      522      523      524      525      526      527      528 
49810.77 49242.86 48794.63 47702.77 41948.43 39479.73 45835.49 47969.14 
     529      530      531      532      533      534      535      536 
48094.62 47513.40 47240.28 41775.35 39038.74 45326.41 48030.10 48159.40 
     537      538      539      540      541      542      543      544 
47990.23 47358.71 41878.04 38891.42 45957.59 48068.59 48030.59 47806.73 
     545      546      547      548      549      550      551      552 
47284.27 42108.98 39241.06 45367.87 47614.53 47724.34 48077.68 47615.87 
     553      554      555      556      557      558      559      560 
42724.90 39917.75 46063.12 48817.33 49042.29 48732.61 48076.71 42874.30 
     561      562      563      564      565      566      567      568 
40057.22 46178.10 48875.44 49191.85 48977.19 48496.46 43593.61 40945.23 
     569      570      571      572      573      574      575      576 
47276.09 49336.20 49613.21 49601.60 49126.98 43438.54 39495.07 45298.07 
     577      578      579      580      581      582      583      584 
46471.20 47065.42 47842.96 47095.40 41013.62 38318.06 44799.48 45079.07 
     585      586      587      588      589      590      591      592 
44339.77 43852.41 42806.50 37759.10 36344.10 43017.68 43209.83 43015.83 
     593      594      595      596      597      598      599      600 
43051.16 39695.90 37187.23 36872.01 43302.80 44264.38 44329.08 44305.13 
     601      602      603      604      605      606      607      608 
44238.20 39261.18 37524.43 43953.33 45579.47 46359.73 46451.97 46191.23 
     609      610      611      612      613      614      615      616 
41209.36 38294.87 44707.57 47279.72 48229.55 48432.37 47241.49 41993.12 
     617      618      619      620      621      622      623      624 
38820.02 45222.84 47508.66 47845.73 48285.05 47843.77 42512.91 39834.07 
     625      626      627      628      629      630      631      632 
46594.78 48791.10 49376.63 49384.73 48434.76 42806.14 39555.39 45816.83 
     633      634      635      636      637      638      639      640 
48040.29 48282.62 48288.70 47664.24 42145.16 39076.11 45787.91 48020.86 
     641      642      643      644      645      646      647      648 
48501.11 48515.06 47712.92 42624.59 39805.58 46485.44 48767.43 49146.09 
     649      650      651      652      653      654      655      656 
51556.85 52940.53 49320.03 45876.64 51787.26 53376.91 52904.79 53107.04 
     657      658      659      660      661      662      663      664 
51754.50 46146.36 42686.03 48572.76 50295.92 49930.26 50028.95 49541.53 
     665      666      667      668      669      670      671      672 
44139.60 40747.81 48025.13 52484.21 54437.63 55578.82 54429.24 44195.25 
     673      674      675      676      677      678      679      680 
45517.49 53700.45 56700.81 57330.77 55513.46 54283.01 50930.79 49631.34 
     681      682      683      684      685      686      687      688 
58104.15 55940.01 61284.23 62671.04 65014.34 61606.70 58842.68 64798.32 
     689      690      691      692      693      694      695      696 
68467.40 70902.01 72968.06 73010.30 67908.79 63646.23 70145.42 72578.65 
     697      698      699      700      701      702      703      704 
74468.82 75259.83 74010.37 67654.81 64839.20 70986.99 74336.79 73891.40 
     705      706      707      708      709      710      711      712 
74212.27 72123.55 67455.72 65423.88 72991.01 76309.09 75643.31 75357.87 
     713      714      715      716      717      718      719      720 
74234.77 67693.05 63623.76 68937.52 69819.25 68019.26 67398.96 68099.75 
     721      722      723      724      725      726      727      728 
63267.36 59479.78 63967.31 61536.02 61150.29 57860.06 60435.60 57842.87 
     729      730      731      732      733      734      735      736 
58190.01 66563.79 66626.13 62620.03 57632.98 59570.67 56641.10 56713.83 
     737      738      739      740      741      742      743      744 
62372.52 63158.29 61982.59 62322.03 64243.49 60305.59 57688.86 65339.40 
     745      746      747      748      749      750      751      752 
68913.69 68662.01 68516.78 67369.13 61626.44 60123.45 67419.71 71606.46 
     753      754      755      756      757      758      759      760 
72616.22 71236.70 70283.76 64150.06 59964.25 68745.42 71956.89 73268.19 
     761      762      763      764      765      766      767      768 
73237.04 71605.89 66581.51 63852.28 70264.53 70658.59 69302.46 68780.99 
     769      770      771      772      773      774      775      776 
67873.75 62307.15 61126.23 68121.18 69765.14 69654.37 69527.25 66944.76 
     777      778      779      780      781      782      783      784 
61815.68 58397.58 65107.65 66204.93 66301.15 64774.59 65552.29 60400.67 
     785      786      787      788      789      790      791      792 
57247.11 61424.60 63925.44 66353.79 66344.41 68233.86 64011.66 60220.05 
     793      794      795      796      797      798      799      800 
66613.28 67960.06 66822.71 65041.96 63609.05 56217.13 50883.31 56546.84 
     801      802      803      804      805      806      807      808 
58906.56 58987.09 58896.39 57685.71 52257.17 48861.99 54977.42 56948.18 
     809      810      811      812      813      814      815      816 
57545.05 55764.40 55412.22 52254.04 52952.43 61338.89 63280.99 63739.99 
     817      818      819      820      821      822      823      824 
63644.56 60532.13 52678.92 48017.96 53488.62 54148.41 53048.52 53211.62 
     825      826      827      828      829      830      831      832 
53514.13 47971.63 44147.82 49515.93 52806.27 53007.70 52737.84 51674.53 
     833      834      835      836      837      838      839      840 
46061.74 43075.30 49725.39 52738.22 53085.00 52603.92 52327.55 47825.77 
     841      842      843      844      845      846      847      848 
45989.24 51430.53 47849.04 51388.65 51405.50 51087.59 46572.46 44696.92 
     849      850      851      852      853      854      855      856 
51038.34 52769.09 52584.33 51667.04 45261.75 44708.44 43774.88 49105.41 
     857      858      859      860      861      862      863      864 
50143.65 49726.73 48832.69 44214.54 41519.30 41530.52 49416.30 52094.91 
     865      866      867      868      869      870      871      872 
51619.11 50687.62 49528.49 43924.76 40233.62 46067.64 48038.38 48221.83 
     873      874      875      876      877      878      879      880 
48627.57 48642.49 43401.51 40320.29 47140.75 49364.26 49597.20 49061.79 
     881      882      883      884      885      886      887      888 
43260.25 39989.56 38730.87 45487.06 47683.69 48304.78 47807.42 47500.14 
     889      890      891      892      893      894      895      896 
42247.12 39281.00 45075.74 42853.66 47557.36 47972.16 47513.31 41942.77 
     897      898      899      900      901      902      903      904 
38391.51 44179.34 46953.60 47378.59 47458.47 47016.54 42030.00 39129.81 
     905      906      907      908      909      910      911      912 
45127.64 47826.94 47849.78 47641.34 47158.15 41800.31 38871.04 45146.03 
     913      914      915      916      917      918      919      920 
47199.05 47601.78 48019.84 47255.77 42226.96 38692.45 45083.60 47167.73 
     921      922      923      924      925      926      927      928 
47549.34 47784.26 47479.96 42217.00 38883.92 45162.77 42477.24 47597.73 
     929      930      931      932      933      934      935      936 
49062.75 48674.51 43329.58 39182.61 44737.95 47285.36 48278.36 48398.48 
     937      938      939      940      941      942      943      944 
47492.04 41913.32 38797.25 44513.06 46236.45 46351.84 46314.70 45459.76 
     945      946      947      948      949      950      951      952 
39688.03 37255.18 43419.27 43914.04 43632.95 43817.11 43173.77 38334.58 
     953      954      955      956      957      958      959      960 
36411.45 42736.91 42758.04 42318.95 42351.26 41573.75 34307.51 35366.37 
     961      962      963      964      965      966      967      968 
42473.46 43195.91 43511.17 43631.20 42961.07 38732.47 37141.88 43815.88 
     969      970      971      972      973      974      975      976 
46006.51 46572.83 46519.38 45841.15 40939.07 38121.38 44710.61 47024.24 
     977      978      979      980      981      982      983      984 
47382.62 47436.66 47070.64 42126.45 39232.38 45480.45 47898.38 47953.86 
     985      986      987      988      989      990      991      992 
47835.70 47212.81 41980.52 38896.49 45426.03 48089.03 48197.90 48024.87 
     993      994      995      996      997      998      999     1000 
47395.00 42291.71 38866.14 45484.41 48109.49 48430.76 48860.40 47879.75 
    1001     1002     1003     1004     1005     1006     1007     1008 
42470.90 39258.02 45509.89 48044.92 48278.68 48149.92 47467.58 42314.34 
    1009     1010     1011     1012     1013     1014     1015     1016 
40362.39 46772.00 49396.96 49627.57 49476.99 48663.48 43518.01 40448.66 
    1017     1018     1019     1020     1021     1022     1023     1024 
47752.43 49955.41 49970.24 49924.76 49159.08 43682.61 40177.14 46192.28 
    1025     1026     1027     1028     1029     1030     1031     1032 
48841.10 52391.78 52511.43 51858.90 46399.38 43308.81 49995.04 52353.17 
    1033     1034     1035     1036     1037     1038     1039     1040 
52862.94 52285.60 50915.94 45446.34 41933.84 50167.43 55354.18 57566.09 
    1041     1042     1043     1044     1045     1046     1047     1048 
59391.53 58510.33 53156.16 50012.22 56738.90 55931.16 53562.10 57616.74 
    1049     1050     1051     1052     1053     1054     1055     1056 
57908.04 53726.59 50864.64 58155.78 60885.24 61948.23 61907.86 60241.47 
    1057     1058     1059     1060     1061     1062     1063     1064 
53328.56 48738.13 55450.89 58298.03 59184.45 59087.19 57231.43 52672.60 
    1065     1066     1067     1068     1069     1070     1071     1072 
51444.68 61886.10 67150.85 69386.95 70631.61 69106.74 64917.45 62138.31 
    1073     1074     1075     1076     1077     1078     1079     1080 
69870.46 72517.23 71466.99 69473.02 67320.49 62592.29 60487.00 67216.06 
    1081     1082     1083     1084     1085     1086     1087     1088 
68275.25 65567.30 64387.20 62191.09 57515.36 57089.38 65279.08 66721.15 
    1089     1090     1091     1092     1093     1094     1095     1096 
65501.22 63354.74 59699.28 59663.09 65706.22 76389.45 77031.24 75928.60 
    1097     1098     1099     1100     1101     1102     1103     1104 
73843.71 67597.20 61878.36 60776.16 69305.38 73654.83 74090.67 70801.66 
    1105     1106     1107     1108     1109     1110     1111     1112 
66554.26 59707.90 56692.28 65065.53 67863.91 67816.26 66811.67 68168.38 
    1113     1114     1115     1116     1117     1118     1119     1120 
65358.13 65101.74 73745.28 77212.18 77167.29 77693.90 77800.48 72749.86 
    1121     1122     1123     1124     1125     1126     1127     1128 
70032.92 74443.89 74798.88 72735.83 72849.19 74316.59 70570.75 68422.65 
    1129     1130     1131     1132     1133     1134     1135     1136 
76726.64 79788.24 79854.69 81326.00 81881.89 77476.55 72469.99 77376.64 
    1137     1138     1139     1140     1141     1142     1143     1144 
77366.01 76108.38 74149.40 71305.60 66007.44 62865.53 69474.89 71789.74 
    1145     1146     1147     1148     1149     1150     1151     1152 
72665.51 72233.45 69890.60 66839.45 64075.11 69468.44 71215.26 70085.78 
    1153     1154     1155     1156     1157     1158     1159     1160 
67860.37 68934.67 63551.50 58688.02 63577.68 65249.62 67400.57 68450.21 
    1161     1162     1163     1164     1165     1166     1167     1168 
67671.11 60787.98 56198.78 61123.78 61519.96 61305.07 61358.19 62190.97 
    1169     1170     1171     1172     1173     1174     1175     1176 
59355.22 57798.67 64337.22 63470.55 61023.94 60526.67 61387.14 57646.83 
    1177     1178     1179     1180     1181     1182     1183     1184 
56419.57 62482.75 63891.94 65702.73 65567.69 64294.96 55830.55 51466.51 
    1185     1186     1187     1188     1189     1190     1191     1192 
57318.42 58683.16 60331.63 60896.52 60369.07 54911.58 53592.76 59758.04 
    1193     1194     1195     1196     1197     1198     1199     1200 
55741.54 58015.84 56206.66 53906.38 48452.57 44797.31 50173.50 50792.89 
    1201     1202     1203     1204     1205     1206     1207     1208 
49669.36 49135.86 49247.70 44759.90 43349.87 49045.60 50042.70 49385.94 
    1209     1210     1211     1212     1213     1214     1215     1216 
49262.57 48429.96 43376.63 40961.88 49338.59 52652.98 52618.99 52170.35 
    1217     1218     1219     1220     1221     1222     1223     1224 
51347.07 40585.67 41715.28 48177.00 49264.84 49517.48 49628.94 48194.87 
    1225     1226     1227     1228     1229     1230     1231     1232 
39056.22 39210.16 45874.24 47350.26 47462.96 47833.50 44014.13 40730.06 
    1233     1234     1235     1236     1237     1238     1239     1240 
39987.70 46373.84 49847.15 50716.76 49863.07 49064.38 43474.87 40005.82 
    1241     1242     1243     1244     1245     1246     1247     1248 
45726.66 44030.07 47993.92 48233.62 47736.78 42538.86 39309.16 45448.19 
    1249     1250     1251     1252     1253     1254     1255     1256 
48192.52 48188.37 48789.00 48079.18 42565.93 39263.53 45280.10 47477.11 
    1257     1258     1259     1260     1261     1262     1263     1264 
47740.21 48252.90 47223.28 42030.47 38561.82 45056.23 47213.94 47399.19 
    1265     1266     1267     1268     1269     1270     1271     1272 
47332.90 47013.12 41462.76 38625.66 44922.00 47526.28 47842.74 48012.31 
    1273     1274     1275     1276     1277     1278     1279     1280 
47752.00 42371.50 39827.93 46487.78 49967.85 50328.84 50135.93 50668.12 
    1281     1282     1283     1284     1285     1286     1287     1288 
45128.87 41588.67 47828.90 49844.61 49529.17 48872.88 48711.48 43269.20 
    1289     1290     1291     1292     1293     1294     1295     1296 
40323.39 46535.70 46796.85 44332.08 49596.30 49514.79 44144.07 40799.72 
    1297     1298     1299     1300     1301     1302     1303     1304 
46852.51 49184.81 49560.51 49190.62 48234.00 42211.31 39016.96 45462.27 
    1305     1306     1307     1308     1309     1310     1311     1312 
46809.03 46771.66 46502.51 45510.66 39940.40 38092.17 44967.41 44855.72 
    1313     1314     1315     1316     1317     1318     1319     1320 
45346.68 45248.32 45082.86 39260.00 37319.18 43695.74 43935.04 44728.06 
    1321     1322     1323     1324     1325     1326     1327     1328 
43735.36 42867.28 37605.39 35458.25 42444.35 43126.67 43510.00 43749.61 
    1329     1330     1331     1332     1333     1334     1335     1336 
43892.87 39230.44 36841.97 43524.63 45359.83 45963.79 45962.99 46313.14 
    1337     1338     1339     1340     1341     1342     1343     1344 
41952.16 39623.08 45561.63 47343.95 47317.79 47058.69 46810.21 41811.65 
    1345     1346     1347     1348     1349     1350     1351     1352 
39011.95 45754.56 47745.80 47751.21 47614.84 47083.26 41814.13 39068.43 
    1353     1354     1355     1356     1357     1358     1359     1360 
45783.46 47858.19 48288.29 48445.17 48016.64 42707.18 39979.31 46446.63 
    1361     1362     1363     1364     1365     1366     1367     1368 
48743.50 49631.98 49298.85 48684.73 43321.04 40348.12 47104.02 49445.11 
    1369     1370     1371     1372     1373     1374     1375     1376 
49660.18 50367.70 49918.63 44855.54 41926.41 48124.86 49809.91 49939.90 
    1377     1378     1379     1380     1381     1382     1383     1384 
50558.00 50475.64 45373.65 42832.48 49919.59 53642.48 56906.80 59491.58 
    1385     1386     1387     1388     1389     1390     1391     1392 
60134.35 54331.22 49399.89 55377.06 57826.43 58042.33 57575.77 56398.55 
    1393     1394     1395     1396     1397     1398     1399     1400 
49811.53 46884.72 52310.38 54131.95 54817.86 54598.40 53827.55 48418.96 
    1401     1402     1403     1404     1405     1406     1407     1408 
45644.14 52696.60 55331.44 55184.47 54231.83 52640.80 46722.01 43144.79 
    1409     1410     1411     1412     1413     1414     1415     1416 
50250.54 52934.08 53930.95 50476.21 54175.33 50396.67 47779.68 54151.39 
    1417     1418     1419     1420     1421     1422     1423     1424 
55440.40 54805.57 54141.61 54828.12 54883.23 57530.97 67892.17 69922.03 
    1425     1426     1427     1428     1429     1430     1431     1432 
68418.61 67083.81 67327.89 62525.44 57398.23 63109.31 63843.74 64458.81 
    1433     1434     1435     1436     1437     1438     1439     1440 
65063.60 64660.92 59952.54 55396.86 60686.56 61781.13 64376.56 67171.73 
    1441     1442     1443     1444     1445     1446     1447     1448 
67092.11 61843.63 58358.74 65149.88 65075.49 62921.16 61398.20 59772.32 
    1449     1450     1451     1452     1453     1454     1455     1456 
53760.27 50911.69 58196.14 59214.02 58994.19 57679.40 54873.68 46300.67 
    1457     1458     1459     1460     1461     1462     1463     1464 
49222.65 57515.98 59050.40 58446.15 59060.91 57720.93 50484.50 54112.97 
    1465     1466     1467     1468     1469     1470     1471     1472 
62677.74 65536.37 66910.59 65682.14 65278.25 59555.30 56784.58 63989.61 
    1473     1474     1475     1476     1477     1478     1479     1480 
67987.76 70586.39 71895.15 73543.71 70406.93 68870.55 76854.62 78507.46 
    1481     1482     1483     1484     1485     1486     1487     1488 
78567.25 78469.76 75176.88 68240.08 61586.74 65927.25 66258.28 64782.48 
    1489     1490     1491     1492     1493     1494     1495     1496 
65623.40 65153.53 60790.83 55778.36 60918.16 62370.80 64814.11 66410.64 
    1497     1498     1499     1500     1501     1502     1503     1504 
66311.85 57931.59 56283.50 61641.98 66017.51 69125.53 71083.90 69063.19 
    1505     1506     1507     1508     1509     1510     1511     1512 
62602.88 60306.21 68764.79 74034.05 75820.19 74934.46 73304.84 63875.05 
    1513     1514     1515     1516     1517     1518     1519     1520 
58085.02 62303.52 65031.19 66338.58 68703.30 69522.98 64782.49 62838.74 
    1521     1522     1523     1524     1525     1526     1527     1528 
70446.84 71005.58 69363.15 68450.80 68787.62 64624.97 63041.54 70585.36 
    1529     1530     1531     1532     1533     1534     1535     1536 
72257.40 71596.01 69851.94 68427.18 62647.63 59148.11 65281.20 66436.24 
    1537     1538     1539     1540     1541     1542     1543     1544 
67289.69 67832.76 66256.46 59583.56 56994.21 62722.63 63691.92 62960.52 
    1545     1546     1547     1548     1549     1550     1551     1552 
63387.36 61580.93 54228.46 51935.27 57451.32 54798.45 58585.06 59466.77 
    1553     1554     1555     1556     1557     1558     1559     1560 
60364.36 54849.76 49974.68 54447.54 56304.86 56201.06 57894.57 58896.28 
    1561     1562     1563     1564     1565     1566     1567     1568 
54496.26 48962.68 54925.85 54771.79 54805.62 53858.89 53095.08 48155.47 
    1569     1570     1571     1572     1573     1574     1575     1576 
47123.08 54881.72 55270.77 53543.70 52268.68 51382.32 47608.21 48268.77 
    1577     1578     1579     1580     1581     1582     1583     1584 
56763.78 59999.72 59824.42 59778.73 57199.63 52863.53 50092.12 54229.75 
    1585     1586     1587     1588     1589     1590     1591     1592 
54928.91 52786.64 51310.85 44841.61 40633.86 39332.83 46045.56 48362.12 
    1593     1594     1595     1596     1597     1598     1599     1600 
49040.18 48604.98 48270.40 44279.68 42071.26 48378.31 45808.02 49194.08 
    1601     1602     1603     1604     1605     1606     1607     1608 
49496.53 48576.14 43264.67 40208.85 47200.63 49560.11 49115.50 48159.24 
    1609     1610     1611     1612     1613     1614     1615     1616 
46715.58 41112.00 38475.22 45506.01 48435.54 48973.68 48438.48 48037.29 
    1617     1618     1619     1620     1621     1622     1623     1624 
42801.32 39554.70 45756.43 47208.17 46925.56 46783.50 45929.26 40776.49 
    1625     1626     1627     1628     1629     1630     1631     1632 
38096.11 44839.97 46958.34 46818.94 47282.52 46781.23 41761.03 38865.72 
    1633     1634     1635     1636     1637     1638     1639     1640 
45382.21 47741.26 48572.00 48413.16 47107.76 41547.37 38582.89 45174.67 
    1641     1642     1643     1644     1645     1646     1647     1648 
47327.43 47282.64 47535.71 47162.56 41739.20 38895.75 45594.79 47700.51 
    1649     1650     1651     1652     1653     1654     1655     1656 
47904.96 48207.79 47864.84 43059.89 40458.70 46347.28 47886.07 47647.46 
    1657     1658     1659     1660     1661     1662     1663     1664 
47216.45 42012.86 39436.78 39010.20 46449.26 49438.05 48777.16 47981.47 
    1665     1666     1667     1668     1669     1670     1671     1672 
47248.69 41961.49 39275.39 45789.86 47731.77 47493.89 47279.04 46757.85 
    1673     1674     1675     1676     1677     1678     1679     1680 
41213.87 38275.32 44659.58 45047.80 45253.87 44640.08 44120.00 38620.38 
    1681     1682     1683     1684     1685     1686     1687     1688 
37188.19 43667.41 43546.70 43337.34 42920.64 42571.99 38478.66 37579.51 
    1689     1690     1691     1692     1693     1694     1695     1696 
44078.84 41425.45 43967.92 44108.20 43803.98 38831.64 37288.68 43765.40 
    1697     1698     1699     1700     1701     1702     1703     1704 
46372.94 47127.72 47862.83 47467.82 43013.66 39678.48 45501.25 47566.12 
    1705     1706     1707     1708     1709     1710     1711     1712 
47926.67 48079.00 47682.21 42904.99 39682.67 46240.40 48500.09 48854.83 
    1713     1714     1715     1716     1717     1718     1719     1720 
48066.06 47563.46 42552.57 39698.21 46839.88 48797.63 47971.02 47557.31 
    1721     1722     1723     1724     1725     1726     1727     1728 
46719.12 41783.95 38884.88 45768.18 47564.50 47537.43 47407.87 46739.49 
    1729     1730     1731     1732     1733     1734     1735     1736 
41660.56 38680.20 45529.09 47578.50 47684.43 47482.61 46798.34 41986.18 
    1737     1738     1739     1740     1741     1742     1743     1744 
40123.85 47038.62 48632.88 49131.43 50818.57 50467.54 46030.09 44180.35 
    1745     1746     1747     1748     1749     1750     1751     1752 
52089.68 55483.87 55802.91 56221.38 55963.53 49633.09 44894.21 51282.28 
    1753     1754     1755     1756     1757     1758     1759     1760 
53591.13 54852.37 56073.16 56357.85 51745.49 47678.22 53842.28 54618.30 
    1761     1762     1763     1764     1765     1766     1767     1768 
54515.37 55441.96 55161.41 50368.96 47329.54 53671.37 53386.82 51907.49 
    1769     1770     1771     1772     1773     1774     1775     1776 
58644.44 59199.38 55221.36 53949.34 62792.66 66879.60 66289.69 64954.15 
    1777     1778     1779     1780     1781     1782     1783     1784 
64201.96 55055.78 56282.75 63695.02 64812.99 63272.85 61346.43 60606.44 
    1785     1786     1787     1788     1789     1790     1791     1792 
57466.19 52586.97 58309.08 60579.92 60388.44 60607.15 61049.56 57604.54 
    1793     1794     1795     1796     1797     1798     1799     1800 
55250.24 64501.45 70648.33 72791.41 73583.00 73043.78 67953.00 63817.48 
    1801     1802     1803     1804     1805     1806     1807     1808 
69965.55 70842.03 70629.19 70301.44 69689.09 64527.76 61936.20 68235.12 
    1809     1810     1811     1812     1813     1814     1815     1816 
69427.03 69512.08 69340.46 68406.13 64080.33 64302.97 72138.22 74060.21 
    1817     1818     1819     1820     1821     1822     1823     1824 
72308.17 70950.42 67629.11 60121.73 55729.44 61400.20 64367.20 67964.17 
    1825     1826     1827     1828     1829     1830     1831     1832 
71560.15 74012.59 70885.89 69735.34 75611.23 77021.85 78206.52 79308.67 
    1833     1834     1835     1836     1837     1838     1839     1840 
81120.62 76711.93 72368.16 75679.31 74925.73 72774.78 71352.04 73343.19 
    1841     1842     1843     1844     1845     1846     1847     1848 
70790.66 69103.34 76584.21 81460.36 84351.96 86199.25 85265.34 78921.02 
    1849     1850     1851     1852     1853     1854     1855     1856 
75910.10 81985.38 83166.64 85081.10 83922.50 78244.47 70900.72 64944.34 
    1857     1858     1859     1860     1861     1862     1863     1864 
69210.33 68483.26 65912.98 64479.57 65738.88 61384.60 60396.01 67820.84 
    1865     1866     1867     1868     1869     1870     1871     1872 
69084.81 70047.80 72393.39 73337.59 69261.06 64680.52 67820.81 66728.47 
    1873     1874     1875     1876     1877     1878     1879     1880 
65028.71 63933.07 63766.25 60103.15 57803.89 63427.97 63261.16 61938.57 
    1881     1882     1883     1884     1885     1886     1887     1888 
61910.59 63937.79 60050.29 56848.42 62080.56 65337.33 65573.22 65483.87 
    1889     1890     1891     1892     1893     1894     1895     1896 
61418.22 58144.04 55628.85 63543.91 66337.91 64147.50 62465.69 59059.97 
    1897     1898     1899     1900     1901     1902     1903     1904 
52409.83 50100.06 56736.95 58863.45 58553.76 56836.64 56047.12 50559.59 
    1905     1906     1907     1908     1909     1910     1911     1912 
48610.41 54971.44 58352.21 59639.22 60715.08 59929.10 54105.81 49567.54 
    1913     1914     1915     1916     1917     1918     1919     1920 
54589.43 54749.04 53310.67 52290.47 51410.85 47600.65 45592.53 51756.29 
    1921     1922     1923     1924     1925     1926     1927     1928 
53184.89 53116.56 53800.42 52828.24 46530.89 42480.64 48659.46 50980.45 
    1929     1930     1931     1932     1933     1934     1935     1936 
51020.13 51216.08 50817.40 45788.93 43880.10 50664.36 49813.23 55704.88 
    1937     1938     1939     1940     1941     1942     1943     1944 
57554.35 55441.26 48592.44 45176.28 50436.97 54744.67 57609.67 60025.14 
    1945     1946     1947     1948     1949     1950     1951     1952 
59068.43 51465.42 47318.65 53574.70 49702.82 54435.49 54556.47 52704.32 
    1953     1954     1955     1956     1957     1958     1959     1960 
47163.42 44262.81 50656.51 48429.76 51171.38 50679.94 49441.15 44068.78 
    1961     1962     1963     1964     1965     1966     1967     1968 
40757.17 46696.10 48671.59 47642.05 47624.46 49014.43 43143.68 39962.32 
    1969     1970     1971     1972     1973     1974     1975     1976 
46204.89 47426.55 47393.12 47437.77 42364.40 39120.73 38678.92 45090.18 
    1977     1978     1979     1980     1981     1982     1983     1984 
47280.81 47448.79 47036.39 46713.42 41409.87 38768.03 44688.55 42756.24 
    1985     1986     1987     1988     1989     1990     1991     1992 
46755.38 47298.44 46464.88 42059.99 39145.94 44859.89 47818.10 48515.35 
    1993     1994     1995     1996     1997     1998     1999     2000 
48295.94 47832.29 42934.81 40477.11 47548.19 50322.67 51025.48 50871.31 
    2001     2002     2003     2004     2005     2006     2007     2008 
49575.06 43626.99 40375.98 46587.92 48627.18 48425.67 47785.84 46762.54 
    2009     2010     2011     2012     2013     2014     2015     2016 
41574.63 38977.99 45781.51 48177.91 48739.72 48918.39 49082.29 43900.04 
    2017     2018     2019     2020     2021     2022     2023     2024 
40353.89 46097.24 48030.09 47994.49 48361.83 47179.85 38017.61 38964.80 
    2025     2026     2027     2028     2029     2030     2031     2032 
46613.05 49541.10 49215.81 48384.98 46803.44 41703.24 39029.16 45453.76 
    2033     2034     2035     2036     2037     2038     2039     2040 
47142.25 47150.12 47161.10 46501.76 41527.07 38580.03 45114.63 45917.70 
    2041     2042     2043     2044     2045     2046     2047     2048 
46407.37 46337.21 45803.55 40301.48 38031.98 44343.72 43733.63 43502.89 
    2049     2050     2051     2052     2053     2054     2055     2056 
43191.75 42568.61 37782.16 36432.20 43683.66 42310.08 40371.38 43282.42 
    2057     2058     2059     2060     2061     2062     2063     2064 
42889.77 38472.94 36971.26 43977.89 45332.79 45492.90 45892.51 45747.71 
    2065     2066     2067     2068     2069     2070     2071     2072 
41371.97 39405.13 46257.71 48673.87 47587.10 47704.85 46521.64 41305.14 
    2073     2074     2075     2076     2077     2078     2079     2080 
38651.44 45723.28 47553.95 47340.41 47323.40 46651.84 42185.94 39301.26 
    2081     2082     2083     2084     2085     2086     2087     2088 
45864.43 48013.21 47646.91 48677.27 49247.99 44334.39 41781.46 48891.97 
    2089     2090     2091     2092     2093     2094     2095     2096 
50790.58 50725.87 50229.74 48954.08 43543.85 40560.07 46882.80 48931.47 
    2097     2098     2099     2100     2101     2102     2103     2104 
48888.69 48683.58 47846.32 42820.73 40302.02 47294.57 49106.63 49517.89 
    2105     2106     2107     2108     2109     2110     2111     2112 
49605.19 50347.10 45572.18 42921.47 49197.44 50651.41 50098.34 49850.02 
    2113     2114     2115     2116     2117     2118     2119     2120 
48923.86 43683.78 40743.08 47007.41 48860.95 48945.98 49068.47 48593.41 
    2121     2122     2123     2124     2125     2126     2127     2128 
44078.70 43175.96 50074.52 51999.67 51226.39 50519.33 50584.84 47279.32 
    2129     2130     2131     2132     2133     2134     2135     2136 
45275.33 54400.90 57715.82 57490.40 52001.81 54370.70 48736.91 49290.27 
    2137     2138     2139     2140     2141     2142     2143     2144 
58838.94 64513.93 65299.24 65269.84 62489.57 55155.52 51607.65 61870.07 
    2145     2146     2147     2148     2149     2150     2151     2152 
67312.48 68163.46 68114.58 66297.66 61174.37 58696.87 64910.98 66011.60 
    2153     2154     2155     2156     2157     2158     2159     2160 
63328.43 60904.95 59760.50 57515.34 58701.49 67622.54 70658.56 71721.31 
    2161     2162     2163     2164     2165     2166     2167     2168 
73124.60 75182.26 71993.29 70543.98 75438.33 74604.22 74521.39 72600.20 
    2169     2170     2171     2172     2173     2174     2175     2176 
73482.78 69854.42 65270.79 71037.46 73321.17 72091.62 71630.25 71156.38 
    2177     2178     2179     2180     2181     2182     2183     2184 
68236.40 66168.14 72652.16 74912.35 73846.02 71477.45 68000.60 61119.09 
    2185     2186     2187     2188     2189     2190     2191     2192 
58992.73 66761.28 62029.52 66423.83 70174.45 68445.99 59411.60 55773.30 
    2193     2194     2195     2196     2197     2198     2199     2200 
61965.22 58622.23 61308.66 61467.00 62333.30 58198.50 56362.86 65098.23 
    2201     2202     2203     2204     2205     2206     2207     2208 
67864.48 67546.90 67871.70 68768.16 65752.46 63835.80 68674.25 68155.59 
    2209     2210     2211     2212     2213     2214     2215     2216 
68401.93 68195.76 68650.28 62141.78 59264.64 63365.02 64013.76 63524.55 
    2217     2218     2219     2220     2221     2222     2223     2224 
63742.30 66000.69 63358.80 60567.94 65663.25 66526.18 66288.51 69349.99 
    2225     2226     2227     2228     2229     2230     2231     2232 
71431.63 67958.04 66428.75 75258.10 78710.06 79763.69 80441.25 78949.76 
    2233     2234     2235     2236     2237     2238     2239     2240 
73032.64 67888.94 75622.40 77289.00 77111.70 72692.99 69457.99 63799.13 
    2241     2242     2243     2244     2245     2246     2247     2248 
62202.97 69577.12 71221.73 73299.81 75789.20 77592.32 72489.43 71098.21 
    2249     2250     2251     2252     2253     2254     2255     2256 
81793.19 87271.19 87686.20 83378.21 78868.82 68385.92 63260.90 68035.15 
    2257     2258     2259     2260     2261     2262     2263     2264 
68653.67 68114.75 67323.60 64306.04 56611.22 51679.13 58402.70 62199.04 
    2265     2266     2267     2268     2269     2270     2271     2272 
62096.59 62166.82 61923.19 59296.71 60464.42 70990.42 74019.23 73985.30 
    2273     2274     2275     2276     2277     2278     2279     2280 
72649.33 69547.43 62514.42 57051.61 62620.84 63157.47 63230.38 63511.06 
    2281     2282     2283     2284     2285     2286     2287     2288 
63496.86 59318.71 55486.98 59363.92 54974.08 57604.18 58579.23 56318.64 
    2289     2290     2291     2292     2293     2294     2295     2296 
50329.51 46064.43 52356.52 55147.56 55654.60 56612.58 54941.77 48628.87 
    2297     2298     2299     2300     2301     2302     2303     2304 
44982.91 51711.51 51703.03 50803.68 48608.19 47118.40 42327.78 39223.63 
    2305     2306     2307     2308     2309     2310     2311     2312 
46007.00 47302.31 47749.49 48908.08 48243.93 44145.97 42244.87 51419.60 
    2313     2314     2315     2316     2317     2318     2319     2320 
53167.70 47908.32 52138.14 50344.85 44230.93 40789.25 46257.42 45511.30 
    2321     2322     2323     2324     2325     2326     2327     2328 
43294.85 46154.28 42676.95 40980.22 42250.73 48870.05 50235.04 49997.31 
    2329     2330     2331     2332     2333     2334     2335     2336 
48792.80 47933.05 42545.11 39692.99 46013.67 43663.06 46988.54 47172.30 
    2337     2338     2339     2340     2341     2342     2343     2344 
46596.02 41946.59 38782.07 44659.35 46971.83 46855.14 47089.69 46548.62 
    2345     2346     2347     2348     2349     2350     2351     2352 
41477.24 38447.54 44865.66 46964.66 47088.82 47223.81 46554.08 41462.67 
    2353     2354     2355     2356     2357     2358     2359     2360 
38704.39 45161.51 47208.05 46917.31 46671.37 46301.71 41368.41 38466.31 
    2361     2362     2363     2364     2365     2366     2367     2368 
45030.46 47193.18 47929.77 47578.70 46652.72 41047.98 38269.00 45237.06 
    2369     2370     2371     2372     2373     2374     2375     2376 
47618.95 48034.48 47950.03 47462.42 43117.21 40880.60 46917.14 48770.69 
    2377     2378     2379     2380     2381     2382     2383     2384 
48933.93 48403.04 47593.26 42963.01 40506.12 46955.93 48571.40 48384.07 
    2385     2386     2387     2388     2389     2390     2391     2392 
48091.95 48122.20 43264.25 39791.55 46298.37 48022.86 48377.65 48702.61 
    2393     2394     2395     2396     2397     2398     2399     2400 
47565.80 42181.01 39405.27 46395.03 48664.05 49309.25 49922.46 48976.15 
    2401     2402     2403     2404     2405     2406     2407     2408 
42658.47 39799.02 46613.80 47887.31 48125.59 48699.47 48617.25 43373.27 
    2409     2410     2411     2412     2413     2414     2415     2416 
41021.07 47729.06 47379.34 46368.90 44714.05 43812.77 38465.36 37470.80 
    2417     2418     2419     2420     2421     2422     2423     2424 
43543.32 43598.04 43318.30 41007.55 42328.50 38099.42 37421.49 44438.29 
    2425     2426     2427     2428     2429     2430     2431     2432 
45650.73 45903.95 45931.06 44768.65 39625.21 37628.05 44445.95 46561.13 
    2433     2434     2435     2436     2437     2438     2439     2440 
46472.96 46492.38 45816.85 40859.67 38302.42 45136.47 46893.30 47191.83 
    2441     2442     2443     2444     2445     2446     2447     2448 
46987.55 46425.57 41430.92 38750.72 45476.90 47549.33 47697.70 47384.80 
    2449     2450     2451     2452     2453     2454     2455     2456 
46778.31 41535.35 38870.81 45933.82 47607.10 47659.03 47786.85 46359.50 
    2457     2458     2459     2460     2461     2462     2463     2464 
41761.97 38750.45 46771.49 48342.35 48115.33 48240.06 47342.78 42146.44 
    2465     2466     2467     2468     2469     2470     2471     2472 
40057.59 48264.16 49523.01 49967.70 49242.06 48117.76 42750.41 40703.86 
    2473     2474     2475     2476     2477     2478     2479     2480 
47975.63 49430.76 48991.31 48495.72 47646.42 42309.41 39181.17 46185.44 
    2481     2482     2483     2484     2485     2486     2487     2488 
48146.06 48372.67 48354.98 47949.22 43352.66 41327.85 48866.52 51757.35 
    2489     2490     2491     2492     2493     2494     2495     2496 
51693.57 52087.30 51945.05 50638.72 51316.79 61777.56 64538.26 62300.89 
    2497     2498     2499     2500     2501     2502     2503     2504 
60487.69 55017.55 53215.49 52397.09 57205.38 58103.52 57359.42 58499.00 
    2505     2506     2507     2508     2509     2510     2511     2512 
57545.26 51776.99 48016.85 54128.64 56280.32 56659.46 57457.57 57909.66 
    2513     2514     2515     2516     2517     2518     2519     2520 
55542.65 55553.26 66495.00 71500.39 71736.85 70080.63 68044.14 61963.80 
    2521     2522     2523     2524     2525     2526     2527     2528 
58520.56 66501.93 68313.50 66687.27 64354.46 63343.50 57489.78 53317.57 
    2529     2530     2531     2532     2533     2534     2535     2536 
58572.04 60072.05 59833.50 59911.30 59655.37 55414.11 53905.86 62283.41 
    2537     2538     2539     2540     2541     2542     2543     2544 
67831.05 70851.30 73419.08 75596.03 69939.35 65718.97 69780.15 69761.92 
    2545     2546     2547     2548     2549     2550     2551     2552 
68904.65 67401.91 64185.79 57368.34 52539.43 60074.63 61520.17 62560.24 
    2553     2554     2555     2556     2557     2558     2559     2560 
68982.42 69588.89 64545.14 62504.23 67301.61 64812.17 63054.05 70739.22 
    2561     2562     2563     2564     2565     2566     2567     2568 
74423.15 70347.19 68154.74 73190.98 72179.62 73276.66 76077.29 76425.28 
    2569     2570     2571     2572     2573     2574     2575     2576 
69043.53 62836.96 67562.71 70299.93 71310.75 72455.44 73871.42 70306.13 
    2577     2578     2579     2580     2581     2582     2583     2584 
67799.19 76193.30 78266.70 78890.92 78868.39 77122.74 68800.84 64710.25 
    2585     2586     2587     2588     2589     2590     2591     2592 
72472.05 75905.99 77225.63 76275.24 74822.33 69547.90 68330.50 76027.75 
    2593     2594     2595     2596     2597     2598     2599     2600 
76378.38 72504.48 70780.43 68439.59 62645.23 59831.21 68183.25 72074.58 
    2601     2602     2603     2604     2605     2606     2607     2608 
72325.90 70499.03 67882.99 60978.12 56622.77 63156.47 65548.46 65659.52 
    2609     2610     2611     2612     2613     2614     2615     2616 
65070.62 62205.97 56392.92 53242.91 59917.95 60597.34 59085.76 58754.64 
    2617     2618     2619     2620     2621     2622     2623     2624 
58760.95 53910.21 50474.88 58919.33 60887.28 60100.52 61368.41 60570.83 
    2625     2626     2627     2628     2629     2630     2631     2632 
54363.17 51216.75 59697.64 62865.87 64241.49 63142.97 61594.06 54555.78 
    2633     2634     2635     2636     2637     2638     2639     2640 
52066.55 60866.27 64427.96 63232.10 61803.39 58639.70 51739.52 49064.75 
    2641     2642     2643     2644     2645     2646     2647     2648 
57091.04 59999.28 59950.13 59042.70 56601.16 50023.87 46204.27 52682.76 
    2649     2650     2651     2652     2653     2654     2655     2656 
55116.53 58701.41 61622.54 60451.51 54823.94 50979.72 56561.97 57462.43 
    2657     2658     2659     2660     2661     2662     2663     2664 
57257.84 57725.00 57922.67 53886.24 52564.57 57484.25 57678.51 54968.87 
    2665     2666     2667     2668     2669     2670     2671     2672 
53092.45 50352.38 43978.97 41366.26 47554.90 44744.40 49503.84 50726.36 
    2673     2674     2675     2676     2677     2678     2679     2680 
51160.30 46743.66 45249.17 52056.35 52627.64 51009.18 46055.68 50581.32 
    2681     2682     2683     2684     2685     2686     2687     2688 
48485.48 48802.05 55894.84 54736.87 52927.85 48501.32 50724.35 46547.93 
    2689     2690     2691     2692     2693     2694     2695     2696 
43949.16 50298.88 50951.57 49873.60 49741.91 49637.40 44660.04 42472.76 
    2697     2698     2699     2700     2701     2702     2703     2704 
49053.68 49985.18 48903.09 48092.44 47060.84 41874.64 39195.72 46001.01 
    2705     2706     2707     2708     2709     2710     2711     2712 
48213.79 47995.86 47778.42 42586.06 39324.75 38395.40 44698.12 46699.78 
    2713     2714     2715     2716     2717     2718     2719     2720 
46807.52 46652.58 46071.61 41295.63 38656.13 45479.20 44561.23 47602.94 
    2721     2722     2723     2724     2725     2726     2727     2728 
47704.20 46639.46 41476.89 38712.36 45232.33 47434.55 46999.91 46704.90 
    2729     2730     2731     2732     2733     2734     2735     2736 
46043.59 41138.07 39041.29 45823.56 48810.50 50302.62 50826.33 50021.27 
    2737     2738     2739     2740     2741     2742     2743     2744 
44930.17 41632.42 47556.28 49007.98 48810.37 49150.85 48994.87 44162.65 
    2745     2746     2747     2748     2749     2750     2751     2752 
40941.77 47144.24 48223.19 47867.86 47985.46 47710.60 42638.81 39728.78 
    2753     2754     2755     2756     2757     2758     2759     2760 
45764.82 47455.69 47905.95 47802.95 47640.04 42585.45 40181.27 47603.73 
    2761     2762     2763     2764     2765     2766     2767     2768 
50786.23 51277.52 51840.03 49115.75 42209.43 38458.08 44847.67 45567.02 
    2769     2770     2771     2772     2773     2774     2775     2776 
45735.85 45654.70 45200.07 39980.15 38163.40 44857.42 44846.73 44354.70 
    2777     2778     2779     2780     2781     2782     2783     2784 
44671.94 44168.32 39125.95 36600.35 43292.50 42713.55 42454.30 42303.41 
    2785     2786     2787     2788     2789     2790     2791     2792 
39443.45 36447.10 36349.12 43231.16 43441.94 43712.56 43641.68 43816.00 
    2793     2794     2795     2796     2797     2798     2799     2800 
39717.34 38154.14 45364.16 46850.09 46997.78 47056.10 46798.59 41752.91 
    2801     2802     2803     2804     2805     2806     2807     2808 
38672.85 45378.51 46577.01 46775.73 46633.91 46141.45 40976.40 38539.95 
    2809     2810     2811     2812     2813     2814     2815     2816 
45776.89 47280.82 47108.70 47019.62 46465.48 41279.96 38664.88 45501.21 
    2817     2818     2819     2820     2821     2822     2823     2824 
47003.35 46690.56 46566.63 46047.96 41152.76 38007.48 45012.92 46383.46 
    2825     2826     2827     2828     2829     2830     2831     2832 
46456.44 46843.73 46363.86 41110.08 38706.53 45629.16 47179.50 47780.90 
    2833     2834     2835     2836     2837     2838     2839     2840 
48860.64 48328.57 43238.06 40585.09 47846.66 49102.13 49523.29 49601.89 
    2841     2842     2843     2844     2845     2846     2847     2848 
48734.02 43227.84 40091.62 46515.56 48820.37 48934.86 49565.36 49054.39 
    2849     2850     2851     2852     2853     2854     2855     2856 
43886.26 41815.24 50074.90 51927.37 52126.14 51675.06 50311.24 44906.69 
    2857     2858     2859     2860     2861     2862     2863     2864 
42795.99 50771.95 53354.01 53779.08 53693.13 52085.23 43032.00 44171.68 
    2865     2866     2867     2868     2869     2870     2871     2872 
52824.86 56129.62 57221.62 59245.44 60373.63 57158.82 55665.63 62520.54 
    2873     2874     2875     2876     2877     2878     2879     2880 
60762.02 65629.76 68123.56 69448.93 64720.16 62201.13 68112.51 70885.04 
    2881     2882     2883     2884     2885     2886     2887     2888 
71287.06 70172.46 66671.93 59363.35 55804.88 61506.29 61405.87 60375.50 
    2889     2890     2891     2892     2893     2894     2895     2896 
60354.78 60673.95 57789.30 57927.87 67418.81 71947.69 73404.65 73428.94 
    2897     2898     2899     2900     2901     2902     2903     2904 
70221.96 63229.30 56891.58 64192.73 69043.46 70323.50 69900.93 67379.81 
    2905     2906     2907     2908     2909     2910     2911     2912 
59482.63 55836.26 61246.47 62012.49 60873.00 59773.41 59815.95 54374.11 
    2913     2914     2915     2916     2917     2918     2919     2920 
53412.98 61085.49 59189.48 58316.63 54580.30 58115.29 55449.33 58894.82 
    2921     2922     2923     2924     2925     2926     2927     2928 
68787.82 69213.94 67617.81 62512.15 63424.31 59625.37 60544.98 68520.77 
    2929     2930     2931     2932     2933     2934     2935     2936 
68907.79 65460.77 63582.52 63632.98 61509.18 60616.29 67667.54 66442.85 
    2937     2938     2939     2940     2941     2942     2943     2944 
65504.04 64352.25 64191.75 62205.05 62761.12 71966.67 74423.27 73985.45 
    2945     2946     2947     2948     2949     2950     2951     2952 
73887.60 73197.40 66570.49 62609.22 67448.75 68451.73 68303.18 66106.53 
    2953     2954     2955     2956     2957     2958     2959     2960 
63444.49 55530.57 51078.65 57532.33 61593.01 65512.62 68278.20 67097.62 
    2961     2962     2963     2964     2965     2966     2967     2968 
60901.93 54689.82 60536.02 63113.64 65815.74 65306.48 64722.39 56809.47 
    2969     2970     2971     2972     2973     2974     2975     2976 
52388.37 59721.17 63125.03 64334.05 64200.13 63900.10 58155.28 53496.02 
    2977     2978     2979     2980     2981     2982     2983     2984 
58863.29 61177.48 64296.74 66452.59 66130.56 59542.85 55592.06 63939.63 
    2985     2986     2987     2988     2989     2990     2991     2992 
67569.38 67249.40 65875.68 65969.24 60586.64 56680.54 63116.01 62286.47 
    2993     2994     2995     2996     2997     2998     2999     3000 
60789.84 58845.20 59383.80 55334.94 51756.84 57588.81 57286.91 54198.45 
    3001     3002     3003     3004     3005     3006     3007     3008 
50783.62 49269.89 45252.58 45940.46 54592.47 54425.91 55123.68 56070.85 
    3009     3010     3011     3012     3013     3014     3015     3016 
54350.94 47613.58 48377.70 59025.72 58534.09 56480.03 55262.63 53423.82 
    3017     3018     3019     3020     3021     3022     3023     3024 
46028.81 43055.76 49299.48 47039.27 46044.39 44731.23 43222.21 37723.85 
    3025     3026     3027     3028                                     
36882.21 44841.07 43218.83 44476.85 50533.30 53143.88 61554.48 66563.40 
                                                                        
67070.53 67827.69 66082.91 58659.01 58395.90 66123.17 67119.39 67805.62 
                                                                        
69437.11 67516.86 64026.91 66515.03 72741.67 74468.34 75067.24 71117.98 
                                                                        
67869.43 62310.49 61886.21 68602.65 70393.22 68318.31 67531.62 65749.20 
                                                                        
61825.86 64611.73 73324.98 74983.64 77707.44 80662.30 80731.14 77989.88 
                                                                        
79771.61 84107.40 87300.08 90455.32 90091.85 87106.02 84895.56 84442.69 
                                                                        
86400.28 84245.00 81587.64 78362.06 76370.40 70160.19 68955.28 74459.98 
                                                                        
74975.76 73119.89 70831.80 65361.31 59589.40 60009.84 67302.81 67209.17 
                                                                        
64759.61 62779.04 59913.37 55145.01 56736.82 65121.35 66218.08 66533.69 
                                                                        
64680.10 62602.68 56673.73 56838.55 61718.82 61809.85 60427.91 59684.45 
                                                                        
56347.66 52016.15 53637.85 60024.73 61039.59 59888.54 57247.75 53964.63 
                                                                        
48802.37 48894.02 54109.61 54355.00 53226.77 52167.12 49649.11 45649.61 
                                                                        
46499.28 52588.94 53534.12 54644.41 54940.11 52659.43 48956.90 49576.98 
                                                                        
50430.84 56741.55 57853.39 57087.96 55158.66 50674.25 50332.91 59530.38 
                                                                        
61111.03 60052.70 58951.57 56556.95 51845.15 52591.17 59464.46 59902.45 
                                                                        
59445.59 57361.55 54059.59 48922.23 48764.21 52039.01 47956.70 52873.75 
                                                                        
52305.22 49955.57 45420.10 44933.47 46236.56 45780.56 49987.12 49581.06 
                                                                        
47583.67 42841.34 42196.83 46491.40 49835.18 50453.48 45301.79 45020.39 
                                                                        
42322.29 43177.41 50536.45 51352.59 48001.46 47909.71 46253.51 41247.36 
                                                                        
41353.40 42691.65 47196.29 47782.43 47482.52 45246.36 40141.55 38339.18 
                                                                        
45808.55 47430.71 47492.76 47439.80 45001.68 40101.99 40777.12 46453.55 
                                                                        
47651.67 47880.09 47324.16 45238.98 40672.65 40454.68 46498.76 47678.07 
                                                                        
47646.90 47588.98 45045.25 40016.75 40558.80 46776.66 48240.18 48546.45 
                                                                        
48439.29 45628.75 40866.92 40787.07 46672.20 48101.79 48504.60 47965.59 
                                                                        
45402.08 40546.83 40490.23 46588.12 47822.22 47815.86 47637.41 45708.62 
                                                                        
39623.82 40514.60 46419.28 47762.96 48252.54 47865.17 45100.99 40384.44 
                                                                        
40433.28 46318.13 47838.81 48156.45 48178.44 45456.72 40169.27 39917.47 
                                                                        
44678.57 45825.39 46039.41 45424.02 42999.18 38886.06 38471.47 41849.75 
                                                                        
42401.56 42646.90 42876.23 41639.21 38411.40 37768.23 41333.34 42119.96 
                                                                        
39557.12 42002.75 41887.87 39335.84 39036.38 43859.73 43880.46 44407.37 
                                                                        
44751.37 43791.10 39681.37 39104.42 44945.07 46293.99 46637.42 46337.81 
                                                                        
44179.11 39477.25 39888.47 46087.03 47389.26 47570.39 47279.29 45333.90 
                                                                        
40534.71 40628.66 46833.00 48093.13 47717.91 47318.58 45325.68 40237.60 
                                                                        
40616.55 46582.50 47549.56 47435.91 47774.98 46147.59 41316.00 41424.86 
                                                                        
47218.98 48550.41 49277.16 49197.63 46839.19 42185.21 42668.60 48834.05 
                                                                        
49776.39 50002.68 49711.54 47301.34 42664.82 43121.25 49553.63 50133.96 
                                                                        
50397.98 50090.54 47814.64 44195.82 45998.59 52291.88 53756.58 53252.11 
                                                                        
51915.94 49724.30 45251.66 45431.16 50994.19 51880.15 52160.07 52346.23 
                                                                        
51371.77 49400.22 52186.29 58349.58 59190.96 58997.18 53807.49 55403.12 
                                                                        
52708.54 53508.20 59863.34 61916.35 60319.71 61065.80 59110.22 52724.88 
                                                                        
54146.77 62276.76 63464.38 64015.22 63687.08 60827.79 55169.98 55591.47 
                                                                        
61893.09 63254.41 63958.52 63809.75 61107.42 54759.17 54247.36 61077.28 
                                                                        
63956.50 65821.50 67192.66 66171.92 62678.52 63264.22 68683.17 69239.25 
                                                                        
71548.73 73888.29 73415.17 67559.20 67893.12 72406.86 75224.55 78638.07 
                                                                        
78060.20 71499.13 63299.30 62914.43 68992.84 70990.54 71186.98 69353.38 
                                                                        
63998.79 59101.14 56959.66 58255.40 55161.62 59837.66 60301.15 58489.59 
                                                                        
54448.70 54453.84 55240.69 54581.83 62081.84 63504.21 61588.64 57886.63 
                                                                        
57631.32 63081.86 68440.66 70061.02 68739.38 66192.59 62856.70 64621.25 
                                                                        
73693.58 75673.33 76057.02 78431.19 78263.31 71894.60 72349.80 77086.48 
                                                                        
77760.85 78538.30 79163.48 78634.20 72600.58 69731.11 73327.25 72254.89 
                                                                        
69295.45 70134.50 67460.50 64112.81 66696.62 69517.62 69258.46 71568.27 
                                                                        
72723.86 71013.24 69111.50 68686.41 72603.23 75311.08 76058.63 75398.07 
                                                                        
71069.42 66292.22 66230.34 71666.15 73114.12 72980.89 74363.44 74225.69 
                                                                        
73024.88 72999.23 76924.52 76453.34 76683.70 76201.31 75742.66 70453.00 
                                                                        
68361.30 71210.06 69068.45 67003.38 65438.80 62510.84 56878.07 57533.09 
                                                                        
65310.06 68927.69 71144.88 70448.27 68392.76 62742.45 62346.03 69635.77 
                                                                        
70639.77 69624.58 69100.94 64140.28 57943.95 59540.93 65785.11 66941.95 
                                                                        
67026.98 66183.20 64218.84 60036.07 60465.12 61480.81 65863.40 66047.07 
                                                                        
67198.48 65766.15 61273.89 58023.69 63955.41 63861.42 63046.70 61499.01 
                                                                        
58963.59 52942.40 49619.22 53440.14 53752.58 52427.93 51730.46 51154.07 
                                                                        
47227.67 47010.36 52178.95 51989.73 50541.38 49813.64 49526.29 47605.69 
                                                                        
49356.80 54618.11 54169.57 46920.81 51331.69 50609.62 46307.12 45549.30 
                                                                        
49970.22 47178.41 44556.09 43882.60 44899.08 42685.64 43356.86 48927.50 
                                                                        
48142.62 48231.65 50298.77 49311.08 45210.48 45559.88 47619.52 52967.13 
                                                                        
53280.62 53330.55 53223.67 48032.73 45728.27 51537.32 52485.16 53538.12 
                                                                        
54466.83 51657.05 45160.81 43765.57 49220.04 49991.56 49818.07 49163.32 
                                                                        
46395.26 41219.62 41403.03 46934.05 47682.72 47747.84 47433.44 45224.20 
                                                                        
40499.55 40625.66 46890.88 48090.29 48082.64 47594.42 45252.21 40434.10 
                                                                        
40974.34 46776.71 47670.88 47578.79 47408.46 45464.59 40863.56 40768.93 
                                                                        
46509.11 47617.66 47877.93 47697.95 45801.32 41033.91 41061.40 47397.11 
                                                                        
48693.00 48808.56 48311.13 46285.61 41421.86 41426.11 47730.14 48939.25 
                                                                        
48844.96 48620.95 46566.92 41742.80 42036.80 48370.00 49188.28 48928.28 
                                                                        
49069.01 46768.94 41593.42 41320.49 45600.18 46310.53 46870.36 47071.97 
                                                                        
44753.95 39755.66 39490.43 43681.36 44067.52 43579.77 42874.91 41077.58 
                                                                        
37854.85 37879.10 41539.41 41771.20 41564.93 38855.62 39861.61 37955.46 
                                                                        
38011.20 42014.79 42639.19 41840.97 43024.06 42675.80 39114.37 38627.14 
                                                                        
43652.51 45261.90 45599.84 45938.12 44489.66 39905.16 39744.55 45687.15 
                                                                        
47226.27 47879.24 47886.49 45616.98 40364.44 40446.12 46353.33 47309.21 
                                                                        
48099.71 48181.30 46000.66 41176.91 41327.04 47111.26 49088.16 49130.45 
                                                                        
48845.63 46349.26 41402.81 41554.49 47445.39 48362.84 48487.27 48172.38 
                                                                        
46020.30 41167.76 41370.63 47036.18 48313.87 48537.14 48458.25 46302.12 
                                                                        
41379.85 41682.89 47545.18 48801.16 49159.67 50230.42 49666.54 46080.41 
                                                                        
46826.45 51987.64 52701.89 53311.54 52708.52 51028.96 46064.21 45387.98 
                                                                        
50607.95 51317.10 50757.22 50873.54 48648.67 43235.12 43798.29 49546.48 
                                                                        
51740.81 53389.25 53939.10 47782.42 45708.19 47455.05 55120.30 58261.25 
                                                                        
56525.07 53490.31 53023.26 50323.91 51540.04 55149.96 59557.45 59678.64 
                                                                        
61673.24 61563.28 57576.48 56591.57 64457.34 66070.05 69524.41 69950.60 
                                                                        
69624.09 64030.06 63218.86 68636.89 71195.71 72196.66 74785.24 71122.62 
                                                                        
64436.44 64731.60 70893.38 73159.30 74906.98 73057.91 69452.19 64801.56 
                                                                        
65641.41 73571.08 74976.89 74228.69 73859.72 71456.84 65497.20 66094.16 
                                                                        
70828.69 70741.95 69616.45 68904.55 67037.37 63299.66 61457.73 63375.16 
                                                                        
60964.62 56279.89 61871.16 61514.10 57891.92 58728.12 62580.00 59715.76 
                                                                        
57349.82 60221.00 60026.62 57940.38 59599.48 63469.71 61275.19 61739.50 
                                                                        
62785.95 62965.57 58882.32 59960.65 64978.78 66754.77 68322.61 67263.53 
                                                                        
64581.16 60447.32 60515.35 68479.36 71146.94 71030.16 69940.62 67514.79 
                                                                        
62181.71 62435.34 69900.20 72343.85 72213.45 71060.84 69598.37 63548.48 
                                                                        
64253.10 70347.76 70483.83 70981.70 70429.76 66052.12 62577.35 63302.90 
                                                                        
67916.92 70139.97 70010.58 68700.76 66828.81 59944.51 60262.67 66050.38 
                                                                        
67767.97 66797.99 65637.95 63160.97 59364.07 57936.11 63153.13 64506.08 
                                                                        
64759.20 66790.88 65557.23 59971.28 59616.78 65524.90 66032.14 65205.51 
                                                                        
64896.84 62060.96 55446.98 54108.88 59270.98 59720.50 59574.13 58951.35 
                                                                        
55764.25 51001.38 50378.92 55859.77 57524.65 56769.95 55774.02 53915.68 
                                                                        
51089.12 53376.13 59597.57 60360.86 59889.78 59810.94 57704.23 51756.38 
                                                                        
50893.34 55578.89 55456.85 55133.11 55213.47 52737.38 47355.14 46176.86 
                                                                        
50726.97 52064.15 53081.33 52517.27 49928.18 44851.07 44807.59 50784.03 
                                                                        
52395.68 52590.59 51854.77 49511.90 46199.00 46358.43 46911.30 51729.23 
                                                                        
51718.48 51022.77 49820.84 45756.53 44523.12 51438.22 52229.93 51323.57 
                                                                        
45152.84 47264.63 45348.39 45517.86 49891.94 49883.95 47446.61 44635.53 
                                                                        
45741.34 43139.03 43537.25 49578.69 50845.39 48887.91 49120.69 47691.23 
                                                                        
42573.05 42425.22 47955.41 48752.17 49090.87 48622.46 46534.94 41859.60 
                                                                        
41902.46 47733.50 49068.50 48913.93 43787.23 43774.58 40813.02 41355.11 
                                                                        
47087.12 48003.91 46501.78 46908.94 45602.67 40788.15 40518.40 42237.61 
                                                                        
47438.84 47751.79 47951.62 45946.14 40478.43 38609.32 46095.14 47296.32 
                                                                        
47496.10 47574.01 45493.61 40679.70 40867.03 46784.30 47786.86 47766.36 
                                                                        
47692.29 45758.46 40990.01 40850.37 46292.04 47491.55 47686.09 48048.50 
                                                                        
45951.11 40873.77 40776.32 46235.18 47403.18 47933.98 48116.15 45978.79 
                                                                        
40933.83 40711.91 41477.65 46950.96 48293.07 48721.48 46893.94 41582.66 
                                                                        
38923.20 46062.43 47758.75 48396.76 48553.25 45702.73 40544.20 40609.91 
                                                                        
45477.93 46196.08 46356.59 45876.12 43534.16 39257.91 38913.44 42543.66 
                                                                        
43164.16 43319.95 43180.48 41692.53 37879.05 37813.92 40992.00 41464.40 
                                                                        
41543.42 41304.98 37558.23 36982.48 37137.89 41147.69 42076.93 42221.23 
                                                                        
41413.80 41543.43 38467.28 38441.73 44083.51 45498.67 45552.63 45829.48 
                                                                        
44328.91 39970.57 40199.18 45966.34 47147.16 47377.20 47455.40 45709.52 
                                                                        
40907.29 41077.02 47114.60 48031.98 48174.99 48022.59 45793.51 41054.84 
                                                                        
41307.34 47180.13 48120.62 48283.77 48038.91 46148.52 41191.95 41296.50 
                                                                        
46753.15 47994.36 48551.73 48319.40 46200.55 41278.10 41317.20 47397.42 
                                                                        
48524.79 48515.75 48266.82 46169.29 41350.97 42104.68 48570.41 49636.11 
                                                                        
49921.82 49327.89 47185.03 42554.05 43725.90 49155.52 50282.66 50474.66 
                                                                        
50217.72 47805.94 42906.29 42558.29 48452.53 50417.97 51682.29 52146.70 
                                                                        
49943.28 44586.20 44773.63 50907.52 52874.77 53097.20 52360.45 49649.43 
                                                                        
44348.09 45855.90 52846.08 54842.79 56575.25 57432.06 55205.21 50237.54 
                                                                        
50960.29 55175.30 53653.86 58462.24 59377.02 57175.71 52142.07 52343.93 
                                                                        
57687.97 60406.37 61718.34 61533.80 58384.19 52612.19 52492.80 58553.58 
                                                                        
60515.55 60349.93 59782.95 56793.13 52436.98 54550.64 62732.23 65194.88 
                                                                        
66682.82 66518.57 64513.61 60899.57 63361.24 68992.53 71669.23 71232.41 
                                                                        
69025.66 66756.12 62135.71 63566.55 69002.73 68071.68 68020.17 64904.34 
                                                                        
61785.91 58797.76 60469.08 64555.78 65225.88 61731.97 56673.76 60852.48 
                                                                        
61055.33 64611.23 71255.12 69651.28 66376.34 65434.74 66782.34 64089.37 
                                                                        
63097.51 71471.89 73784.42 71752.88 70398.11 66456.16 57830.34 61172.95 
                                                                        
69973.51 69669.70 68899.69 67981.48 64750.54 62576.96 65691.99 72039.28 
                                                                        
73882.56 74481.33 74427.29 74308.20 70146.52 70117.16 74834.23 74535.95 
                                                                        
74771.47 75003.18 73735.88 68793.34 69645.43 75128.71 77121.06 78381.89 
                                                                        
79339.08 79503.34 73519.70 71778.98 76633.16 77060.66 76207.84 76602.12 
                                                                        
72728.09 65707.03 65465.84 71833.49 71823.36 72117.66 71527.69 68626.29 
                                                                        
63875.49 65610.03 69871.91 71149.30 70059.83 68701.71 67130.09 63785.99 
                                                                        
61221.67 65623.55 66149.37 67046.75 67804.59 65648.91 58873.84 57182.18 
                                                                        
61529.84 63238.41 62666.82 62365.05 61096.08 57435.06 58302.16 63030.96 
                                                                        
61634.38 60117.00 61602.73 61000.63 57476.07 58081.41 61909.70 62336.97 
                                                                        
64316.95 65157.23 61205.93 55598.86 55002.71 59736.38 60583.27 62180.64 
                                                                        
61811.44 58424.00 53863.15 54207.92 54608.86 58823.00 58400.71 56215.34 
                                                                        
53277.52 48983.18 46936.74 52200.59 51810.48 50621.65 50028.49 48629.35 
                                                                        
44602.81 44806.67 49450.90 49371.12 49088.53 49141.24 47474.08 43200.91 
                                                                        
43626.06 49618.19 51121.06 51539.45 51438.58 44491.64 44189.93 44567.70 
                                                                        
49573.90 49777.34 50265.21 48169.62 43632.68 42015.53 42161.48 47072.82 
                                                                        
47969.37 48122.57 42118.48 44195.41 41948.92 41882.50 47250.54 48716.15 
                                                                        
47797.66 48903.41 47391.34 42251.99 41625.62 43344.49 48399.00 48938.65 
                                                                        
48689.81 46569.98 41497.04 40060.46 47292.77 48509.71 48641.89 48761.88 
                                                                        
46966.80 41650.25 41492.78 47190.54 48115.80 48337.37 48725.38 46186.46 
                                                                        
40974.79 41095.53 46671.17 47538.98 47896.47 47998.32 45406.84 40785.71 
                                                                        
40929.39 46995.03 47848.34 48079.10 48038.04 46438.76 41623.49 42012.41 
                                                                        
48040.48 49570.12 50205.37 50331.33 48514.81 43795.12 43274.27 49500.27 
                                                                        
50841.10 49928.79 49579.30 47850.04 42992.73 42779.52 46552.71 44243.25 
                                                                        
49103.18 50170.35 48339.30 42933.23 41672.62 46671.12 49994.73 50021.35 
                                                                        
49406.37 47105.74 41612.52 41780.22 46972.20 47494.50 47192.82 46507.43 
                                                                        
43988.59 39725.25 40027.25 44387.66 44522.83 44665.71 44847.20 43244.30 
                                                                        
39511.75 38889.96 42017.00 43108.50 43844.48 43570.23 41528.06 37433.80 
                                                                        
37504.28 41629.55 42802.86 43007.89 43135.59 42054.55 39051.75 38672.09 
                                                                        
43657.99 44722.84 45622.56 46242.60 44881.26 40933.92 41072.72 46783.45 
                                                                        
47345.15 47245.68 47288.19 45649.00 41126.61 41625.36 47028.96 47777.86 
                                                                        
47990.41 47866.81 45983.51 41437.75 41758.98 47109.86 48395.22 49061.25 
                                                                        
48736.45 46547.54 41928.49 42058.30 47744.77 49235.69 49460.60 49368.07 
                                                                        
47069.90 42452.29 42800.95 48506.48 49725.07 49983.63 50044.98 48328.15 
                                                                        
43683.79 44191.32 49753.00 50031.75 50483.71 50668.85 49041.02 44799.38 
                                                                        
45128.35 51149.56 53321.95 55756.91 57576.13 56510.61 50571.16 50337.52 
                                                                        
56299.01 57956.75 59215.24 58892.47 55287.65 50035.78 49684.02 55193.63 
                                                                        
55661.75 55706.38 55652.48 53245.95 48170.21 48685.69 55198.00 56255.63 
                                                                        
55647.81 54815.55 52091.35 46607.88 47488.44 52929.88 54122.80 50574.33 
                                                                        
54582.77 53243.98 49810.95 49499.32 55389.48 54625.93 55544.95 56140.96 
                                                                        
55352.62 53931.44 57380.43 64738.05 67100.79 64607.49 63992.70 64635.09 
                                                                        
61204.33 61373.41 65844.73 64676.81 65164.32 66749.25 63953.95 59736.24 
                                                                        
57996.31 61842.70 63622.16 64989.27 67584.91 65620.06 58863.46 59768.58 
                                                                        
65573.39 65397.77 64691.62 63137.01 59898.98 55663.92 56178.62 60332.71 
                                                                        
59936.91 58703.20 56247.84 50209.26 51157.48 53222.00 57439.81 57396.27 
                                                                        
57354.30 54575.61 53010.54 54301.80 56856.35 62294.32 64632.44 64746.81 
                                                                        
63630.51 63482.26 58712.32 59859.62 66527.63 68758.25 70316.08 70724.36 
                                                                        
70037.83 66576.66 67526.41 75384.38 76539.18 76444.12 77200.80 74949.28 
                                                                        
66107.90 65884.24 69925.91 69522.30 69401.61 68899.84 66202.45 61178.30 
                                                                        
59508.20 62997.79 63787.76 65996.55 68514.84 63909.13 58032.58 58131.13 
                                                                        
64364.16 66805.66 69828.91 68645.73 66983.35 61011.80 60847.58 69453.70 
                                                                        
72405.78 73568.80 73548.60 69250.23 62623.15 60415.00 66175.62 67759.16 
                                                                        
69317.82 69013.11 67435.17 62229.16 62857.36 69275.44 70150.23 68154.20 
                                                                        
69507.49 67156.83 62979.91 64482.13 70344.02 70489.84 70182.95 68960.00 
                                                                        
66069.38 60900.77 61553.38 66758.66 66382.67 67713.73 66783.45 63282.13 
                                                                        
58871.85 58904.75 63087.09 63538.39 64119.10 63009.62 60805.54 54645.30 
                                                                        
54525.14 55388.30 60448.46 60043.41 60036.72 58895.86 54192.80 49977.00 
                                                                        
56260.07 56915.38 57318.66 58660.23 57892.91 52195.49 51089.76 54816.49 
                                                                        
55080.12 55156.87 55651.66 53173.27 48371.76 49145.32 55012.35 54668.65 
                                                                        
53435.12 52431.76 50264.78 47847.70 50920.27 57236.39 57106.83 57527.09 
                                                                        
56391.59 54459.91 51258.86 51797.05 55330.05 54910.68 53821.57 47765.19 
                                                                        
46920.53 43882.04 43706.57 48929.11 49587.35 47641.30 48175.04 47106.58 
                                                                        
43026.66 43555.30 44968.93 48952.38 49382.84 49765.78 47788.11 42523.37 
                                                                        
41255.40 48679.73 49501.95 49005.53 48030.59 45621.10 41116.46 41759.49 
                                                                        
47883.71 49081.22 48544.48 48680.12 46752.92 41874.30 41883.91 47117.85 
                                                                        
47499.97 47670.84 47521.99 45369.23 40772.67 41040.66 46312.80 46859.36 
                                                                        
47350.63 47356.59 45549.41 41161.04 41370.84 46854.96 48024.76 48379.23 
                                                                        
48384.60 46155.71 41259.29 41430.49 46939.23 47945.37 48277.12 48250.36 
                                                                        
46276.12 41379.10 41544.83 47282.73 48307.95 48540.75 48781.41 46959.66 
                                                                        
42535.77 42911.51 47999.57 48277.29 47885.16 43353.35 43752.65 41322.50 
                                                                        
41900.29 47788.93 49046.31 47200.21 47660.90 46002.77 41519.55 41939.87 
                                                                        
47649.27 48288.59 48088.36 47667.36 45439.88 41109.95 40916.40 44555.23 
                                                                        
45308.56 45579.25 45152.88 42732.50 39214.34 39270.65 42949.61 43302.70 
                                                                        
42840.09 42561.62 41777.32 39377.79 39033.39 39770.84 43220.81 43796.80 
                                                                        
43866.57 42992.06 39632.36 37915.73 44191.70 46125.11 46816.91 47266.05 
                                                                        
46062.13 41994.29 41525.18 46926.65 48094.02 48668.58 48799.16 47239.25 
                                                                        
42426.32 42470.30 47870.81 49142.78 49083.13 48891.73 46947.71 42344.77 
                                                                        
42512.86 48347.06 49201.31 48656.07 48081.94 46043.73 41506.80 42214.77 
                                                                        
47491.84 48073.40 47941.37 47797.03 45874.11 41562.18 41689.72 47179.00 
                                                                        
48160.38 48146.25 48005.66 46180.45 42004.68 42524.51 47960.90 49015.01 
                                                                        
49408.60 50084.89 49182.97 44802.38 45587.25 52443.42 54100.82 55398.79 
                                                                        
56329.59 53938.00 48053.19 48253.19 53570.30 55141.49 56504.94 56678.85 
                                                                        
54584.62 50332.85 50844.75 54827.48 55364.29 55809.34 56211.69 54902.79 
                                                                        
50501.02 49857.73 52713.37 51237.75 56498.41 59064.82 58378.77 52460.84 
                                                                        
53498.69 60945.77 64795.58 65681.84 64283.72 56978.85 58660.87 59265.69 
                                                                        
65503.45 65898.12 63825.55 60840.60 61681.99 57790.27 57182.09 61159.70 
                                                                        
62247.82 61914.35 61698.67 61281.30 56688.27 57078.44 65204.37 69372.48 
                                                                        
69761.88 70456.06 68897.33 63697.55 65593.88 71009.71 71349.37 71859.20 
                                                                        
71662.44 68637.20 64655.91 64161.97 69183.57 70057.19 69861.09 70007.98 
                                                                        
66855.65 63623.30 65461.25 71648.41 71842.64 71751.66 68822.22 65981.10 
                                                                        
60566.14 59437.74 62534.87 66175.12 67669.25 69328.94 68600.93 66761.33 
                                                                        
65844.85 70671.48 75084.79 77200.05 76711.11 78688.28 75493.80 71621.56 
                                                                        
76109.05 76464.97 74961.23 74855.96 75692.15 71057.86 71151.37 75923.01 
                                                                        
79538.13 81745.06 82979.96 81261.78 76298.70 75917.50 81684.76 84077.61 
                                                                        
85317.95 84892.33 79513.09 70478.69 69901.61 74068.70 72459.78 71594.98 
                                                                        
69964.58 67084.66 63240.26 63682.40 68883.97 68382.81 69435.56 71359.62 
                                                                        
70691.22 67364.36 66479.00 68515.45 68085.09 66753.59 66724.38 65561.25 
                                                                        
61032.43 61000.67 64834.46 63263.57 62888.30 63408.16 62477.62 59853.53 
                                                                        
58752.84 63301.93 65784.24 66394.36 64093.94 61895.99 57447.64 59729.66 
                                                                        
65883.54 66918.35 65188.41 61564.06 58491.70 54125.33 54616.58 60125.06 
                                                                        
60822.05 58851.80 57263.26 54898.08 52026.60 51627.40 57081.13 58157.36 
                                                                        
59405.14 59544.60 58447.59 52305.23 51468.69 55503.84 56011.08 55550.93 
                                                                        
54514.16 51521.12 48126.53 48678.15 52929.55 53011.23 52948.20 53226.27 
                                                                        
51782.99 46331.27 45274.35 50007.35 51560.29 52322.57 52158.43 49377.82 
                                                                        
45441.41 45510.31 47115.73 52910.76 54919.15 55302.89 52955.33 47313.86 
                                                                        
45407.89 52226.52 54622.19 57516.23 58253.26 55607.31 49116.91 48111.06 
                                                                        
48921.41 55228.02 56419.36 55761.26 52212.06 47352.72 45391.33 48501.53 
                                                                        
52871.41 52424.57 51581.39 49352.00 44385.34 42718.82 49059.71 49186.61 
                                                                        
49028.09 48938.04 47169.20 42907.91 42714.15 47395.66 47927.27 48006.11 
                                                                        
43655.68 43724.32 41319.72 41744.65 47390.52 48096.36 46309.50 47119.05 
                                                                        
46195.33 41504.68 41194.40 42675.73 47208.66 47814.80 48011.95 46072.35 
                                                                        
41399.37 40186.46 47182.27 48521.94 48979.25 49231.20 47035.96 42530.08 
                                                                        
42937.48 49322.17 50853.37 51444.66 51370.23 48352.10 43285.49 43437.48 
                                                                        
49283.57 50312.09 49601.00 48488.36 46345.70 41980.96 42377.76 47842.71 
                                                                        
48678.58 49042.34 49419.24 48069.54 43282.96 42831.69 48003.43 48925.46 
                                                                        
49593.70 49122.12 42959.35 41516.83 42470.35 48505.30 50031.96 49750.23 
                                                                        
47060.86 46169.57 42065.65 42399.48 47569.55 48231.36 48089.15 47761.02 
                                                                        
45586.71 41437.21 41401.20 45494.03 46356.05 46678.55 46628.70 44927.02 
                                                                        
40978.03 40212.03 43687.45 43967.81 43749.39 43586.50 42157.18 39015.82 
                                                                        
38842.65 41427.45 40022.91 42697.00 43444.70 42663.69 39384.06 38870.70 
                                                                        
42770.38 45109.83 45745.48 46080.98 44965.26 41299.93 41348.20 47351.69 
                                                                        
48657.99 48633.57 47618.98 45747.56 41453.75 42208.44 47687.37 48630.69 
                                                                        
48226.92 47898.62 46357.77 41872.88 42191.95 47782.89 48311.52 49263.20 
                                                                        
49234.28 47407.40 43366.71 44276.53 49720.14 51011.39 51067.83 50332.76 
                                                                        
48411.88 43993.18 44069.40 49498.34 50260.07 50092.79 49758.30 47601.85 
                                                                        
43238.68 44022.41 49359.55 49960.94 50401.96 50611.32 49048.39 45547.89 
                                                                        
45520.93 50507.44 51162.54 51331.15 50940.72 49128.60 44389.54 44395.62 
                                                                        
49453.52 50300.51 50453.70 50204.36 48642.73 44528.96 45885.81 52040.57 
                                                                        
52140.09 51543.41 51728.54 50057.60 47463.08 48748.52 54431.21 56625.93 
                                                                        
52747.92 55337.47 53672.31 49562.38 51609.12 60721.51 61909.26 63540.07 
                                                                        
62646.01 59986.72 53995.70 55636.94 64955.05 67313.17 67052.95 66593.00 
                                                                        
63175.62 59014.50 60615.70 67103.87 66531.34 64923.61 62766.70 60983.03 
                                                                        
58365.17 62571.12 68489.36 68373.97 68935.49 70789.84 70909.16 69329.70 
                                                                        
70778.66 73385.57 73448.02 74623.89 75602.20 72909.41 70733.57 69029.75 
                                                                        
71786.76 74316.36 75273.62 72564.28 71737.86 67932.62 68572.58 73390.05 
                                                                        
75299.39 73947.77 71247.03 67373.91 62379.45 62950.71 63502.75 66773.71 
                                                                        
67566.22 68847.30 67066.28 59979.70 56902.63 58854.80 64362.99 65806.80 
                                                                        
65458.31 62138.48 58748.36 58635.46 66421.97 67668.58 66609.31 67141.03 
                                                                        
67218.68 64359.42 65120.92 70450.31 68603.44 70279.70 70078.95 68397.03 
                                                                        
64683.68 63127.61 65957.17 66952.58 65897.78 66600.38 66688.52 63049.04 
                                                                        
61778.98 66019.44 65990.52 67213.17 69663.91 70357.73 65828.02 66639.86 
                                                                        
74348.70 75721.30 77432.40 78206.25 76559.11 70998.77 70435.23 75895.30 
                                                                        
79522.72 78893.74 74506.80 70240.06 64636.66 66326.59 73321.67 72943.92 
                                                                        
73103.04 74866.23 73757.62 69859.70 70723.88 78768.95 83121.50 86003.48 
                                                                        
82745.38 75854.62 68339.42 68243.05 73292.99 73809.80 73061.21 70817.52 
                                                                        
65510.48 57947.32 56330.69 62604.86 64542.97 64503.43 63202.41 60115.63 
                                                                        
57852.82 61819.77 69924.55 71281.24 70470.78 69364.48 66866.25 61311.70 
                                                                        
61175.88 66132.45 67309.71 65953.31 65357.08 62898.24 58192.89 57620.55 
                                                                        
57421.88 59850.61 60205.61 60541.73 57781.38 51184.38 48577.29 55063.27 
                                                                        
56716.75 57500.00 56893.82 53453.86 48455.48 48345.55 53198.66 53316.37 
                                                                        
51490.57 50203.93 48491.87 43793.65 43551.73 47957.56 48211.37 48482.77 
                                                                        
49045.39 47541.07 43236.05 44554.56 50345.31 47253.90 52101.26 51706.39 
                                                                        
49219.16 44450.87 43861.07 45519.78 45118.34 47139.44 44710.86 44983.51 
                                                                        
42278.20 43278.82 48563.09 49824.79 48026.23 48551.44 47067.96 43128.59 
                                                                        
43547.90 44664.77 48351.26 48541.22 48354.96 46734.82 41985.51 40481.69 
                                                                        
47027.04 47770.98 48237.97 48206.59 46194.71 41581.25 41681.96 47111.77 
                                                                        
48086.20 48274.91 47904.13 46081.73 41793.67 42093.07 47421.81 47879.88 
                                                                        
47643.00 47835.46 46195.16 41702.35 41748.75 46911.40 48106.44 48867.20 
                                                                        
48469.70 45789.17 41277.55 41677.82 47365.77 48809.02 48944.10 48499.44 
                                                                        
46899.43 43090.03 43324.69 48735.44 49860.50 49825.14 49115.25 47439.87 
                                                                        
43319.42 43618.24 49168.24 49835.48 49227.31 49254.20 47801.25 42773.54 
                                                                        
43417.71 48279.57 49169.00 49564.85 49744.83 47197.18 42458.65 42582.62 
                                                                        
48283.04 49686.51 50242.42 50212.11 47759.46 42486.54 42853.91 47783.89 
                                                                        
48995.49 49184.03 49109.54 47094.59 43247.77 43164.78 46957.88 47351.26 
                                                                        
46559.15 45559.43 43464.98 40557.68 40512.86 43532.10 43613.65 41150.98 
                                                                        
43351.98 42360.07 39784.53 39878.17 44227.19 44590.39 45999.36 45883.68 
                                                                        
44154.50 40445.24 40611.17 45955.57 47502.47 47549.07 47053.90 45393.70 
                                                                        
41255.25 41614.70 46930.80 48042.38 48245.57 48030.82 46151.36 41747.07 
                                                                        
42110.72 47458.31 48713.49 48846.48 48557.28 46408.00 42010.02 42372.32 
                                                                        
47868.99 48680.13 48856.88 48561.64 46600.11 42020.89 42884.16 47795.15 
                                                                        
48977.88 49368.15 49084.43 46852.68 42888.82 43493.94 49083.51 51006.04 
                                                                        
50425.43 49802.57 47846.78 43229.62 44290.93 50134.24 50455.10 50081.00 
                                                                        
49537.85 47414.68 43053.55 43538.81 48596.19 49441.30 49629.70 49449.85 
                                                                        
47715.38 43692.51 44547.84 50522.16 52261.07 52289.13 51741.47 51218.43 
                                                                        
49216.97 52851.11 61035.06 61553.41 60071.22 54434.46 55829.73 55845.48 
                                                                        
56953.35 60060.74 59461.75 58221.31 59626.89 58499.13 53276.66 52251.96 
                                                                        
56834.79 58135.05 59346.55 59123.46 57982.41 55064.11 57228.50 65767.64 
                                                                        
68456.00 68506.28 67851.17 65601.74 60115.18 62535.99 69177.11 70152.26 
                                                                        
68387.08 66170.17 63076.34 59560.49 58795.66 62441.56 62614.49 63278.09 
                                                                        
62091.39 60961.66 57743.36 57288.38 63562.01 67068.09 69181.72 71541.86 
                                                                        
71556.30 67675.19 65530.97 69563.55 71644.22 70768.86 70614.00 67090.38 
                                                                        
59425.83 58657.96 59922.59 59356.59 66545.50 68135.95 65497.19 62541.98 
                                                                        
60298.59 60960.46 61223.23 67592.05 71864.23 72793.79 68598.50 66864.45 
                                                                        
69796.67 71269.99 74178.07 77267.85 75333.31 68978.86 66190.89 70003.94 
                                                                        
72234.58 74801.72 73463.94 73691.38 68787.70 68171.89 75558.09 78006.17 
                                                                        
77067.84 79235.52 76027.33 68127.14 68701.41 75997.69 77757.62 78602.75 
                                                                        
78571.16 72688.80 68601.36 70846.17 78241.79 76402.73 74668.58 72174.57 
                                                                        
70434.78 65174.58 66254.75 71982.75 72560.45 72114.94 70635.66 66435.88 
                                                                        
61100.07 61065.42 66355.39 67500.16 67561.48 65872.90 62420.30 56447.13 
                                                                        
57350.40 62522.98 62322.24 61260.89 60882.97 59022.14 55350.56 55386.46 
                                                                        
60982.60 61848.20 61810.43 61426.02 60327.24 55161.59 54848.05 62161.87 
                                                                        
64060.48 64146.20 64847.83 60565.09 53862.90 56019.14 63602.85 64089.45 
                                                                        
63761.02 61370.34 57455.41 52836.41 53758.23 59666.47 60743.19 60097.20 
                                                                        
58690.30 55434.89 49996.85 50471.68 55147.05 56370.76 59541.95 60586.48 
                                                                        
57767.10 52738.45 52228.27 56839.47 58196.53 59089.76 58937.09 57229.74 
                                                                        
53500.30 54536.64 58147.51 57395.14 56175.60 54189.06 51270.37 47063.23 
                                                                        
46635.43 46712.42 50850.20 51011.11 51454.06 50070.71 46199.21 46078.01 
                                                                        
52050.74 51772.00 46474.81 51357.19 50981.18 49177.65 51015.90 55055.95 
                                                                        
51681.25 49239.18 52947.39 51859.93 47914.21 48211.18 52122.45 50145.48 
                                                                        
51262.01 51025.58 49543.73 45596.45 46039.38 50493.52 50316.44 49731.27 
                                                                        
49348.08 47428.82 43219.06 43492.73 48157.36 48934.61 49038.17 44336.32 
                                                                        
44003.05 41672.28 42052.46 47168.61 48139.36 46520.32 46892.48 45997.86 
                                                                        
41839.36 41923.45 43630.27 48215.04 48741.78 48349.16 46512.20 41950.09 
                                                                        
40782.41 47393.69 48542.77 48386.63 48001.85 46000.82 41790.29 42359.04 
                                                                        
48399.70 49900.68 50473.27 50754.05 48732.53 44720.13 44830.88 49604.26 
                                                                        
50282.71 50610.39 50637.91 49454.86 44858.86 44373.69 49018.27 49018.09 
                                                                        
49459.66 49838.04 48129.49 43544.16 43163.62 47629.43 48956.43 49382.02 
                                                                        
49280.38 47607.34 43211.17 43375.76 49324.45 51054.39 51526.01 51398.98 
                                                                        
48127.74 42251.81 41938.21 46908.03 47848.89 47685.72 47080.73 44616.18 
                                                                        
40781.91 41167.27 44649.92 44995.63 44684.35 44974.07 44019.84 40357.37 
                                                                        
39918.41 42549.36 42702.88 43069.35 40777.41 41077.52 39396.28 39433.05 
                                                                        
42795.41 43599.79 42906.95 43808.60 43584.81 40814.46 40896.70 46110.12 
                                                                        
47286.65 47574.72 47618.83 46456.52 42469.71 42482.36 47008.56 48068.25 
                                                                        
48268.89 47890.07 46137.93 41705.00 42038.04 47324.21 48121.89 48106.72 
                                                                        
48029.89 46124.83 41934.89 42409.73 47504.59 48182.60 48044.12 47779.16 
                                                                        
46080.00 41896.46 42082.88 46782.37 47602.99 48071.86 48227.33 46229.73 
                                                                        
42009.01 42535.64 47522.20 48601.43 48975.00 49812.45 48126.53 43821.55 
                                                                        
44129.27 49407.00 50212.18 50975.25 50918.19 48733.35 44002.94 44153.09 
                                                                        
48899.54 50148.91 51145.16 50970.61 48960.43 44852.37 45601.67 51310.75 
                                                                        
53444.79 52996.36 52263.60 50414.11 46046.59 47205.75 53866.74 54938.39 
                                                                        
54998.31 54397.05 48125.54 46973.18 49456.04 55381.34 57439.66 57960.22 
                                                                        
57371.71 58413.18 55499.02 56399.01 58486.16 63720.76 65469.75 67438.55 
                                                                        
67066.49 62518.12 61173.05 68567.41 69884.90 71310.84 70707.29 66384.97 
                                                                        
60109.62 60769.70 65113.87 65082.82 64401.20 63436.41 61270.43 59423.00 
                                                                        
60822.94 67246.23 69833.84 70844.28 70667.53 69411.73 61831.74 61981.34 
                                                                        
69023.43 71816.79 72298.80 72132.21 66894.24 60557.05 60417.76 65733.11 
                                                                        
64932.83 65294.08 63388.94 60334.21 57454.20 58287.72 60815.19 58587.95 
                                                                        
54750.08 59384.50 58395.30 57257.63 60714.58 64026.85 63114.17 60934.01 
                                                                        
63759.77 62661.99 61552.08 63023.18 69749.68 67077.02 66188.20 64608.69 
                                                                        
64475.07 62507.72 64623.65 68781.29 67577.99 66044.30 65598.67 64808.67 
                                                                        
63295.21 65840.37 72127.94 72918.05 73130.12 72833.41 71010.71 66700.77 
                                                                        
65897.16 70846.72 71758.48 71076.11 69884.01 64096.61 58327.59 58054.83 
                                                                        
62333.23 65669.04 67314.06 67176.00 65296.08 58846.60 58307.50 63024.93 
                                                                        
65940.65 67718.61 68174.69 63258.96 58223.51 56314.73 62720.97 65588.22 
                                                                        
66231.17 64584.22 63127.72 57789.22 56734.71 61353.62 63251.74 66591.98 
                                                                        
67814.70 65367.21 58144.10 58676.91 64933.41 68382.55 68708.38 66818.03 
                                                                        
65082.54 60637.93 60156.41 65361.08 65514.34 61666.80 61774.68 60848.48 
                                                                        
56624.24 55465.68 59308.21 56421.11 53635.69 52257.82 50155.05 48631.98 
                                                                        
50514.06 52317.18 52015.41 52000.55 52229.41 51409.45 48304.70 50029.40 
                                                                        
55345.68 54830.79 53926.65 53325.38 50807.92 47965.01 47158.95 48570.65 
                                                                        
48169.63 47084.83 45933.53 43804.77 40779.74 40595.57 41351.14 44467.07 
       1        2        3        4        5        6        7        8 
50491.69 58421.18 63466.78 66256.69 65500.66 67397.93 63482.11 60799.28 
       9       10       11       12       13       14       15       16 
64777.75 67858.55 68851.50 71071.41 71991.48 70663.42 71398.15 76807.77 
      17       18       19       20       21       22       23       24 
78492.68 77247.05 71937.44 68965.07 63029.05 60518.89 64917.09 67468.92 
      25       26       27       28       29       30       31       32 
67762.02 67778.81 69089.87 67285.45 67497.31 75005.62 78907.44 81090.03 
      33       34       35       36       37       38       39       40 
85788.02 86875.18 87566.32 86846.29 87028.68 87754.49 89149.23 89225.71 
      41       42       43       44       45       46       47       48 
90047.30 90079.67 89257.43 85636.68 80892.27 77002.73 73959.65 72142.32 
      49       50       51       52       53       54       55       56 
68607.89 66874.39 73586.55 75845.26 75118.42 71374.93 67981.09 61491.13 
      57       58       59       60       61       62       63       64 
59583.91 65859.81 66664.12 64515.29 62733.00 61590.85 57148.54 55635.06 
      65       66       67       68       69       70       71       72 
64325.49 68674.68 68966.20 68054.54 66986.72 61204.79 56532.15 60639.90 
      73       74       75       76       77       78       79       80 
59321.40 58597.78 56993.19 57512.18 53851.29 53532.44 61906.98 63242.94 
      81       82       83       84       85       86       87       88 
60963.00 57313.33 55356.08 50258.36 47233.40 49672.27 51368.43 51991.20 
      89       90       91       92       93       94       95       96 
51676.17 50972.50 47437.39 46593.37 52295.80 54470.64 55353.54 56315.06 
      97       98       99      100      101      102      103      104 
56571.60 51987.52 50348.74 53464.60 54750.85 58266.59 59061.43 58189.54 
     105      106      107      108      109      110      111      112 
55464.77 54082.75 58060.19 61797.51 61095.80 60431.25 59884.77 54646.61 
     113      114      115      116      117      118      119      120 
52827.40 58404.06 60278.56 58500.38 56306.61 54323.95 49247.17 46434.27 
     121      122      123      124      125      126      127      128 
51564.72 49172.35 49960.07 51278.53 51270.68 47528.02 45509.56 49541.79 
     129      130      131      132      133      134      135      136 
44977.86 45722.01 48866.38 48692.52 44623.03 42962.69 47677.11 48104.69 
     137      138      139      140      141      142      143      144 
52161.73 48141.81 48755.34 44382.10 42958.91 48779.73 50275.42 49872.68 
     145      146      147      148      149      150      151      152 
47070.31 48231.64 42488.68 39436.21 42411.17 46180.24 48033.58 48101.34 
     153      154      155      156      157      158      159      160 
47798.91 42038.06 38743.82 44536.84 47518.71 48085.41 48126.42 47779.92 
     161      162      163      164      165      166      167      168 
42531.13 39627.31 46428.15 48683.61 48759.04 48376.00 47870.02 42074.14 
     169      170      171      172      173      174      175      176 
38918.25 46477.07 48138.10 48324.18 48430.88 47867.81 41893.81 38758.18 
     177      178      179      180      181      182      183      184 
46695.71 48298.15 48822.30 49140.85 48433.04 42170.84 38911.17 46591.86 
     185      186      187      188      189      190      191      192 
48126.66 48439.64 48447.58 48141.37 42050.05 38967.07 46666.30 48223.12 
     193      194      195      196      197      198      199      200 
48327.67 48200.60 47835.86 41548.02 38760.84 46219.96 48201.62 48508.75 
     201      202      203      204      205      206      207      208 
48565.32 48096.89 42023.97 38910.08 46558.24 48183.92 48908.91 48994.20 
     209      210      211      212      213      214      215      216 
48942.08 42949.03 38705.86 43529.42 44894.34 45481.48 45780.80 45420.42 
     217      218      219      220      221      222      223      224 
40890.24 37744.47 41896.22 42032.69 42181.84 42816.04 43052.42 40301.27 
     225      226      227      228      229      230      231      232 
37968.51 41997.24 42159.16 41866.27 42237.18 43196.61 41545.01 40798.10 
     233      234      235      236      237      238      239      240 
44675.10 45358.39 45582.55 45591.26 46024.24 41841.29 38523.81 43083.80 
     241      242      243      244      245      246      247      248 
45603.79 47028.02 47140.04 46187.88 42372.78 38542.23 45628.40 48024.66 
     249      250      251      252      253      254      255      256 
48268.14 48141.56 47895.88 42042.15 39032.83 46482.84 48215.55 48214.80 
     257      258      259      260      261      262      263      264 
48286.17 47947.47 42355.22 39543.67 46699.08 47990.80 48649.32 49133.46 
     265      266      267      268      269      270      271      272 
48346.93 42947.04 39707.53 46854.57 48160.93 49170.30 49836.35 49789.59 
     273      274      275      276      277      278      279      280 
44451.09 42984.75 48940.75 49541.10 50310.63 50461.60 50031.09 43767.38 
     281      282      283      284      285      286      287      288 
41535.05 48448.01 50000.66 50126.61 50444.76 50361.10 46507.81 46047.12 
     289      290      291      292      293      294      295      296 
52494.62 53291.88 53012.91 51553.40 50713.98 46013.24 42922.26 49074.34 
     297      298      299      300      301      302      303      304 
50486.20 51382.47 51585.70 52290.55 51631.73 57179.64 62573.08 63532.77 
     305      306      307      308      309      310      311      312 
62464.33 59353.57 56747.56 54371.52 52627.86 58842.95 62588.15 63618.98 
     313      314      315      316      317      318      319      320 
61914.10 61545.04 54960.55 54107.81 61796.96 64439.73 64674.93 65035.58 
     321      322      323      324      325      326      327      328 
64309.08 57425.69 54752.71 59995.55 62694.48 62888.04 63830.91 63647.88 
     329      330      331      332      333      334      335      336 
56553.22 53689.04 56803.52 62474.92 65735.64 68275.31 72098.13 71028.35 
     337      338      339      340      341      342      343      344 
68709.24 68113.19 69220.38 71490.03 73793.18 74893.14 71497.93 70528.64 
     345      346      347      348      349      350      351      352 
72515.75 76113.71 80932.12 77317.55 71260.43 65048.60 60426.98 63609.07 
     353      354      355      356      357      358      359      360 
66843.61 67918.71 67489.05 67218.50 60120.74 55782.94 58082.56 55834.96 
     361      362      363      364      365      366      367      368 
57648.99 57929.76 58511.40 57223.74 55914.48 60240.59 59888.14 63677.17 
     369      370      371      372      373      374      375      376 
64592.59 64036.32 61955.81 60771.01 66311.17 67999.15 71635.73 71354.76 
     377      378      379      380      381      382      383      384 
69534.55 66816.40 66105.56 74614.10 76504.29 79845.96 81928.50 84474.81 
     385      386      387      388      389      390      391      392 
77917.75 74495.74 76467.78 76728.14 77846.90 78049.25 80246.91 75823.03 
     393      394      395      396      397      398      399      400 
70968.57 69990.71 68830.19 64628.69 64981.81 64629.57 63264.46 66803.73 
     401      402      403      404      405      406      407      408 
68194.57 69002.52 70912.28 73020.77 75867.08 72915.65 71733.81 74005.63 
     409      410      411      412      413      414      415      416 
75880.43 77248.05 75324.77 72792.51 68709.93 68404.04 71752.38 72851.02 
     417      418      419      420      421      422      423      424 
72871.07 75744.78 79924.90 81460.35 79279.36 79946.21 79040.59 78154.77 
     425      426      427      428      429      430      431      432 
76815.96 75523.17 72380.65 71224.70 69787.99 66495.33 63497.24 61073.79 
     433      434      435      436      437      438      439      440 
59482.55 56890.69 56787.97 62726.93 68718.99 73453.67 75204.28 75163.25 
     441      442      443      444      445      446      447      448 
68782.71 63295.01 69145.67 70056.64 68792.49 67780.91 64681.66 59892.81 
     449      450      451      452      453      454      455      456 
59269.96 65330.26 67632.03 68082.26 68242.71 67404.39 63153.63 62127.14 
     457      458      459      460      461      462      463      464 
66607.45 65464.31 66833.31 66820.45 67029.24 64327.41 62361.25 64373.44 
     465      466      467      468      469      470      471      472 
63209.80 60394.76 58504.46 58452.00 53431.73 46858.75 48503.66 49563.37 
     473      474      475      476      477      478      479      480 
49035.43 49724.11 50765.00 51223.62 49914.32 53604.67 52947.65 51177.47 
     481      482      483      484      485      486      487      488 
49063.17 48968.59 49701.09 51762.14 55860.42 55598.69 53324.71 51035.35 
     489      490      491      492      493      494      495      496 
51905.43 48450.86 45963.14 49032.51 48581.61 43612.66 43661.51 48428.62 
     497      498      499      500      501      502      503      504 
45390.55 44189.72 49285.64 49911.60 50053.50 51351.16 51507.57 49081.19 
     505      506      507      508      509      510      511      512 
46956.09 49500.54 52291.12 52480.11 53783.59 55893.30 53085.45 49175.26 
     513      514      515      516      517      518      519      520 
50087.78 52662.33 54398.14 54163.04 52446.59 47935.91 44419.12 48384.84 
     521      522      523      524      525      526      527      528 
49215.52 48441.29 48483.00 48076.08 42411.61 40500.55 46734.12 47827.10 
     529      530      531      532      533      534      535      536 
48148.06 48042.60 47557.62 41777.16 39223.25 46360.21 48029.43 48316.72 
     537      538      539      540      541      542      543      544 
48123.38 47539.57 41854.38 39198.53 46915.17 48071.61 48163.58 47993.12 
     545      546      547      548      549      550      551      552 
47913.31 42295.07 39127.01 46425.35 47904.63 48110.61 48223.94 48002.50 
     553      554      555      556      557      558      559      560 
42507.36 39987.86 47523.50 48883.12 49079.22 48697.16 48206.05 42381.37 
     561      562      563      564      565      566      567      568 
39232.66 47354.47 48909.02 49184.50 49048.30 49046.83 43546.31 41332.85 
     569      570      571      572      573      574      575      576 
47738.49 49217.57 49330.71 49305.40 49622.10 44197.77 40045.04 44993.31 
     577      578      579      580      581      582      583      584 
44784.90 46249.68 46958.56 47618.60 42968.67 39459.43 43798.24 43462.10 
     585      586      587      588      589      590      591      592 
43599.05 42962.29 42507.15 39215.01 37963.27 41298.68 41636.64 41667.58 
     593      594      595      596      597      598      599      600 
39983.42 41732.86 38497.16 37466.80 42160.78 42336.04 43359.15 43560.19 
     601      602      603      604      605      606      607      608 
43967.24 40850.18 37767.89 43697.96 44902.00 46541.57 46532.85 46722.68 
     609      610      611      612      613      614      615      616 
41882.15 38678.04 45346.46 47256.70 48225.48 48303.55 48138.49 42412.43 
     617      618      619      620      621      622      623      624 
39314.99 45974.91 47880.55 48103.96 48387.34 48206.43 42487.99 39918.89 
     625      626      627      628      629      630      631      632 
46878.94 48740.68 48942.99 48978.80 48899.44 42919.44 39654.93 46801.60 
     633      634      635      636      637      638      639      640 
48050.52 48429.48 48423.18 48200.10 42285.86 39831.21 46787.97 47997.71 
     641      642      643      644      645      646      647      648 
48543.51 48542.49 48521.40 42644.96 40957.04 47886.10 49095.88 49557.25 
     649      650      651      652      653      654      655      656 
50862.40 52680.15 51163.25 49072.15 50853.63 52372.26 52334.20 52221.42 
     657      658      659      660      661      662      663      664 
52286.07 46843.29 44664.75 49041.33 49379.36 49680.12 50178.58 49937.83 
     665      666      667      668      669      670      671      672 
45143.30 42591.57 48676.76 51406.08 54199.92 55137.93 54181.25 47816.99 
     673      674      675      676      677      678      679      680 
47658.81 53972.54 55834.11 54708.92 54457.11 53277.79 51734.38 51160.15 
     681      682      683      684      685      686      687      688 
57382.64 57628.99 60342.98 61883.92 64227.90 62870.26 60564.91 63061.31 
     689      690      691      692      693      694      695      696 
66931.58 70472.54 72412.42 73383.61 68782.01 64678.38 69187.32 72127.46 
     697      698      699      700      701      702      703      704 
74505.55 76273.08 74804.21 68213.06 65869.89 70303.09 73440.45 73210.20 
     705      706      707      708      709      710      711      712 
73180.24 71467.94 68117.32 66448.06 72674.22 75435.47 75036.85 74680.63 
     713      714      715      716      717      718      719      720 
74384.48 67697.32 64591.07 67169.63 68516.75 67501.45 66494.65 67036.12 
     721      722      723      724      725      726      727      728 
64473.57 59931.90 60939.57 60286.38 58020.07 62022.39 61440.90 59405.21 
     729      730      731      732      733      734      735      736 
61331.57 64970.40 65956.37 60670.24 61555.79 61578.14 58192.79 59192.01 
     737      738      739      740      741      742      743      744 
62531.22 62482.13 61288.80 61798.29 63996.76 60948.79 59772.38 65707.19 
     745      746      747      748      749      750      751      752 
68002.84 68623.92 67249.73 67659.75 62763.22 60509.68 68425.26 71864.35 
     753      754      755      756      757      758      759      760 
72570.04 71098.62 69745.21 65163.46 60366.00 67501.38 71796.78 72531.49 
     761      762      763      764      765      766      767      768 
72923.57 72565.24 66726.60 64372.06 70405.39 70948.05 69624.62 68365.76 
     769      770      771      772      773      774      775      776 
66443.47 63485.59 62177.69 67445.73 69326.04 69746.93 69349.99 66578.87 
     777      778      779      780      781      782      783      784 
61607.30 59569.20 65558.62 66404.37 66379.23 64620.05 65366.77 61722.10 
     785      786      787      788      789      790      791      792 
57330.76 62032.75 63330.19 66384.21 67169.89 68138.06 65078.60 62402.55 
     793      794      795      796      797      798      799      800 
65408.57 67270.41 66542.22 65555.92 64466.82 58050.25 52860.90 56825.55 
     801      802      803      804      805      806      807      808 
58508.98 58755.74 58773.95 58183.18 54565.74 51255.71 55491.72 56808.75 
     809      810      811      812      813      814      815      816 
57017.28 55260.75 54447.08 53690.11 55483.71 62056.67 63484.18 63535.51 
     817      818      819      820      821      822      823      824 
63549.70 61191.93 53818.54 49968.38 52062.62 52336.85 51763.22 51593.86 
     825      826      827      828      829      830      831      832 
52809.94 49349.69 44819.79 47622.64 50249.92 52163.54 51649.75 51528.24 
     833      834      835      836      837      838      839      840 
47160.06 43870.56 49696.32 51306.79 52422.77 51847.80 51439.35 48866.65 
     841      842      843      844      845      846      847      848 
47595.55 50313.63 51359.64 50724.92 50942.05 50571.85 47910.36 45849.59 
     849      850      851      852      853      854      855      856 
49143.05 51839.40 51442.38 48018.30 50096.48 46868.55 46973.30 48683.61 
     857      858      859      860      861      862      863      864 
49024.44 49129.15 45694.73 45181.33 43070.95 41933.25 49435.79 51399.43 
     865      866      867      868      869      870      871      872 
50862.73 49782.72 49791.42 44564.35 40829.46 46232.49 48125.90 48155.69 
     873      874      875      876      877      878      879      880 
48393.58 48710.94 44126.69 42276.28 47722.02 48998.65 49334.82 45871.78 
     881      882      883      884      885      886      887      888 
45810.36 41843.75 39669.82 46500.56 47962.08 48295.84 45888.31 46560.59 
     889      890      891      892      893      894      895      896 
42025.03 38927.22 42887.26 46731.31 48034.92 48183.12 48225.96 42221.17 
     897      898      899      900      901      902      903      904 
38687.89 44237.64 47347.67 47619.91 47814.80 47679.10 42034.68 38792.52 
     905      906      907      908      909      910      911      912 
45964.31 47936.87 48044.27 47964.33 47800.06 41899.53 38791.91 46111.61 
     913      914      915      916      917      918      919      920 
47138.39 47859.98 48158.71 47917.25 42106.99 38643.61 45664.15 47331.67 
     921      922      923      924      925      926      927      928 
47543.74 47861.97 47783.17 41879.18 38799.33 42466.04 46429.01 48024.73 
     929      930      931      932      933      934      935      936 
49128.91 49210.15 44265.72 40011.93 43585.99 47343.73 48305.06 48769.05 
     937      938      939      940      941      942      943      944 
48253.95 42481.04 39016.85 45308.66 45925.83 46390.82 46452.73 46001.39 
     945      946      947      948      949      950      951      952 
41075.33 38269.86 42255.64 42416.58 42747.52 42993.92 42914.56 39854.87 
     953      954      955      956      957      958      959      960 
37657.43 41473.45 41575.42 41626.87 41468.52 40109.97 38454.31 37747.49 
     961      962      963      964      965      966      967      968 
42368.08 41611.09 42811.43 42979.56 42526.99 40721.27 38190.74 42854.12 
     969      970      971      972      973      974      975      976 
45182.01 46154.74 46443.75 46444.15 41574.27 38504.09 44108.15 46859.10 
     977      978      979      980      981      982      983      984 
47618.12 47504.80 47458.84 42235.10 38791.58 46086.03 47867.84 48190.25 
     985      986      987      988      989      990      991      992 
48078.14 47854.93 41874.51 38944.64 46488.28 47855.92 48302.07 48288.05 
     993      994      995      996      997      998      999     1000 
48022.32 42165.12 38984.28 46254.49 47750.82 48653.58 48896.04 48660.01 
    1001     1002     1003     1004     1005     1006     1007     1008 
42802.42 39643.58 46662.72 47860.46 48578.80 48588.40 48585.70 42675.82 
    1009     1010     1011     1012     1013     1014     1015     1016 
40531.16 48267.95 49296.76 49480.03 49697.69 49690.91 43368.49 40883.69 
    1017     1018     1019     1020     1021     1022     1023     1024 
48296.95 49837.93 50243.79 49893.44 49849.65 43637.75 41223.70 47495.21 
    1025     1026     1027     1028     1029     1030     1031     1032 
48599.47 51165.38 52631.32 52233.37 46363.63 45069.55 50965.74 52722.82 
    1033     1034     1035     1036     1037     1038     1039     1040 
53118.22 52717.81 51883.00 46491.68 44744.72 50481.92 54277.31 56306.27 
    1041     1042     1043     1044     1045     1046     1047     1048 
58992.17 57919.29 53200.99 51591.91 56432.80 54681.88 56309.44 58072.23 
    1049     1050     1051     1052     1053     1054     1055     1056 
57948.40 54544.13 53167.57 57747.83 59067.81 61634.45 61408.33 59244.11 
    1057     1058     1059     1060     1061     1062     1063     1064 
53937.69 50723.32 54395.67 57780.79 59003.42 59227.13 56651.20 53544.28 
    1065     1066     1067     1068     1069     1070     1071     1072 
52223.85 60425.89 65578.63 68574.02 70426.84 69213.86 66030.93 63499.47 
    1073     1074     1075     1076     1077     1078     1079     1080 
68143.09 71797.20 71491.72 69123.95 67394.00 63122.02 61465.20 65126.57 
    1081     1082     1083     1084     1085     1086     1087     1088 
67598.64 65035.90 61929.62 60331.91 58134.20 58693.10 62409.47 64721.31 
    1089     1090     1091     1092     1093     1094     1095     1096 
64402.39 61263.86 65341.74 62744.94 68134.59 74507.63 74892.99 74273.09 
    1097     1098     1099     1100     1101     1102     1103     1104 
73970.33 71836.29 65855.29 62738.12 69712.91 73349.78 73144.53 69916.85 
    1105     1106     1107     1108     1109     1110     1111     1112 
65755.51 59973.99 58211.49 66002.98 67634.09 67286.95 66577.85 67647.10 
    1113     1114     1115     1116     1117     1118     1119     1120 
65591.17 67524.11 72742.70 76189.31 77749.92 77659.05 78051.57 73731.15 
    1121     1122     1123     1124     1125     1126     1127     1128 
71327.16 73291.43 73645.20 72889.31 71579.56 73288.39 70805.64 70516.70 
    1129     1130     1131     1132     1133     1134     1135     1136 
76484.90 79554.43 79474.69 79744.66 81494.73 78452.71 74305.78 75243.32 
    1137     1138     1139     1140     1141     1142     1143     1144 
76224.48 75876.01 74264.04 71862.00 67013.01 65147.44 69575.65 70686.86 
    1145     1146     1147     1148     1149     1150     1151     1152 
72369.77 72486.22 70268.08 66643.25 65789.50 69249.68 70148.12 70012.18 
    1153     1154     1155     1156     1157     1158     1159     1160 
67827.28 68019.27 65186.50 59693.21 62451.15 64228.42 66759.72 68488.01 
    1161     1162     1163     1164     1165     1166     1167     1168 
67639.50 61268.82 57206.84 61483.66 61832.26 60801.37 61715.22 63119.01 
    1169     1170     1171     1172     1173     1174     1175     1176 
60222.36 60735.21 64052.83 62025.35 60264.23 59877.80 61982.73 58666.36 
    1177     1178     1179     1180     1181     1182     1183     1184 
58713.04 61952.43 63551.13 64862.64 65782.62 63514.48 57285.31 52177.53 
    1185     1186     1187     1188     1189     1190     1191     1192 
55320.09 55921.66 58568.31 60161.00 60222.06 55279.14 54289.52 57626.59 
    1193     1194     1195     1196     1197     1198     1199     1200 
57284.40 57535.96 55817.53 53415.50 49727.17 47230.48 48855.17 48476.98 
    1201     1202     1203     1204     1205     1206     1207     1208 
48671.62 48096.37 48492.93 45911.31 44527.87 49362.21 49329.97 49012.87 
    1209     1210     1211     1212     1213     1214     1215     1216 
48728.92 48892.81 43670.19 42026.49 49034.31 51850.97 52620.83 51914.93 
    1217     1218     1219     1220     1221     1222     1223     1224 
48116.00 44767.76 42653.17 45901.20 48233.36 48813.88 49143.26 44575.10 
    1225     1226     1227     1228     1229     1230     1231     1232 
41319.56 39983.20 46334.19 47498.51 47547.89 44055.44 46307.01 43539.78 
    1233     1234     1235     1236     1237     1238     1239     1240 
40543.74 46224.40 48415.92 50634.31 49902.86 49744.37 43863.72 40145.36 
    1241     1242     1243     1244     1245     1246     1247     1248 
42349.93 47055.85 47851.36 48079.07 48081.45 42816.86 39292.45 44827.27 
    1249     1250     1251     1252     1253     1254     1255     1256 
47740.35 48127.67 48375.31 48693.91 42363.26 38954.89 46095.44 47492.75 
    1257     1258     1259     1260     1261     1262     1263     1264 
47676.17 47895.34 47975.82 41798.68 38681.83 46147.25 47274.99 47696.79 
    1265     1266     1267     1268     1269     1270     1271     1272 
47485.29 47604.40 41544.72 38689.31 46105.35 47460.25 47608.72 48073.10 
    1273     1274     1275     1276     1277     1278     1279     1280 
48072.96 42387.36 39136.19 46561.27 49061.92 50456.99 49932.35 50307.72 
    1281     1282     1283     1284     1285     1286     1287     1288 
45402.82 41621.84 48036.56 49296.12 48908.30 48307.57 48334.97 43584.59 
    1289     1290     1291     1292     1293     1294     1295     1296 
40002.43 46690.23 43100.86 46658.33 49502.75 49969.84 44152.08 40359.98 
    1297     1298     1299     1300     1301     1302     1303     1304 
46393.36 47395.60 49310.41 48810.30 48494.18 42917.53 38890.02 45970.65 
    1305     1306     1307     1308     1309     1310     1311     1312 
46191.25 46904.85 46790.27 46320.94 40933.25 38531.44 43389.73 43870.88 
    1313     1314     1315     1316     1317     1318     1319     1320 
44226.52 44190.63 45103.80 40576.45 37606.47 41507.77 42551.75 44013.49 
    1321     1322     1323     1324     1325     1326     1327     1328 
43981.44 42716.60 38986.11 37687.51 42159.99 41781.69 42293.28 43017.27 
    1329     1330     1331     1332     1333     1334     1335     1336 
43397.40 40556.62 37749.26 42988.02 43947.75 45410.29 46149.62 46605.74 
    1337     1338     1339     1340     1341     1342     1343     1344 
42134.36 39913.17 44617.52 46482.39 47202.88 46859.94 46969.38 42112.48 
    1345     1346     1347     1348     1349     1350     1351     1352 
39434.05 46780.50 47757.70 47863.91 47884.64 47684.48 41840.09 39106.51 
    1353     1354     1355     1356     1357     1358     1359     1360 
46589.97 47717.83 48062.47 48228.40 48224.12 42416.63 40449.02 47269.32 
    1361     1362     1363     1364     1365     1366     1367     1368 
48379.98 49188.28 49190.49 48719.41 43967.43 41210.13 47041.36 48894.17 
    1369     1370     1371     1372     1373     1374     1375     1376 
49434.47 50009.11 50474.99 45325.82 43433.50 48073.35 49210.87 50029.27 
    1377     1378     1379     1380     1381     1382     1383     1384 
50527.76 50838.38 46269.54 43538.14 50655.09 52436.03 54790.26 58497.97 
    1385     1386     1387     1388     1389     1390     1391     1392 
58965.64 54986.75 51475.17 53858.84 57299.84 57469.35 56945.96 55327.83 
    1393     1394     1395     1396     1397     1398     1399     1400 
50628.98 48674.93 53170.92 53651.49 53947.74 54532.33 54178.65 50583.25 
    1401     1402     1403     1404     1405     1406     1407     1408 
48080.17 53570.74 54568.51 55201.57 54791.63 51751.12 46874.14 45173.69 
    1409     1410     1411     1412     1413     1414     1415     1416 
49610.81 52267.14 52861.72 54203.79 54985.32 52098.29 49968.89 54356.45 
    1417     1418     1419     1420     1421     1422     1423     1424 
54434.17 54114.41 54582.51 54336.88 54754.61 57836.60 67402.75 70496.47 
    1425     1426     1427     1428     1429     1430     1431     1432 
67414.92 66450.10 66815.65 63751.11 59388.45 61424.07 62410.13 64001.11 
    1433     1434     1435     1436     1437     1438     1439     1440 
65065.60 64152.83 61359.24 57109.32 59322.53 61197.58 63162.24 67269.61 
    1441     1442     1443     1444     1445     1446     1447     1448 
66790.09 61869.66 59448.62 64185.27 63640.18 61171.36 60127.21 59744.81 
    1449     1450     1451     1452     1453     1454     1455     1456 
55528.09 53939.94 56691.05 58099.54 58346.29 57595.09 54265.00 52913.79 
    1457     1458     1459     1460     1461     1462     1463     1464 
53286.93 57500.69 57892.72 58696.04 58303.86 57924.11 55481.15 57556.67 
    1465     1466     1467     1468     1469     1470     1471     1472 
62912.45 64738.90 66631.81 65774.02 64449.14 60203.35 58745.29 64172.03 
    1473     1474     1475     1476     1477     1478     1479     1480 
67268.82 69723.15 71367.31 72767.55 71033.82 71126.98 76506.91 78145.79 
    1481     1482     1483     1484     1485     1486     1487     1488 
78216.24 78280.29 76575.03 67521.92 63801.75 65993.22 65822.22 64892.28 
    1489     1490     1491     1492     1493     1494     1495     1496 
65376.45 65638.27 62147.02 57566.92 58622.77 61240.95 64109.90 66627.81 
    1497     1498     1499     1500     1501     1502     1503     1504 
65732.40 59072.90 56601.46 61374.58 64365.85 68101.83 71016.02 69606.41 
    1505     1506     1507     1508     1509     1510     1511     1512 
62210.26 59925.34 68530.44 73809.73 76120.35 75146.90 74054.78 65162.84 
    1513     1514     1515     1516     1517     1518     1519     1520 
59192.45 62859.31 64365.51 66736.80 68686.55 70577.54 65622.27 63569.74 
    1521     1522     1523     1524     1525     1526     1527     1528 
68896.92 70553.37 68126.49 67704.19 68818.69 64989.07 65232.62 71054.99 
    1529     1530     1531     1532     1533     1534     1535     1536 
72763.04 71838.23 70243.64 69166.16 63607.92 61700.23 65917.08 66162.98 
    1537     1538     1539     1540     1541     1542     1543     1544 
67412.18 67799.61 66952.22 61054.57 59486.05 62773.73 62655.07 62962.34 
    1545     1546     1547     1548     1549     1550     1551     1552 
63035.42 62298.72 54991.55 52342.37 55129.15 56451.96 57757.95 58628.58 
    1553     1554     1555     1556     1557     1558     1559     1560 
59584.70 54454.13 49334.01 53043.52 54976.56 55302.67 57425.74 59196.26 
    1561     1562     1563     1564     1565     1566     1567     1568 
55660.34 50137.60 53890.87 53515.19 54072.44 53892.41 52118.88 48560.16 
    1569     1570     1571     1572     1573     1574     1575     1576 
48382.02 54465.20 54535.97 52509.67 51124.57 50643.47 47907.06 49132.74 
    1577     1578     1579     1580     1581     1582     1583     1584 
56710.22 58201.50 58717.07 58356.95 56524.74 53831.32 52426.69 54743.49 
    1585     1586     1587     1588     1589     1590     1591     1592 
54101.52 52245.86 47823.41 47083.72 43016.75 41174.66 46190.83 47395.64 
    1593     1594     1595     1596     1597     1598     1599     1600 
48615.19 47209.25 47046.26 44484.10 43367.33 46915.25 48671.33 49301.50 
    1601     1602     1603     1604     1605     1606     1607     1608 
49371.48 48787.14 43008.82 40396.49 46364.65 49499.70 48775.67 47905.56 
    1609     1610     1611     1612     1613     1614     1615     1616 
47208.79 41666.52 39278.70 46535.05 47939.46 48803.37 48240.18 47897.02 
    1617     1618     1619     1620     1621     1622     1623     1624 
42888.13 39405.18 46291.36 46963.81 47027.08 46924.87 46766.58 41434.95 
    1625     1626     1627     1628     1629     1630     1631     1632 
38956.88 45710.50 46730.68 47069.92 47312.25 46503.72 42604.97 39275.09 
    1633     1634     1635     1636     1637     1638     1639     1640 
46074.63 47236.35 48127.48 48795.71 48033.55 41857.80 38745.51 46138.36 
    1641     1642     1643     1644     1645     1646     1647     1648 
47105.56 47337.02 47526.76 47663.63 41831.62 38793.04 46243.93 47553.17 
    1649     1650     1651     1652     1653     1654     1655     1656 
47990.18 48024.08 48231.35 43075.78 40357.85 46663.74 47434.60 47543.66 
    1657     1658     1659     1660     1661     1662     1663     1664 
43041.39 45183.82 40755.60 39489.72 47758.07 49599.28 49301.96 46629.69 
    1665     1666     1667     1668     1669     1670     1671     1672 
46674.08 42160.51 39119.87 46152.17 47579.77 47498.55 47152.22 47316.75 
    1673     1674     1675     1676     1677     1678     1679     1680 
41952.25 38888.24 43292.32 43647.84 44486.18 44327.81 43764.80 40208.56 
    1681     1682     1683     1684     1685     1686     1687     1688 
37649.45 41884.53 42106.71 42011.25 41634.07 41700.79 39376.92 38280.91 
    1689     1690     1691     1692     1693     1694     1695     1696 
41530.23 43468.88 43369.30 43792.79 43432.80 40035.43 37636.36 41802.76 
    1697     1698     1699     1700     1701     1702     1703     1704 
44758.65 46523.54 47287.36 47484.18 44231.27 40694.34 44734.86 46908.27 
    1705     1706     1707     1708     1709     1710     1711     1712 
47251.57 47956.94 47937.06 43323.60 39739.10 46361.95 47863.22 48545.08 
    1713     1714     1715     1716     1717     1718     1719     1720 
48174.91 48025.05 42575.63 39527.56 46899.99 48477.56 48263.56 47686.21 
    1721     1722     1723     1724     1725     1726     1727     1728 
47080.16 42153.65 39603.66 46804.61 47536.18 47666.64 47647.34 47408.04 
    1729     1730     1731     1732     1733     1734     1735     1736 
42061.48 39876.52 46811.30 47195.14 47763.22 47567.56 47206.93 42520.34 
    1737     1738     1739     1740     1741     1742     1743     1744 
41007.69 48276.75 49048.65 49486.99 50737.92 51038.11 47412.89 45731.79 
    1745     1746     1747     1748     1749     1750     1751     1752 
51956.48 54249.88 55535.11 56161.92 55587.94 50609.62 46272.55 50718.52 
    1753     1754     1755     1756     1757     1758     1759     1760 
52949.90 54645.65 55780.88 56488.90 52786.90 49012.31 51086.56 53779.77 
    1761     1762     1763     1764     1765     1766     1767     1768 
54255.28 54839.14 54714.26 51847.83 49549.60 53448.61 52008.36 55224.65 
    1769     1770     1771     1772     1773     1774     1775     1776 
58765.36 59841.71 55131.14 56038.20 63484.80 66462.41 65707.29 63371.96 
    1777     1778     1779     1780     1781     1782     1783     1784 
61882.72 58795.34 57344.97 63532.78 64572.69 63300.61 61491.82 59910.34 
    1785     1786     1787     1788     1789     1790     1791     1792 
58141.47 54145.72 56702.39 58797.84 60273.89 59981.33 60475.77 58824.96 
    1793     1794     1795     1796     1797     1798     1799     1800 
56819.18 63691.73 69458.39 72491.50 73129.43 73189.14 69329.77 65736.70 
    1801     1802     1803     1804     1805     1806     1807     1808 
68860.64 69724.42 70351.33 69757.32 69923.63 65469.54 63309.32 66730.38 
    1809     1810     1811     1812     1813     1814     1815     1816 
68864.53 69393.00 69185.19 67664.22 64138.80 65100.46 70553.66 72282.55 
    1817     1818     1819     1820     1821     1822     1823     1824 
71543.50 70064.24 66954.88 60484.24 57417.05 61360.76 63921.28 68651.44 
    1825     1826     1827     1828     1829     1830     1831     1832 
72121.98 74257.39 73885.47 70046.69 72014.28 74002.03 77807.62 77513.22 
    1833     1834     1835     1836     1837     1838     1839     1840 
80975.42 81885.35 74339.01 74201.83 73504.23 71823.64 70724.74 71550.92 
    1841     1842     1843     1844     1845     1846     1847     1848 
71239.06 69901.66 74142.56 79945.76 82972.46 84923.06 85518.10 82640.49 
    1849     1850     1851     1852     1853     1854     1855     1856 
78737.75 78166.04 81012.86 83155.24 83668.39 76600.00 69386.81 66392.31 
    1857     1858     1859     1860     1861     1862     1863     1864 
67574.83 67142.95 65873.25 64336.03 64987.79 62521.92 61144.16 67060.00 
    1865     1866     1867     1868     1869     1870     1871     1872 
67505.85 69467.58 71758.61 73820.50 70378.36 65582.60 66230.47 65224.03 
    1873     1874     1875     1876     1877     1878     1879     1880 
64021.64 63869.82 64494.45 62311.35 61747.28 63665.20 63020.21 62312.73 
    1881     1882     1883     1884     1885     1886     1887     1888 
62270.76 64081.01 61830.79 57651.43 62537.12 64100.54 65029.84 64108.16 
    1889     1890     1891     1892     1893     1894     1895     1896 
61907.42 58132.17 58347.03 63152.23 65187.82 63696.27 59900.38 58265.66 
    1897     1898     1899     1900     1901     1902     1903     1904 
53679.98 52630.12 57804.49 58778.72 58648.45 56318.11 55995.82 52962.40 
    1905     1906     1907     1908     1909     1910     1911     1912 
50553.30 54057.97 56545.79 59918.99 60574.68 60337.65 54736.98 50489.75 
    1913     1914     1915     1916     1917     1918     1919     1920 
53276.42 53502.52 51976.60 50241.64 50555.74 48747.09 48221.72 51676.58 
    1921     1922     1923     1924     1925     1926     1927     1928 
51690.30 52434.24 53348.60 52664.94 47558.26 43015.19 48075.90 49848.16 
    1929     1930     1931     1932     1933     1934     1935     1936 
50999.74 50751.11 50809.32 45933.58 45137.50 48761.30 51876.56 55233.93 
    1937     1938     1939     1940     1941     1942     1943     1944 
57152.36 55771.22 49685.84 47307.32 49184.37 52684.18 58430.85 60529.00 
    1945     1946     1947     1948     1949     1950     1951     1952 
59783.80 53161.28 48388.45 52819.74 53591.89 55129.82 54433.74 52036.57 
    1953     1954     1955     1956     1957     1958     1959     1960 
47616.25 45221.33 46994.71 50344.08 50756.19 49701.96 48887.22 45106.93 
    1961     1962     1963     1964     1965     1966     1967     1968 
40886.58 45225.80 47353.47 47394.44 47044.00 48379.85 44673.73 41021.17 
    1969     1970     1971     1972     1973     1974     1975     1976 
46291.48 47212.98 47220.10 43288.46 45507.22 40563.21 39982.98 46109.84 
    1977     1978     1979     1980     1981     1982     1983     1984 
47084.95 47177.58 45687.99 46064.70 42186.07 38918.45 41728.27 45817.96 
    1985     1986     1987     1988     1989     1990     1991     1992 
47139.15 47018.33 46991.16 41751.91 39033.08 44828.63 47247.92 48200.39 
    1993     1994     1995     1996     1997     1998     1999     2000 
48413.03 47965.07 42600.39 40426.96 48328.05 49992.76 50703.15 50042.21 
    2001     2002     2003     2004     2005     2006     2007     2008 
49360.12 43251.25 39763.43 46572.96 47798.11 48138.45 47323.37 46676.70 
    2009     2010     2011     2012     2013     2014     2015     2016 
42074.80 39123.78 46175.35 47413.45 48471.22 49132.59 49358.43 43829.97 
    2017     2018     2019     2020     2021     2022     2023     2024 
40242.23 46592.49 47582.66 47778.36 48001.73 43489.34 40353.80 39322.96 
    2025     2026     2027     2028     2029     2030     2031     2032 
47795.84 49329.58 49483.44 48504.50 45683.23 42121.80 38950.04 45842.28 
    2033     2034     2035     2036     2037     2038     2039     2040 
46778.42 47089.20 47125.28 47070.21 42238.84 39265.79 43617.76 44740.34 
    2041     2042     2043     2044     2045     2046     2047     2048 
45626.87 46036.39 45879.76 42364.55 38260.22 42325.96 42191.12 42115.53 
    2049     2050     2051     2052     2053     2054     2055     2056 
42331.66 41658.61 39897.49 37867.80 42005.00 40213.67 41552.15 42133.86 
    2057     2058     2059     2060     2061     2062     2063     2064 
42835.83 39928.49 37549.81 42622.54 43140.98 44941.12 45761.37 45843.17 
    2065     2066     2067     2068     2069     2070     2071     2072 
42112.65 40484.09 45721.59 47404.47 47036.17 47123.80 46616.35 41725.97 
    2073     2074     2075     2076     2077     2078     2079     2080 
38987.35 46071.18 47204.60 47469.26 47285.05 46982.28 42019.28 40107.92 
    2081     2082     2083     2084     2085     2086     2087     2088 
46726.83 47815.61 47534.91 48129.21 48678.88 45017.29 43393.21 48840.22 
    2089     2090     2091     2092     2093     2094     2095     2096 
50089.28 50285.46 49963.73 49280.93 43489.38 40572.22 47154.32 48274.61 
    2097     2098     2099     2100     2101     2102     2103     2104 
48520.05 48298.12 48096.61 42643.25 41004.70 47696.44 48801.67 49508.41 
    2105     2106     2107     2108     2109     2110     2111     2112 
49763.66 49914.00 46406.20 43861.75 49295.11 50577.59 50307.15 49780.73 
    2113     2114     2115     2116     2117     2118     2119     2120 
49725.29 45084.81 41410.97 47568.42 48804.56 49114.56 49516.69 49252.69 
    2121     2122     2123     2124     2125     2126     2127     2128 
44258.29 44431.74 50578.84 51139.81 50808.89 50443.75 50998.00 46800.91 
    2129     2130     2131     2132     2133     2134     2135     2136 
48288.78 53274.13 57935.81 56688.35 53718.51 54202.98 50717.13 50640.68 
    2137     2138     2139     2140     2141     2142     2143     2144 
59687.80 64251.63 64667.02 64972.92 62776.81 55310.93 52863.31 61110.77 
    2145     2146     2147     2148     2149     2150     2151     2152 
67225.01 67960.60 68064.70 67003.87 62548.44 61099.27 65433.09 64586.63 
    2153     2154     2155     2156     2157     2158     2159     2160 
63184.16 58680.58 59966.79 57988.70 62418.82 66902.01 69024.70 70683.53 
    2161     2162     2163     2164     2165     2166     2167     2168 
72987.08 74546.31 72863.37 72000.05 73671.14 73436.40 73818.47 72878.44 
    2169     2170     2171     2172     2173     2174     2175     2176 
72185.16 71091.45 67386.72 68338.01 71915.57 71675.39 69010.80 69328.75 
    2177     2178     2179     2180     2181     2182     2183     2184 
68438.86 67680.17 69477.72 72952.48 73433.13 70700.12 67562.00 61596.36 
    2185     2186     2187     2188     2189     2190     2191     2192 
60654.41 64534.08 64054.60 65358.51 70284.14 69413.24 60024.45 56990.70 
    2193     2194     2195     2196     2197     2198     2199     2200 
58300.64 61877.50 62018.57 60162.61 61881.16 59240.01 58443.55 62210.35 
    2201     2202     2203     2204     2205     2206     2207     2208 
67393.46 67581.53 67675.06 68616.04 66731.40 64409.26 67805.06 67099.89 
    2209     2210     2211     2212     2213     2214     2215     2216 
67278.06 67744.77 68449.78 63952.06 60607.79 61689.67 63811.49 63231.39 
    2217     2218     2219     2220     2221     2222     2223     2224 
63307.70 66149.72 64290.37 61310.12 65405.07 66830.66 66127.75 68360.17 
    2225     2226     2227     2228     2229     2230     2231     2232 
71961.31 68690.09 67776.35 75254.23 77005.77 79016.10 80516.79 78958.15 
    2233     2234     2235     2236     2237     2238     2239     2240 
74510.63 69631.13 74126.43 77423.69 76387.53 70805.86 68824.65 64633.28 
    2241     2242     2243     2244     2245     2246     2247     2248 
64397.98 68826.56 69680.91 72531.35 74684.96 77050.94 74117.36 72348.39 
    2249     2250     2251     2252     2253     2254     2255     2256 
79369.53 86199.18 86595.88 80765.24 75804.96 68306.18 64710.60 67478.61 
    2257     2258     2259     2260     2261     2262     2263     2264 
67725.73 67521.61 66608.52 63651.15 56455.08 53560.53 57908.18 60542.96 
    2265     2266     2267     2268     2269     2270     2271     2272 
61893.98 61278.79 61674.09 60190.57 63475.16 70330.77 73393.23 73230.53 
    2273     2274     2275     2276     2277     2278     2279     2280 
72788.05 69459.22 63743.47 58667.31 61250.21 62342.41 61919.39 61981.72 
    2281     2282     2283     2284     2285     2286     2287     2288 
63107.84 60383.59 56670.63 56373.56 55688.90 57176.63 58118.21 55878.59 
    2289     2290     2291     2292     2293     2294     2295     2296 
50515.81 47129.24 50675.81 53703.08 55228.98 55930.58 54873.74 49838.36 
    2297     2298     2299     2300     2301     2302     2303     2304 
46052.64 50434.15 50189.59 48955.22 47546.76 46106.01 43581.28 40840.12 
    2305     2306     2307     2308     2309     2310     2311     2312 
45543.90 46253.46 47559.63 48315.11 48606.59 45081.97 43644.00 50009.76 
    2313     2314     2315     2316     2317     2318     2319     2320 
51950.11 51717.84 51701.50 50416.62 44985.57 40928.10 45888.15 42395.81 
    2321     2322     2323     2324     2325     2326     2327     2328 
45923.47 44155.28 46660.93 42809.97 45759.73 49852.18 49372.67 48927.16 
    2329     2330     2331     2332     2333     2334     2335     2336 
47406.35 47148.79 43413.46 39933.27 42215.41 45601.16 47231.74 47029.84 
    2337     2338     2339     2340     2341     2342     2343     2344 
46794.88 42121.84 39094.02 45041.94 46827.30 46985.25 47172.49 47075.65 
    2345     2346     2347     2348     2349     2350     2351     2352 
41497.95 38795.03 45614.50 46880.67 47021.09 47165.64 46970.36 41778.61 
    2353     2354     2355     2356     2357     2358     2359     2360 
38834.77 45767.30 46889.99 47032.51 46789.70 46903.26 41518.58 38786.62 
    2361     2362     2363     2364     2365     2366     2367     2368 
45724.83 46845.03 47899.02 47940.39 47152.51 41377.58 38698.59 45340.05 
    2369     2370     2371     2372     2373     2374     2375     2376 
47519.45 48225.06 48221.21 48108.61 43950.36 41265.38 47615.69 48673.80 
    2377     2378     2379     2380     2381     2382     2383     2384 
48879.39 48217.39 48142.24 42924.02 41321.52 47586.79 48556.02 48216.77 
    2385     2386     2387     2388     2389     2390     2391     2392 
47856.48 48372.46 42963.66 40603.47 46954.61 47895.06 48192.96 48809.93 
    2393     2394     2395     2396     2397     2398     2399     2400 
48192.30 42263.42 39122.51 46796.85 48608.40 49490.51 49931.81 49565.99 
    2401     2402     2403     2404     2405     2406     2407     2408 
43461.30 39944.02 45987.96 46738.20 47502.35 47690.82 47509.84 44426.93 
    2409     2410     2411     2412     2413     2414     2415     2416 
41185.54 45444.28 46091.46 46236.23 44286.88 42995.48 40216.34 39221.68 
    2417     2418     2419     2420     2421     2422     2423     2424 
42255.76 42239.46 39811.82 41937.81 42021.18 39136.84 37531.63 42552.17 
    2425     2426     2427     2428     2429     2430     2431     2432 
43976.94 44873.93 45782.81 45251.63 40692.87 37854.27 43741.13 45373.96 
    2433     2434     2435     2436     2437     2438     2439     2440 
46732.34 46471.96 46100.99 41543.19 38573.16 45682.29 46734.36 47275.64 
    2441     2442     2443     2444     2445     2446     2447     2448 
47265.76 46976.63 41510.48 39139.70 45663.29 47262.19 47733.77 47692.12 
    2449     2450     2451     2452     2453     2454     2455     2456 
47032.77 41580.47 39109.65 46277.69 47572.94 47708.43 47761.38 47192.71 
    2457     2458     2459     2460     2461     2462     2463     2464 
41822.39 38991.73 46515.44 48121.70 48361.54 48196.62 47880.11 42068.15 
    2465     2466     2467     2468     2469     2470     2471     2472 
41077.02 48176.38 49537.89 49843.16 49357.33 48687.62 43260.77 41341.08 
    2473     2474     2475     2476     2477     2478     2479     2480 
48807.47 49539.26 49184.35 48800.48 47957.70 42128.45 40746.48 47139.11 
    2481     2482     2483     2484     2485     2486     2487     2488 
48212.39 48783.10 48822.07 48577.94 44040.71 43203.19 49526.71 51269.17 
    2489     2490     2491     2492     2493     2494     2495     2496 
51752.70 51690.11 52639.38 51500.06 55303.10 62271.59 63968.54 61858.77 
    2497     2498     2499     2500     2501     2502     2503     2504 
56975.08 56767.57 56731.65 54930.20 56276.23 57590.25 58434.10 57463.03 
    2505     2506     2507     2508     2509     2510     2511     2512 
58131.21 54080.40 49866.26 53304.79 55282.76 57153.09 57769.27 57628.14 
    2513     2514     2515     2516     2517     2518     2519     2520 
56075.78 56941.38 65350.64 70782.49 71584.24 69090.30 67677.68 62472.60 
    2521     2522     2523     2524     2525     2526     2527     2528 
60866.58 66695.38 67882.24 66482.20 63560.83 62722.61 58779.66 54832.57 
    2529     2530     2531     2532     2533     2534     2535     2536 
57207.58 58532.18 59933.13 59638.95 59657.10 56660.72 54306.87 59047.55 
    2537     2538     2539     2540     2541     2542     2543     2544 
65414.51 70405.40 72782.73 74571.99 72070.35 66992.11 67294.15 68815.76 
    2545     2546     2547     2548     2549     2550     2551     2552 
67857.59 67435.26 63406.18 58133.77 54989.88 57482.72 60429.53 66189.67 
    2553     2554     2555     2556     2557     2558     2559     2560 
70745.44 69381.52 66657.40 64093.08 65194.98 63656.29 66639.38 71543.00 
    2561     2562     2563     2564     2565     2566     2567     2568 
75217.25 72488.92 69981.07 71194.85 70101.30 72181.61 75112.18 76674.16 
    2569     2570     2571     2572     2573     2574     2575     2576 
69601.95 64131.56 66245.61 69024.90 71451.43 71164.65 73412.70 72003.08 
    2577     2578     2579     2580     2581     2582     2583     2584 
68391.20 75633.08 78033.50 78227.65 78618.91 77227.89 68803.71 65275.00 
    2585     2586     2587     2588     2589     2590     2591     2592 
71027.68 73790.90 75355.45 76223.88 73145.47 69778.74 69693.42 75796.62 
    2593     2594     2595     2596     2597     2598     2599     2600 
74104.23 71121.41 69948.26 68588.60 63798.76 60979.40 66654.65 70649.23 
    2601     2602     2603     2604     2605     2606     2607     2608 
71781.60 69957.54 68249.63 61593.89 57450.58 63483.08 64955.59 66077.81 
    2609     2610     2611     2612     2613     2614     2615     2616 
64846.80 62413.22 56938.43 56132.74 60569.06 60192.58 58607.43 59266.44 
    2617     2618     2619     2620     2621     2622     2623     2624 
59513.36 55349.29 51814.54 58116.14 60105.20 58765.55 60291.02 60363.31 
    2625     2626     2627     2628     2629     2630     2631     2632 
55146.60 51931.91 59238.37 62611.95 63943.19 62082.41 60651.39 54280.88 
    2633     2634     2635     2636     2637     2638     2639     2640 
52987.55 61634.40 63963.80 62551.15 62062.87 58604.28 53433.50 50495.37 
    2641     2642     2643     2644     2645     2646     2647     2648 
58035.23 59855.56 59937.27 59036.22 56444.44 51491.92 48852.87 52556.56 
    2649     2650     2651     2652     2653     2654     2655     2656 
53758.16 57989.14 61207.36 60225.79 55219.97 52038.52 54629.63 56045.82 
    2657     2658     2659     2660     2661     2662     2663     2664 
57193.63 57543.62 57864.58 54909.15 54873.34 56186.77 56214.19 53505.98 
    2665     2666     2667     2668     2669     2670     2671     2672 
50447.59 48519.21 44412.95 41466.70 42815.49 45699.30 48624.60 50390.51 
    2673     2674     2675     2676     2677     2678     2679     2680 
51186.69 47688.40 47474.70 50539.54 51805.42 47872.40 48927.39 51178.10 
    2681     2682     2683     2684     2685     2686     2687     2688 
49539.96 51693.35 55566.08 53811.49 49797.94 50477.53 50003.69 47217.77 
    2689     2690     2691     2692     2693     2694     2695     2696 
46451.39 49172.15 50030.66 49240.72 49507.47 49372.07 45846.97 44318.16 
    2697     2698     2699     2700     2701     2702     2703     2704 
48561.66 48866.19 47699.03 47369.87 47133.62 42658.03 39850.31 45765.44 
    2705     2706     2707     2708     2709     2710     2711     2712 
47703.96 48377.80 43801.70 44891.62 40678.31 39316.02 45882.98 46311.38 
    2713     2714     2715     2716     2717     2718     2719     2720 
46723.04 45332.75 45197.97 41812.34 39141.14 42941.33 47497.17 48246.47 
    2721     2722     2723     2724     2725     2726     2727     2728 
47163.20 46916.66 41972.34 39057.41 44885.83 47367.41 47463.54 46937.81 
    2729     2730     2731     2732     2733     2734     2735     2736 
46643.97 41094.78 38879.72 46165.07 48339.46 49826.34 50231.36 49889.51 
    2737     2738     2739     2740     2741     2742     2743     2744 
45067.80 41833.15 46830.91 48313.70 48515.46 48980.07 49339.78 45294.90 
    2745     2746     2747     2748     2749     2750     2751     2752 
41034.12 46801.83 47919.22 47915.42 48061.77 48321.95 42964.68 39623.67 
    2753     2754     2755     2756     2757     2758     2759     2760 
46063.42 47269.21 47790.13 47990.65 48195.41 42727.45 40338.64 47250.98 
    2761     2762     2763     2764     2765     2766     2767     2768 
49721.18 50552.54 50267.50 49879.42 43345.33 39519.53 43986.67 44563.99 
    2769     2770     2771     2772     2773     2774     2775     2776 
44841.28 44364.99 45049.68 41182.61 38751.29 43681.46 43351.33 43374.07 
    2777     2778     2779     2780     2781     2782     2783     2784 
43512.61 44560.70 41099.09 38332.97 41494.91 41239.28 41233.13 39516.71 
    2785     2786     2787     2788     2789     2790     2791     2792 
41670.83 38676.54 37837.06 41679.25 42015.78 43183.71 43059.06 43341.82 
    2793     2794     2795     2796     2797     2798     2799     2800 
41559.85 39438.73 44336.37 45864.56 46435.46 46390.74 46418.89 42817.92 
    2801     2802     2803     2804     2805     2806     2807     2808 
39415.02 45240.95 46313.05 46809.08 46852.45 46453.45 41789.13 39694.23 
    2809     2810     2811     2812     2813     2814     2815     2816 
46105.51 47054.78 47134.07 46817.54 46621.94 41948.07 39185.05 45797.35 
    2817     2818     2819     2820     2821     2822     2823     2824 
47013.43 46886.98 46659.51 46529.54 41571.80 39252.29 45966.62 46207.32 
    2825     2826     2827     2828     2829     2830     2831     2832 
46722.57 46775.45 46887.74 41712.45 39296.13 45510.74 46672.64 47351.27 
    2833     2834     2835     2836     2837     2838     2839     2840 
49174.49 48950.18 43722.00 41300.73 47547.07 48693.85 49327.40 49886.34 
    2841     2842     2843     2844     2845     2846     2847     2848 
49407.44 43450.76 40991.34 47145.92 48401.46 49731.35 49871.99 49854.24 
    2849     2850     2851     2852     2853     2854     2855     2856 
44773.66 42986.93 49553.12 51705.21 51761.59 51548.68 51223.15 45813.81 
    2857     2858     2859     2860     2861     2862     2863     2864 
44682.91 51676.61 53374.83 53825.30 53324.40 52143.47 45903.97 47381.85 
    2865     2866     2867     2868     2869     2870     2871     2872 
53399.94 55602.37 57560.11 58032.99 59052.06 58485.31 58754.55 61441.45 
    2873     2874     2875     2876     2877     2878     2879     2880 
62436.46 65472.59 67372.37 69975.45 66418.50 64625.83 66993.44 69869.07 
    2881     2882     2883     2884     2885     2886     2887     2888 
71811.28 70145.73 66759.79 60646.16 58155.97 60987.11 60945.11 60150.15 
    2889     2890     2891     2892     2893     2894     2895     2896 
60460.20 60998.37 59038.91 58669.48 66557.48 70832.51 73416.63 73067.71 
    2897     2898     2899     2900     2901     2902     2903     2904 
70211.35 63597.25 57757.71 61630.74 67424.65 69248.28 69201.78 66718.32 
    2905     2906     2907     2908     2909     2910     2911     2912 
59201.62 56105.41 59322.73 60384.26 60825.39 58932.39 59214.78 56610.62 
    2913     2914     2915     2916     2917     2918     2919     2920 
55096.24 58036.97 57806.98 56075.65 59249.97 58534.28 58226.48 63554.95 
    2921     2922     2923     2924     2925     2926     2927     2928 
68425.48 69024.24 68320.43 65169.70 63971.44 61783.86 62562.40 70132.67 
    2929     2930     2931     2932     2933     2934     2935     2936 
68256.21 65082.93 63803.16 63906.55 62895.85 62632.12 67627.99 66317.16 
    2937     2938     2939     2940     2941     2942     2943     2944 
64527.18 64320.95 64695.30 63308.66 64710.56 70797.83 73639.15 73111.96 
    2945     2946     2947     2948     2949     2950     2951     2952 
72443.23 73158.61 67278.41 63948.12 67306.13 67691.85 67947.28 66279.98 
    2953     2954     2955     2956     2957     2958     2959     2960 
62215.06 57374.97 53833.36 53006.84 58824.98 65332.26 68493.01 66977.11 
    2961     2962     2963     2964     2965     2966     2967     2968 
61784.49 56443.15 57498.18 61330.92 65834.05 65036.27 64373.14 58172.06 
    2969     2970     2971     2972     2973     2974     2975     2976 
54694.36 58726.95 63850.26 64479.40 64335.44 64473.74 59383.31 55542.84 
    2977     2978     2979     2980     2981     2982     2983     2984 
57909.79 61260.38 64486.47 66760.83 65695.40 59332.15 56877.57 62723.27 
    2985     2986     2987     2988     2989     2990     2991     2992 
66513.30 66652.54 64520.49 66046.75 61441.25 57462.83 62825.65 60916.47 
    2993     2994     2995     2996     2997     2998     2999     3000 
57935.30 57919.75 59135.92 56334.79 54180.08 55817.87 55560.09 52679.92 
    3001     3002     3003     3004     3005     3006     3007     3008 
50514.54 49697.82 49947.87 50667.97 54298.98 54022.05 54954.43 55696.71 
    3009     3010     3011     3012     3013     3014     3015     3016 
54552.56 52272.33 51544.38 57628.10 58295.00 54329.27 53394.04 52530.98 
    3017     3018     3019     3020     3021     3022     3023     3024 
49761.86 47064.99 47819.47 46293.34 45231.96 44098.76 43671.31 40975.72 
    3025     3026     3027     3028 
40271.38 42641.93 46542.13 45178.46 
> dim(list_experts_train)
NULL

> length(list_experts_train)